# 项目结构 (tree /f 或 tree -af)
# ===============================
.
├── ./.datalad
│   ├── ./.datalad/.gitattributes
│   └── ./.datalad/config
├── ./.git
│   ├── ./.git/COMMIT_EDITMSG
│   ├── ./.git/FETCH_HEAD
│   ├── ./.git/HEAD
│   ├── ./.git/annex
│   │   ├── ./.git/annex/fsck
│   │   │   ├── ./.git/annex/fsck/fsck.lck
│   │   │   └── ./.git/annex/fsck/fsckdb
│   │   │       └── ./.git/annex/fsck/fsckdb/db
│   │   ├── ./.git/annex/index
│   │   ├── ./.git/annex/index.lck
│   │   ├── ./.git/annex/journal
│   │   ├── ./.git/annex/journal.lck
│   │   ├── ./.git/annex/keysdb
│   │   │   └── ./.git/annex/keysdb/db
│   │   ├── ./.git/annex/keysdb.lck
│   │   ├── ./.git/annex/mergedrefs
│   │   ├── ./.git/annex/othertmp
│   │   ├── ./.git/annex/othertmp.lck
│   │   ├── ./.git/annex/sentinal
│   │   ├── ./.git/annex/sentinal.cache
│   │   ├── ./.git/annex/smudge.lck
│   │   └── ./.git/annex/smudge.log
│   ├── ./.git/branches
│   ├── ./.git/config
│   ├── ./.git/config.dataladlock
│   ├── ./.git/description
│   ├── ./.git/hooks
│   │   ├── ./.git/hooks/applypatch-msg.sample
│   │   ├── ./.git/hooks/commit-msg.sample
│   │   ├── ./.git/hooks/fsmonitor-watchman.sample
│   │   ├── ./.git/hooks/post-checkout
│   │   ├── ./.git/hooks/post-merge
│   │   ├── ./.git/hooks/post-receive
│   │   ├── ./.git/hooks/post-update.sample
│   │   ├── ./.git/hooks/pre-applypatch.sample
│   │   ├── ./.git/hooks/pre-commit
│   │   ├── ./.git/hooks/pre-commit.sample
│   │   ├── ./.git/hooks/pre-merge-commit.sample
│   │   ├── ./.git/hooks/pre-push.sample
│   │   ├── ./.git/hooks/pre-rebase.sample
│   │   ├── ./.git/hooks/pre-receive.sample
│   │   ├── ./.git/hooks/prepare-commit-msg.sample
│   │   ├── ./.git/hooks/push-to-checkout.sample
│   │   ├── ./.git/hooks/sendemail-validate.sample
│   │   └── ./.git/hooks/update.sample
│   ├── ./.git/index
│   ├── ./.git/info
│   │   ├── ./.git/info/attributes
│   │   └── ./.git/info/exclude
│   ├── ./.git/logs
│   │   ├── ./.git/logs/HEAD
│   │   └── ./.git/logs/refs
│   │       ├── ./.git/logs/refs/heads
│   │       │   ├── ./.git/logs/refs/heads/git-annex
│   │       │   └── ./.git/logs/refs/heads/main
│   │       └── ./.git/logs/refs/remotes
│   │           └── ./.git/logs/refs/remotes/origin
│   │               └── ./.git/logs/refs/remotes/origin/HEAD
│   ├── ./.git/objects
│   │   ├── ./.git/objects/1f
│   │   │   └── ./.git/objects/1f/0bd672157052c9887dcc3fb761b1159e2e9e58
│   │   ├── ./.git/objects/4b
│   │   │   └── ./.git/objects/4b/825dc642cb6eb9a060e54bf8d69288fbee4904
│   │   ├── ./.git/objects/59
│   │   │   └── ./.git/objects/59/28185081410e652fcb8b4446fae9caf43288e5
│   │   ├── ./.git/objects/5f
│   │   │   └── ./.git/objects/5f/84c346beb09d2eed54a14446d1b175c1857af0
│   │   ├── ./.git/objects/6b
│   │   │   └── ./.git/objects/6b/38ca9dfe215971a9e329782ea0c3fdbf8a31b6
│   │   ├── ./.git/objects/80
│   │   │   └── ./.git/objects/80/5770d083b03913ddcc30815b012d4a4fe91fd6
│   │   ├── ./.git/objects/82
│   │   │   └── ./.git/objects/82/ccb5d829f624f42c5994df967cec904ff2469c
│   │   ├── ./.git/objects/af
│   │   │   └── ./.git/objects/af/926ef0c359556ac1d36d71f7e173d97b893ff2
│   │   ├── ./.git/objects/c2
│   │   │   └── ./.git/objects/c2/c0ea6f3921fd03337a9592657eaa200a780e7b
│   │   ├── ./.git/objects/d4
│   │   │   └── ./.git/objects/d4/9f3251d735c3e3775dd1c930e0c3fa5e867869
│   │   ├── ./.git/objects/d5
│   │   │   └── ./.git/objects/d5/9574abe8fbaa810502881ab4ccfb738c1d705a
│   │   ├── ./.git/objects/d9
│   │   │   └── ./.git/objects/d9/9cd4afa8955f94d4fde4a3846e121d77a7c87d
│   │   ├── ./.git/objects/e6
│   │   │   └── ./.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391
│   │   ├── ./.git/objects/info
│   │   └── ./.git/objects/pack
│   │       ├── ./.git/objects/pack/pack-44261f52b16e2554a4dac2d17f6c4669e26c8bc1.idx
│   │       ├── ./.git/objects/pack/pack-44261f52b16e2554a4dac2d17f6c4669e26c8bc1.pack
│   │       └── ./.git/objects/pack/pack-44261f52b16e2554a4dac2d17f6c4669e26c8bc1.rev
│   ├── ./.git/packed-refs
│   └── ./.git/refs
│       ├── ./.git/refs/heads
│       │   ├── ./.git/refs/heads/git-annex
│       │   └── ./.git/refs/heads/main
│       ├── ./.git/refs/remotes
│       │   └── ./.git/refs/remotes/origin
│       │       └── ./.git/refs/remotes/origin/HEAD
│       └── ./.git/refs/tags
├── ./.gitattributes
├── ./.gitignore
├── ./README.md
├── ./__pycache__
│   └── ./__pycache__/config.cpython-311.pyc
├── ./algorithm_comparison.py
├── ./config.py
├── ./gather_all_py_to_txt.py
├── ./logs
│   └── ./logs/experiment_20250801_072306.log
├── ./main.py
├── ./quick_test.py
├── ./requirements.txt
├── ./results
│   ├── ./results/algorithm_comparison
│   ├── ./results/analysis
│   ├── ./results/topographies
│   ├── ./results/trajectories
│   └── ./results/videos
├── ./src
│   ├── ./src/__init__.py
│   ├── ./src/__pycache__
│   │   ├── ./src/__pycache__/__init__.cpython-311.pyc
│   │   ├── ./src/__pycache__/data_loader.cpython-311.pyc
│   │   ├── ./src/__pycache__/topography.cpython-311.pyc
│   │   ├── ./src/__pycache__/tracker.cpython-311.pyc
│   │   ├── ./src/__pycache__/trajectory_analysis.cpython-311.pyc
│   │   └── ./src/__pycache__/visualization.cpython-311.pyc
│   ├── ./src/data_loader.py
│   ├── ./src/topography.py
│   ├── ./src/trajectory_analysis.py
│   └── ./src/visualization.py
├── ./test_results
│   ├── ./test_results/comparison_charts_test.png
│   └── ./test_results/font_test.png
├── ./trackers
│   ├── ./trackers/__init__.py
│   ├── ./trackers/__pycache__
│   │   ├── ./trackers/__pycache__/__init__.cpython-311.pyc
│   │   ├── ./trackers/__pycache__/base_tracker.cpython-311.pyc
│   │   └── ./trackers/__pycache__/greedy_tracker.cpython-311.pyc
│   ├── ./trackers/base_tracker.py
│   ├── ./trackers/greedy_tracker.py
│   ├── ./trackers/hungarian_tracker.py
│   └── ./trackers/tracker_factory.py
└── ./usage_guide.md

51 directories, 99 files

# ===============================

# ========== algorithm_comparison.py ==========
# 相对路径: algorithm_comparison.py
# 在项目中的相对位置: ./algorithm_comparison.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EEG轨迹跟踪算法对比模块
系统性评估和对比不同的跟踪算法性能
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score
import time
import pandas as pd
from typing import Dict, List, Tuple, Optional
import logging

class TrackingAlgorithmComparison:
    """跟踪算法对比类"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
    def hungarian_matching(self, current_regions: List[Dict], 
                          tracked_centers: np.ndarray,
                          distance_threshold: float = 20.0) -> List[Tuple[int, int]]:
        """匈牙利算法匹配"""
        if not current_regions or len(tracked_centers) == 0:
            return []
        
        current_centers = np.array([r['center'] for r in current_regions])
        distances = cdist(tracked_centers, current_centers)
        
        # 使用匈牙利算法求解最优分配
        row_indices, col_indices = linear_sum_assignment(distances)
        
        matches = []
        for row_idx, col_idx in zip(row_indices, col_indices):
            if distances[row_idx, col_idx] < distance_threshold:
                matches.append((row_idx, col_idx))
        
        return matches
    
    def greedy_matching(self, current_regions: List[Dict], 
                       tracked_centers: np.ndarray,
                       distance_threshold: float = 20.0) -> List[Tuple[int, int]]:
        """贪婪算法匹配（当前使用的方法）"""
        if not current_regions or len(tracked_centers) == 0:
            return []
        
        current_centers = np.array([r['center'] for r in current_regions])
        distances = cdist(tracked_centers, current_centers)
        
        matches = []
        used_current = set()
        used_tracked = set()
        
        dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
        
        for tracked_idx, current_idx in zip(dist_indices[0], dist_indices[1]):
            if tracked_idx in used_tracked or current_idx in used_current:
                continue
            if distances[tracked_idx, current_idx] < distance_threshold:
                matches.append((tracked_idx, current_idx))
                used_tracked.add(tracked_idx)
                used_current.add(current_idx)
        
        return matches
    
    def kalman_prediction_matching(self, current_regions: List[Dict],
                                  tracked_regions: List,
                                  distance_threshold: float = 20.0) -> List[Tuple[int, int]]:
        """基于卡尔曼滤波预测的匹配"""
        if not current_regions or not tracked_regions:
            return []
        
        # 简化的卡尔曼预测：基于速度的线性预测
        predicted_centers = []
        for region in tracked_regions:
            trajectory = region.trajectory
            if len(trajectory) >= 2:
                # 计算速度
                velocity = np.array(trajectory[-1]) - np.array(trajectory[-2])
                # 预测下一个位置
                predicted_pos = np.array(trajectory[-1]) + velocity
                predicted_centers.append(predicted_pos)
            else:
                predicted_centers.append(trajectory[-1])
        
        if not predicted_centers:
            return []
        
        predicted_centers = np.array(predicted_centers)
        current_centers = np.array([r['center'] for r in current_regions])
        distances = cdist(predicted_centers, current_centers)
        
        # 使用匈牙利算法
        row_indices, col_indices = linear_sum_assignment(distances)
        
        matches = []
        for row_idx, col_idx in zip(row_indices, col_indices):
            if distances[row_idx, col_idx] < distance_threshold:
                matches.append((row_idx, col_idx))
        
        return matches
    
    def overlap_based_matching(self, current_regions: List[Dict],
                              previous_regions: List[Dict],
                              overlap_threshold: float = 0.3) -> List[Tuple[int, int]]:
        """基于区域重叠的匹配"""
        if not current_regions or not previous_regions:
            return []
        
        matches = []
        
        for i, curr_region in enumerate(current_regions):
            curr_mask = curr_region['mask']
            best_match = -1
            best_overlap = 0
            
            for j, prev_region in enumerate(previous_regions):
                prev_mask = prev_region['mask']
                
                # 计算重叠率
                intersection = np.sum(curr_mask & prev_mask)
                union = np.sum(curr_mask | prev_mask)
                
                if union > 0:
                    overlap = intersection / union
                    if overlap > overlap_threshold and overlap > best_overlap:
                        best_overlap = overlap
                        best_match = j
            
            if best_match >= 0:
                matches.append((best_match, i))
        
        return matches

class SimilarityAlgorithmComparison:
    """相似性算法对比类"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def dtw_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """DTW距离"""
        try:
            from fastdtw import fastdtw
            from scipy.spatial.distance import euclidean
            distance, _ = fastdtw(traj1, traj2, dist=euclidean)
            return distance
        except ImportError:
            return self.euclidean_distance(traj1, traj2)
    
    def euclidean_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """欧几里得距离（插值对齐）"""
        # 插值到相同长度
        target_length = min(len(traj1), len(traj2), 50)
        
        if len(traj1) < 2 or len(traj2) < 2:
            return np.linalg.norm(traj1[-1] - traj2[-1])
        
        from scipy.interpolate import interp1d
        
        # 插值
        t1 = np.linspace(0, 1, len(traj1))
        t2 = np.linspace(0, 1, len(traj2))
        t_new = np.linspace(0, 1, target_length)
        
        interp1_x = interp1d(t1, traj1[:, 0], kind='linear')
        interp1_y = interp1d(t1, traj1[:, 1], kind='linear')
        interp2_x = interp1d(t2, traj2[:, 0], kind='linear')
        interp2_y = interp1d(t2, traj2[:, 1], kind='linear')
        
        traj1_interp = np.column_stack([interp1_x(t_new), interp1_y(t_new)])
        traj2_interp = np.column_stack([interp2_x(t_new), interp2_y(t_new)])
        
        return np.linalg.norm(traj1_interp - traj2_interp)
    
    def frechet_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """简化的Fréchet距离"""
        # 这里实现简化版本，完整版本需要更复杂的算法
        return self.euclidean_distance(traj1, traj2)
    
    def hausdorff_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """Hausdorff距离"""
        # 计算有向Hausdorff距离
        def directed_hausdorff(X, Y):
            return max(min(np.linalg.norm(x - y) for y in Y) for x in X)
        
        return max(directed_hausdorff(traj1, traj2), directed_hausdorff(traj2, traj1))

class ClusteringAlgorithmComparison:
    """聚类算法对比类"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def compare_clustering_methods(self, features: np.ndarray, 
                                  true_labels: Optional[np.ndarray] = None) -> Dict:
        """对比不同聚类方法"""
        results = {}
        
        # 确定聚类数量范围
        n_samples = len(features)
        max_clusters = min(n_samples // 2, 8)
        
        if max_clusters < 2:
            return results
        
        # 1. K-means聚类
        for n_clusters in range(2, max_clusters + 1):
            try:
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                labels = kmeans.fit_predict(features)
                
                # 计算轮廓系数
                if len(np.unique(labels)) > 1:
                    silhouette = silhouette_score(features, labels)
                else:
                    silhouette = -1
                
                results[f'kmeans_k{n_clusters}'] = {
                    'labels': labels,
                    'silhouette_score': silhouette,
                    'n_clusters': n_clusters,
                    'method': 'kmeans'
                }
                
                # 如果有真实标签，计算ARI
                if true_labels is not None:
                    ari = adjusted_rand_score(true_labels, labels)
                    results[f'kmeans_k{n_clusters}']['ari'] = ari
                    
            except Exception as e:
                self.logger.warning(f"K-means with k={n_clusters} failed: {e}")
        
        # 2. 层次聚类
        for n_clusters in range(2, max_clusters + 1):
            try:
                agg_clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')
                labels = agg_clustering.fit_predict(features)
                
                if len(np.unique(labels)) > 1:
                    silhouette = silhouette_score(features, labels)
                else:
                    silhouette = -1
                
                results[f'hierarchical_k{n_clusters}'] = {
                    'labels': labels,
                    'silhouette_score': silhouette,
                    'n_clusters': n_clusters,
                    'method': 'hierarchical'
                }
                
                if true_labels is not None:
                    ari = adjusted_rand_score(true_labels, labels)
                    results[f'hierarchical_k{n_clusters}']['ari'] = ari
                    
            except Exception as e:
                self.logger.warning(f"Hierarchical clustering with k={n_clusters} failed: {e}")
        
        # 3. DBSCAN聚类
        eps_values = [0.3, 0.5, 0.7, 1.0]
        min_samples_values = [2, 3, 4]
        
        for eps in eps_values:
            for min_samples in min_samples_values:
                try:
                    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
                    labels = dbscan.fit_predict(features)
                    
                    n_clusters = len(np.unique(labels[labels >= 0]))
                    
                    if n_clusters > 1:
                        # 只对非噪声点计算轮廓系数
                        mask = labels >= 0
                        if np.sum(mask) > 1:
                            silhouette = silhouette_score(features[mask], labels[mask])
                        else:
                            silhouette = -1
                    else:
                        silhouette = -1
                    
                    results[f'dbscan_eps{eps}_min{min_samples}'] = {
                        'labels': labels,
                        'silhouette_score': silhouette,
                        'n_clusters': n_clusters,
                        'eps': eps,
                        'min_samples': min_samples,
                        'method': 'dbscan',
                        'n_noise': np.sum(labels == -1)
                    }
                    
                    if true_labels is not None:
                        ari = adjusted_rand_score(true_labels, labels)
                        results[f'dbscan_eps{eps}_min{min_samples}']['ari'] = ari
                        
                except Exception as e:
                    self.logger.warning(f"DBSCAN (eps={eps}, min_samples={min_samples}) failed: {e}")
        
        return results

class AlgorithmBenchmark:
    """算法性能基准测试"""
    
    def __init__(self, config):
        self.config = config
        self.tracking_comparison = TrackingAlgorithmComparison(config)
        self.similarity_comparison = SimilarityAlgorithmComparison()
        self.clustering_comparison = ClusteringAlgorithmComparison()
        self.logger = logging.getLogger(__name__)
    
    def benchmark_tracking_algorithms(self, topographies: np.ndarray) -> Dict:
        """基准测试跟踪算法"""
        results = {
            'hungarian': {'execution_times': [], 'match_counts': []},
            'greedy': {'execution_times': [], 'match_counts': []},
            'kalman': {'execution_times': [], 'match_counts': []},
            'overlap': {'execution_times': [], 'match_counts': []}
        }
        
        self.logger.info("开始跟踪算法基准测试...")
        
        # 模拟跟踪过程
        for frame_idx in range(1, min(50, topographies.shape[0])):  # 限制帧数以节省时间
            # 检测当前帧区域（使用简化的检测逻辑）
            current_topo = topographies[frame_idx]
            prev_topo = topographies[frame_idx - 1]
            
            # 简化的区域检测
            threshold = np.percentile(current_topo[current_topo > 0], 90)
            current_binary = current_topo > threshold
            
            threshold_prev = np.percentile(prev_topo[prev_topo > 0], 90)
            prev_binary = prev_topo > threshold_prev
            
            # 模拟区域
            current_regions = [{'center': (50, 50), 'mask': current_binary}]
            prev_regions = [{'center': (48, 52), 'mask': prev_binary}]
            tracked_centers = np.array([[48, 52]])
            
            # 测试不同算法
            algorithms = {
                'hungarian': lambda: self.tracking_comparison.hungarian_matching(
                    current_regions, tracked_centers),
                'greedy': lambda: self.tracking_comparison.greedy_matching(
                    current_regions, tracked_centers),
                'overlap': lambda: self.tracking_comparison.overlap_based_matching(
                    current_regions, prev_regions)
            }
            
            for alg_name, alg_func in algorithms.items():
                try:
                    start_time = time.time()
                    matches = alg_func()
                    end_time = time.time()
                    
                    results[alg_name]['execution_times'].append(end_time - start_time)
                    results[alg_name]['match_counts'].append(len(matches))
                except Exception as e:
                    self.logger.warning(f"Algorithm {alg_name} failed on frame {frame_idx}: {e}")
        
        return results
    
    def benchmark_similarity_algorithms(self, trajectories: Dict) -> Dict:
        """基准测试相似性算法"""
        if len(trajectories) < 2:
            return {}
        
        results = {
            'dtw': {'execution_times': [], 'distances': []},
            'euclidean': {'execution_times': [], 'distances': []},
            'hausdorff': {'execution_times': [], 'distances': []}
        }
        
        trajectory_list = list(trajectories.values())
        
        self.logger.info("开始相似性算法基准测试...")
        
        # 比较所有轨迹对
        for i in range(min(10, len(trajectory_list))):  # 限制比较数量
            for j in range(i + 1, min(10, len(trajectory_list))):
                traj1 = trajectory_list[i]['trajectory']
                traj2 = trajectory_list[j]['trajectory']
                
                if len(traj1) < 2 or len(traj2) < 2:
                    continue
                
                # 测试不同相似性算法
                algorithms = {
                    'dtw': self.similarity_comparison.dtw_distance,
                    'euclidean': self.similarity_comparison.euclidean_distance,
                    'hausdorff': self.similarity_comparison.hausdorff_distance
                }
                
                for alg_name, alg_func in algorithms.items():
                    try:
                        start_time = time.time()
                        distance = alg_func(traj1, traj2)
                        end_time = time.time()
                        
                        results[alg_name]['execution_times'].append(end_time - start_time)
                        results[alg_name]['distances'].append(distance)
                    except Exception as e:
                        self.logger.warning(f"Similarity algorithm {alg_name} failed: {e}")
        
        return results
    
    def generate_comparison_report(self, tracking_results: Dict, 
                                 similarity_results: Dict,
                                 clustering_results: Dict) -> str:
        """生成算法对比报告"""
        report = []
        report.append("=" * 60)
        report.append("EEG轨迹跟踪算法性能对比报告")
        report.append("=" * 60)
        report.append("")
        
        # 跟踪算法对比
        if tracking_results:
            report.append("1. 跟踪算法性能对比")
            report.append("-" * 30)
            
            for alg_name, metrics in tracking_results.items():
                if metrics['execution_times']:
                    avg_time = np.mean(metrics['execution_times'])
                    avg_matches = np.mean(metrics['match_counts'])
                    
                    report.append(f"{alg_name.upper()}算法:")
                    report.append(f"  平均执行时间: {avg_time*1000:.3f} ms")
                    report.append(f"  平均匹配数量: {avg_matches:.1f}")
                    report.append("")
        
        # 相似性算法对比
        if similarity_results:
            report.append("2. 相似性算法性能对比")
            report.append("-" * 30)
            
            for alg_name, metrics in similarity_results.items():
                if metrics['execution_times']:
                    avg_time = np.mean(metrics['execution_times'])
                    avg_distance = np.mean(metrics['distances'])
                    
                    report.append(f"{alg_name.upper()}算法:")
                    report.append(f"  平均执行时间: {avg_time*1000:.3f} ms")
                    report.append(f"  平均距离: {avg_distance:.3f}")
                    report.append("")
        
        # 聚类算法对比
        if clustering_results:
            report.append("3. 聚类算法性能对比")
            report.append("-" * 30)
            
            best_method = None
            best_score = -1
            
            for method_name, result in clustering_results.items():
                silhouette = result.get('silhouette_score', -1)
                if silhouette > best_score:
                    best_score = silhouette
                    best_method = method_name
                
                report.append(f"{method_name}:")
                report.append(f"  轮廓系数: {silhouette:.3f}")
                report.append(f"  聚类数量: {result.get('n_clusters', 0)}")
                
                if 'ari' in result:
                    report.append(f"  调整兰德指数: {result['ari']:.3f}")
                
                report.append("")
            
            if best_method:
                report.append(f"推荐聚类方法: {best_method} (轮廓系数: {best_score:.3f})")
                report.append("")
        
        report.append("4. 算法选择建议")
        report.append("-" * 30)
        report.append("• 实时应用: 选择执行时间最短的算法")
        report.append("• 精度优先: 选择匹配质量最好的算法")
        report.append("• 平衡方案: 在性能和精度间找到最佳平衡")
        report.append("")
        
        return "\n".join(report)
    
    def visualize_comparison_results(self, results: Dict, save_path: str):
        """可视化对比结果"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('算法性能对比结果', fontsize=16, fontweight='bold')
        
        # 跟踪算法执行时间对比
        if 'tracking' in results:
            ax = axes[0, 0]
            tracking_data = results['tracking']
            
            alg_names = []
            avg_times = []
            
            for alg_name, metrics in tracking_data.items():
                if metrics['execution_times']:
                    alg_names.append(alg_name)
                    avg_times.append(np.mean(metrics['execution_times']) * 1000)
            
            if alg_names:
                bars = ax.bar(alg_names, avg_times, color='skyblue', alpha=0.7)
                ax.set_title('跟踪算法执行时间对比')
                ax.set_ylabel('平均执行时间 (ms)')
                ax.tick_params(axis='x', rotation=45)
                
                # 添加数值标签
                for bar, time in zip(bars, avg_times):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{time:.2f}', ha='center', va='bottom')
        
        # 相似性算法对比
        if 'similarity' in results:
            ax = axes[0, 1]
            similarity_data = results['similarity']
            
            alg_names = []
            avg_times = []
            
            for alg_name, metrics in similarity_data.items():
                if metrics['execution_times']:
                    alg_names.append(alg_name)
                    avg_times.append(np.mean(metrics['execution_times']) * 1000)
            
            if alg_names:
                bars = ax.bar(alg_names, avg_times, color='lightgreen', alpha=0.7)
                ax.set_title('相似性算法执行时间对比')
                ax.set_ylabel('平均执行时间 (ms)')
                ax.tick_params(axis='x', rotation=45)
                
                for bar, time in zip(bars, avg_times):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{time:.2f}', ha='center', va='bottom')
        
        # 聚类算法轮廓系数对比
        if 'clustering' in results:
            ax = axes[1, 0]
            clustering_data = results['clustering']
            
            methods = []
            scores = []
            
            for method_name, result in clustering_data.items():
                if result.get('silhouette_score', -1) > -1:
                    methods.append(method_name.replace('_', '\n'))
                    scores.append(result['silhouette_score'])
            
            if methods:
                bars = ax.bar(methods, scores, color='orange', alpha=0.7)
                ax.set_title('聚类算法轮廓系数对比')
                ax.set_ylabel('轮廓系数')
                ax.tick_params(axis='x', rotation=45)
                
                for bar, score in zip(bars, scores):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{score:.3f}', ha='center', va='bottom')
        
        # 综合性能雷达图
        ax = axes[1, 1]
        ax.text(0.5, 0.5, '综合性能评估\n(基于执行时间、精度、稳定性)', 
                ha='center', va='center', fontsize=12,
                bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.5))
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"算法对比可视化结果已保存: {save_path}")

def run_algorithm_comparison(config, topographies, trajectories):
    """运行完整的算法对比"""
    benchmark = AlgorithmBenchmark(config)
    
    print("开始算法性能对比测试...")
    
    # 1. 跟踪算法对比
    tracking_results = benchmark.benchmark_tracking_algorithms(topographies)
    
    # 2. 相似性算法对比
    similarity_results = benchmark.benchmark_similarity_algorithms(trajectories)
    
    # 3. 聚类算法对比（需要特征数据）
    if trajectories:
        from src.trajectory_analysis import TrajectoryAnalyzer
        analyzer = TrajectoryAnalyzer(config)
        
        # 提取特征
        feature_data = {}
        for traj_id, traj_data in trajectories.items():
            features = analyzer.compute_trajectory_features(traj_data['trajectory'])
            if features:
                feature_data[traj_id] = features
        
        if feature_data:
            # 转换为特征矩阵
            feature_matrix = []
            for features in feature_data.values():
                feature_vector = [
                    features.get('total_distance', 0),
                    features.get('displacement', 0),
                    features.get('mean_velocity', 0),
                    features.get('tortuosity', 1),
                    features.get('straightness', 0),
                    features.get('complexity', 0)
                ]
                feature_matrix.append(feature_vector)
            
            feature_matrix = np.array(feature_matrix)
            
            # 标准化
            from sklearn.preprocessing import StandardScaler
            scaler = StandardScaler()
            feature_matrix = scaler.fit_transform(feature_matrix)
            
            clustering_results = benchmark.clustering_comparison.compare_clustering_methods(feature_matrix)
        else:
            clustering_results = {}
    else:
        clustering_results = {}
    
    # 生成报告
    results = {
        'tracking': tracking_results,
        'similarity': similarity_results,
        'clustering': clustering_results
    }
    
    report = benchmark.generate_comparison_report(
        tracking_results, similarity_results, clustering_results
    )
    
    # 保存报告
    import os
    report_path = os.path.join(config.RESULTS_ROOT, "algorithm_comparison_report.txt")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # 可视化结果
    viz_path = os.path.join(config.RESULTS_ROOT, "algorithm_comparison.png")
    benchmark.visualize_comparison_results(results, viz_path)
    
    print(f"算法对比报告已保存: {report_path}")
    print("\n" + "="*50)
    print("算法对比报告预览:")
    print("="*50)
    print(report[:1000] + "..." if len(report) > 1000 else report)
    
    return results, report

# ========== config.py ==========
# 相对路径: config.py
# 在项目中的相对位置: ./config.py

import os
import numpy as np

class Config:
    # 数据路径配置
    DATA_ROOT = "../data/ds005262"
    RESULTS_ROOT = "./results"
    LOGS_ROOT = "./logs"
    
    # 确保目录存在
    for path in [RESULTS_ROOT, LOGS_ROOT, 
                 os.path.join(RESULTS_ROOT, "topographies"),
                 os.path.join(RESULTS_ROOT, "trajectories"),
                 os.path.join(RESULTS_ROOT, "analysis"),
                 os.path.join(RESULTS_ROOT, "videos"),
                 os.path.join(RESULTS_ROOT, "algorithm_comparison")]:  # 新增
        os.makedirs(path, exist_ok=True)
    
    # EEG数据处理参数
    SAMPLING_RATE = 500  
    LOW_FREQ = 1.0       
    HIGH_FREQ = 50.0     
    
    # 地形图生成参数
    TOPO_SIZE = (128, 128)
    INTERPOLATION_METHOD = 'cubic'
    
    # 实验规模配置 - 支持所有被试
    MAX_SUBJECTS = 12              # 处理所有12个被试
    MAX_EPOCHS_PER_SUBJECT = 3     # 每个被试处理3个epoch
    MAX_SESSIONS_PER_SUBJECT = 5   # 每个被试最多处理5个session
    MEMORY_LIMIT_MB = 4096         # 增加内存限制
    
    # 算法对比配置
    ENABLE_ALGORITHM_COMPARISON = True    # 启用算法对比
    COMPARISON_ALGORITHMS = [             # 要对比的算法
        'greedy',           # 贪婪匹配（原默认算法）
        'hungarian',        # 匈牙利算法
        'kalman',          # 卡尔曼预测
        'overlap',         # 重叠度匹配
        'hybrid'           # 混合算法
    ]
    
    # 目标跟踪参数 - 保持原有设置
    THRESHOLD_PERCENTILE = 88
    MIN_REGION_SIZE = 25       
    MAX_REGIONS = 6            
    
    # 轨迹分析参数
    TIME_WINDOW = 2.0          
    TRAJECTORY_SMOOTH_FACTOR = 3  
    
    # 可视化参数
    COLORMAP = 'RdYlBu_r'      
    FPS = 10                   
    DPI = 150                  
    
    # 各算法的具体参数配置
    ALGORITHM_CONFIGS = {
        'greedy': {
            'distance_threshold': 25.0,
            'enable_prediction': False,
            'enable_reconnection': True,
            'max_inactive_frames': 25,
            'description': '贪婪匹配算法 - 快速局部最优'
        },
        'hungarian': {
            'distance_threshold': 25.0,
            'enable_prediction': False,
            'enable_reconnection': True,
            'max_inactive_frames': 25,
            'description': '匈牙利算法 - 全局最优匹配'
        },
        'kalman': {
            'distance_threshold': 30.0,
            'enable_prediction': True,
            'prediction_weight': 0.4,
            'enable_reconnection': True,
            'max_inactive_frames': 30,
            'description': '卡尔曼预测算法 - 基于运动预测'
        },
        'overlap': {
            'overlap_threshold': 0.3,
            'distance_threshold': 35.0,
            'enable_reconnection': True,
            'max_inactive_frames': 20,
            'description': '重叠度匹配 - 基于区域重叠'
        },
        'hybrid': {
            'distance_threshold': 25.0,
            'overlap_weight': 0.4,
            'intensity_weight': 0.1,
            'area_weight': 0.1,
            'enable_prediction': True,
            'enable_reconnection': True,
            'max_inactive_frames': 30,
            'description': '混合算法 - 综合多种特征'
        }
    }
    
    # 性能评估指标
    EVALUATION_METRICS = [
        'trajectory_count',           # 轨迹数量
        'average_trajectory_length',  # 平均轨迹长度
        'max_trajectory_length',     # 最大轨迹长度
        'tracking_continuity',       # 跟踪连续性
        'trajectory_smoothness',     # 轨迹平滑度
        'computation_time',          # 计算时间
        'memory_usage',             # 内存使用
        'detection_stability',      # 检测稳定性
        'trajectory_quality'        # 轨迹质量
    ]
    
    # 可视化配置
    VISUALIZATION_CONFIG = {
        'generate_comparison_plots': True,     # 生成算法对比图
        'generate_heatmaps': True,            # 生成性能热图
        'generate_trajectory_overlays': True,  # 生成轨迹叠加图
        'generate_detailed_reports': True,     # 生成详细报告
        'save_individual_results': True,      # 保存各算法单独结果
        'create_summary_animations': False    # 创建对比动画（耗时）
    }
    
    # 保持原有的跟踪优化参数
    TRACKING_OPTIMIZATION = {
        'enable_adaptive_threshold': True,     
        'enable_prediction': True,             
        'enable_reconnection': True,           
        'base_distance_threshold': 25.0,      
        'max_distance_threshold': 60.0,       
        'reconnection_distance': 40.0,        
        'max_inactive_frames': 25,            
        'prediction_weight': 0.3,             
        'quality_threshold': 0.2,             
        'min_trajectory_length': 3            
    }
    
    # 保持原有的可视化优化参数
    VISUALIZATION_OPTIMIZATION = {
        'auto_font_detection': True,          
        'fallback_to_english': True,         
        'max_animation_frames': 300,         
        'save_frame_sequence_fallback': True, 
        'trajectory_alpha': 0.8,             
        'show_direction_arrows': True,       
        'legend_max_items': 10               
    }
    
    # 标准电极位置 (保持原有)
    ELECTRODE_POSITIONS = {
        'Fp1': (-0.3, 0.85), 'Fp2': (0.3, 0.85), 'Fpz': (0, 0.9),
        'F7': (-0.7, 0.4), 'F3': (-0.4, 0.4), 'Fz': (0, 0.4), 'F4': (0.4, 0.4), 'F8': (0.7, 0.4),
        'FC5': (-0.5, 0.2), 'FC1': (-0.2, 0.2), 'FCz': (0, 0.2), 'FC2': (0.2, 0.2), 'FC6': (0.5, 0.2),
        'T7': (-0.85, 0), 'C3': (-0.4, 0), 'Cz': (0, 0), 'C4': (0.4, 0), 'T8': (0.85, 0),
        'CP5': (-0.5, -0.2), 'CP1': (-0.2, -0.2), 'CPz': (0, -0.2), 'CP2': (0.2, -0.2), 'CP6': (0.5, -0.2),
        'P7': (-0.7, -0.4), 'P3': (-0.4, -0.4), 'Pz': (0, -0.4), 'P4': (0.4, -0.4), 'P8': (0.7, -0.4),
        'PO9': (-0.8, -0.65), 'PO7': (-0.6, -0.65), 'PO3': (-0.25, -0.65), 'POz': (0, -0.65), 
        'PO4': (0.25, -0.65), 'PO8': (0.6, -0.65), 'PO10': (0.8, -0.65),
        'O1': (-0.3, -0.85), 'Oz': (0, -0.9), 'O2': (0.3, -0.85),
        
        # 额外电极
        'AF7': (-0.5, 0.65), 'AF3': (-0.25, 0.65), 'AFz': (0, 0.65), 'AF4': (0.25, 0.65), 'AF8': (0.5, 0.65),
        'F5': (-0.55, 0.4), 'F1': (-0.2, 0.4), 'F2': (0.2, 0.4), 'F6': (0.55, 0.4),
        'FT9': (-0.9, 0.2), 'FT7': (-0.75, 0.2), 'FT8': (0.75, 0.2), 'FT10': (0.9, 0.2),
        'C5': (-0.55, 0), 'C1': (-0.2, 0), 'C2': (0.2, 0), 'C6': (0.55, 0),
        'TP9': (-0.9, -0.2), 'TP7': (-0.75, -0.2), 'TP8': (0.75, -0.2), 'TP10': (0.9, -0.2),
        'P5': (-0.55, -0.4), 'P1': (-0.2, -0.4), 'P2': (0.2, -0.4), 'P6': (0.55, -0.4),
        
        # 大小写变体
        'FP1': (-0.3, 0.85), 'FP2': (0.3, 0.85),
        'fp1': (-0.3, 0.85), 'fp2': (0.3, 0.85), 'fpz': (0, 0.9),
        'f7': (-0.7, 0.4), 'f3': (-0.4, 0.4), 'fz': (0, 0.4), 'f4': (0.4, 0.4), 'f8': (0.7, 0.4),
        't7': (-0.85, 0), 'c3': (-0.4, 0), 'cz': (0, 0), 'c4': (0.4, 0), 't8': (0.85, 0),
        'p7': (-0.7, -0.4), 'p3': (-0.4, -0.4), 'pz': (0, -0.4), 'p4': (0.4, -0.4), 'p8': (0.7, -0.4),
        'o1': (-0.3, -0.85), 'oz': (0, -0.9), 'o2': (0.3, -0.85),
    }
    
    # 保持原有方法
    @staticmethod
    def get_default_electrode_position(ch_name: str, n_channels: int, ch_index: int):
        """为未知电极生成默认位置"""
        angle = 2 * np.pi * ch_index / n_channels
        radius = 0.7
        x = radius * np.cos(angle)
        y = radius * np.sin(angle)
        return (x, y)
    
    # 新增方法
    @classmethod
    def get_algorithm_config(cls, algorithm_name: str):
        """获取特定算法的配置"""
        return cls.ALGORITHM_CONFIGS.get(algorithm_name, cls.ALGORITHM_CONFIGS['greedy'])
    
    @classmethod
    def get_experiment_summary(cls):
        """获取实验配置摘要"""
        return {
            'total_subjects': cls.MAX_SUBJECTS,
            'algorithms_count': len(cls.COMPARISON_ALGORITHMS),
            'algorithm_names': cls.COMPARISON_ALGORITHMS,
            'metrics_count': len(cls.EVALUATION_METRICS),
            'max_epochs_per_subject': cls.MAX_EPOCHS_PER_SUBJECT,
            'max_sessions_per_subject': cls.MAX_SESSIONS_PER_SUBJECT,
            'algorithm_comparison_enabled': cls.ENABLE_ALGORITHM_COMPARISON
        }
    
    # 保持原有的自动调整参数方法
    @classmethod
    def auto_adjust_parameters(cls, data_characteristics: dict):
        """根据数据特征自动调整参数"""
        if 'signal_strength' in data_characteristics:
            signal_strength = data_characteristics['signal_strength']
            
            if signal_strength < 0.3:  # 弱信号
                cls.THRESHOLD_PERCENTILE = 85
                cls.MIN_REGION_SIZE = 15
                print("✓ 检测到弱信号，已调整为高敏感性参数")
                
            elif signal_strength > 0.8:  # 强信号
                cls.THRESHOLD_PERCENTILE = 92
                cls.MIN_REGION_SIZE = 40
                print("✓ 检测到强信号，已调整为高精度参数")
        
        if 'noise_level' in data_characteristics:
            noise_level = data_characteristics['noise_level']
            
            if noise_level > 0.6:  # 高噪声
                cls.TRAJECTORY_SMOOTH_FACTOR = 5
                print("✓ 检测到高噪声，已启用强平滑")
            elif noise_level < 0.2:  # 低噪声
                cls.TRAJECTORY_SMOOTH_FACTOR = 2
                print("✓ 检测到低噪声，已启用精细追踪")
    
    # 保持原有的配置摘要方法
    @classmethod
    def get_config_summary(cls):
        """获取当前配置摘要"""
        summary = {
            'detection_sensitivity': 'High' if cls.THRESHOLD_PERCENTILE < 90 else 'Medium' if cls.THRESHOLD_PERCENTILE < 95 else 'Low',
            'tracking_aggressiveness': 'High' if cls.TRACKING_OPTIMIZATION['base_distance_threshold'] > 25 else 'Medium',
            'quality_filter': 'Strict' if cls.TRACKING_OPTIMIZATION['quality_threshold'] > 0.25 else 'Lenient',
            'smoothing_level': 'High' if cls.TRAJECTORY_SMOOTH_FACTOR > 4 else 'Medium' if cls.TRAJECTORY_SMOOTH_FACTOR > 2 else 'Low',
            'algorithm_comparison': 'Enabled' if cls.ENABLE_ALGORITHM_COMPARISON else 'Disabled',
            'algorithms_to_compare': len(cls.COMPARISON_ALGORITHMS)
        }
        return summary

# ========== main.py ==========
# 相对路径: main.py
# 在项目中的相对位置: ./main.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EEG脑电地形图运动轨迹分析主程序 - 算法对比增强版
集成多种跟踪算法对比功能
版本: 3.0.0 - 算法对比版
更新时间: 2025-08-01
"""

import os
import sys
import logging
import json
import pickle
import numpy as np
import gc
import argparse
import platform
from datetime import datetime
from tqdm import tqdm
import warnings
import time

# 抑制警告
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# 添加src到路径
sys.path.append('src')
sys.path.append('trackers')

# 字体配置 - 保持原有设置
def setup_matplotlib_font():
    """配置matplotlib字体"""
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    
    try:
        fm._rebuild()
    except:
        pass
    
    system = platform.system()
    use_chinese = False
    
    chinese_fonts = []
    if system == "Windows":
        chinese_fonts = ['Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi']
    elif system == "Darwin":  # macOS
        chinese_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti']
    elif system == "Linux":
        chinese_fonts = ['Noto Sans CJK SC', 'WenQuanYi Micro Hei', 'Droid Sans Fallback']
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in chinese_fonts:
        if font in available_fonts:
            try:
                plt.rcParams['font.sans-serif'] = [font] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
                
                fig, ax = plt.subplots(figsize=(1, 1))
                ax.text(0.5, 0.5, '测试', ha='center', va='center')
                plt.close(fig)
                use_chinese = True
                print(f"✓ 字体配置成功: {font}")
                break
            except:
                continue
    
    if not use_chinese:
        print("⚠️  使用英文标签模式")
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    return use_chinese

# 设置字体
USE_CHINESE = setup_matplotlib_font()

from config import Config
from src import EEGDataLoader, TopographyGenerator, TrajectoryAnalyzer, Visualizer
from trackers import TrackerFactory

def get_label(key, chinese_text, english_text):
    """获取标签文本"""
    return chinese_text if USE_CHINESE else english_text

def setup_logging():
    """设置日志系统"""
    log_dir = Config.LOGS_ROOT
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f"experiment_{timestamp}.log")
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"日志系统已初始化，日志文件: {log_file}")
    
    return logger

def check_dependencies():
    """检查必要的依赖库"""
    required_packages = {
        'mne': 'MNE-Python',
        'numpy': 'NumPy',
        'scipy': 'SciPy',
        'matplotlib': 'Matplotlib',
        'sklearn': 'Scikit-learn',
        'cv2': 'OpenCV',
        'tqdm': 'tqdm'
    }
    
    missing_packages = []
    
    for package, name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(name)
    
    if missing_packages:
        print("❌ 缺少以下必要的依赖库:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\n请使用以下命令安装:")
        print("pip install -r requirements.txt")
        return False
    
    return True

def print_system_info():
    """打印系统信息"""
    print("=" * 70)
    title = get_label('title', 'EEG脑电地形图运动轨迹分析系统 - 算法对比版', 
                     'EEG Topography Motion Trajectory Analysis System - Algorithm Comparison Edition')
    print(title)
    print("=" * 70)
    print(f"Python版本: {platform.python_version()}")
    print(f"操作系统: {platform.system()} {platform.release()}")
    print(f"处理器: {platform.machine()}")
    print(f"字体支持: {'中文' if USE_CHINESE else 'English Only'}")
    
    # 显示实验配置
    summary = Config.get_experiment_summary()
    print(f"\n实验配置:")
    print(f"  被试数量: {summary['total_subjects']}")
    print(f"  对比算法数量: {summary['algorithms_count']}")
    print(f"  算法列表: {', '.join(summary['algorithm_names'])}")
    print(f"  评估指标数量: {summary['metrics_count']}")
    print(f"  算法对比: {'启用' if summary['algorithm_comparison_enabled'] else '禁用'}")
    
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"  总内存: {memory.total / (1024**3):.1f} GB")
        print(f"  可用内存: {memory.available / (1024**3):.1f} GB")
    except ImportError:
        pass
    
    print("=" * 70)

def validate_config():
    """验证配置参数"""
    logger = logging.getLogger(__name__)
    
    # 检查数据目录
    if not os.path.exists(Config.DATA_ROOT):
        error_msg = get_label('data_error', 
                             f"数据目录不存在: {Config.DATA_ROOT}",
                             f"Data directory not found: {Config.DATA_ROOT}")
        logger.error(error_msg)
        print(f"\n❌ {error_msg}")
        print(get_label('check_config', 
                       "请检查config.py中的DATA_ROOT设置",
                       "Please check DATA_ROOT setting in config.py"))
        return False
    
    # 验证算法配置
    validation_results = TrackerFactory.validate_algorithm_config(Config)
    invalid_algorithms = [alg for alg, valid in validation_results.items() if not valid]
    
    if invalid_algorithms:
        logger.warning(f"以下算法配置无效: {invalid_algorithms}")
        print(f"⚠️  以下算法配置可能有问题: {', '.join(invalid_algorithms)}")
    
    # 检查可用算法
    available = TrackerFactory.get_available_algorithms()
    missing = [alg for alg in Config.COMPARISON_ALGORITHMS if alg not in available]
    
    if missing:
        logger.error(f"以下算法不可用: {missing}")
        print(f"❌ 以下算法不可用: {', '.join(missing)}")
        return False
    
    logger.info("配置验证完成")
    return True

def process_subject_with_multiple_algorithms(data_loader, topo_generator, analyzer, visualizer,
                                           subject_id, sessions, logger):
    """使用多种算法处理单个被试的数据"""
    subject_results = {}
    
    session_label = get_label('session_process', 
                             f"处理被试 {subject_id} (共{len(sessions)}个session, {len(Config.COMPARISON_ALGORITHMS)}种算法)",
                             f"Processing subject {subject_id} ({len(sessions)} sessions, {len(Config.COMPARISON_ALGORITHMS)} algorithms)")
    logger.info(session_label)
    
    # 创建所有跟踪器
    trackers = TrackerFactory.create_all_trackers(Config)
    if not trackers:
        logger.error(f"无法创建跟踪器")
        return None
    
    logger.info(f"成功创建 {len(trackers)} 个跟踪器: {', '.join(trackers.keys())}")
    
    for session_id, session_data in sessions.items():
        session_key = f"{subject_id}_{session_id}"
        session_info = get_label('session_info', 
                                f"  处理session {session_id}",
                                f"  Processing session {session_id}")
        logger.info(session_info)
        
        try:
            epochs = session_data['epochs']
            positions = session_data['positions']
            ch_names = epochs.ch_names
            
            # 选择多个epoch进行分析
            n_epochs_to_analyze = min(len(epochs), Config.MAX_EPOCHS_PER_SUBJECT)
            
            session_algorithm_results = {}
            
            for epoch_idx in range(n_epochs_to_analyze):
                try:
                    epoch_data = epochs.get_data()[epoch_idx]
                    
                    # 生成地形图序列
                    epoch_info = get_label('epoch_topo', 
                                          f"    生成epoch {epoch_idx+1} 地形图序列...",
                                          f"    Generating epoch {epoch_idx+1} topographies...")
                    logger.info(epoch_info)
                    
                    # 限制时间点数量以提高效率
                    max_time_points = min(epoch_data.shape[1], 1000)
                    epoch_data_subset = epoch_data[:, :max_time_points]
                    
                    topographies = topo_generator.generate_time_series_topographies(
                        epoch_data_subset[np.newaxis, :, :], positions, ch_names
                    )[0]
                    
                    if topographies is None or topographies.size == 0:
                        logger.warning(f"    Epoch {epoch_idx+1}: 地形图生成失败")
                        continue
                    
                    # 标准化地形图
                    for t in range(topographies.shape[0]):
                        topographies[t] = topo_generator.normalize_topography(topographies[t])
                    
                    # 使用每种算法进行轨迹跟踪
                    epoch_algorithm_results = {}
                    
                    for algorithm_name, tracker in trackers.items():
                        try:
                            track_info = get_label('epoch_track',
                                                  f"    使用{algorithm_name}算法跟踪epoch {epoch_idx+1}...",
                                                  f"    Tracking epoch {epoch_idx+1} with {algorithm_name}...")
                            logger.info(track_info)
                            
                            start_time = time.time()
                            tracking_results = tracker.track_sequence(topographies)
                            end_time = time.time()
                            
                            if not tracking_results or 'trajectories' not in tracking_results:
                                logger.warning(f"    {algorithm_name}: Epoch {epoch_idx+1} 轨迹跟踪返回空结果")
                                continue
                            
                            trajectories = tracking_results['trajectories']
                            if not trajectories:
                                logger.warning(f"    {algorithm_name}: Epoch {epoch_idx+1} 未检测到有效轨迹")
                                continue
                            
                            # 记录结果
                            epoch_algorithm_results[algorithm_name] = {
                                'trajectories': trajectories,
                                'metrics': tracking_results.get('metrics', {}),
                                'summary': tracking_results.get('summary', {}),
                                'computation_time': end_time - start_time
                            }
                            
                            found_info = get_label('found_traj',
                                                  f"    {algorithm_name}: Epoch {epoch_idx+1} 找到 {len(trajectories)} 条轨迹 "
                                                  f"(耗时 {end_time - start_time:.2f}s)",
                                                  f"    {algorithm_name}: Epoch {epoch_idx+1} found {len(trajectories)} trajectories "
                                                  f"(time: {end_time - start_time:.2f}s)")
                            logger.info(found_info)
                            
                        except Exception as e:
                            logger.error(f"    {algorithm_name}: Epoch {epoch_idx+1} 轨迹跟踪失败: {e}")
                            continue
                    
                    # 如果有结果，保存epoch级别的对比
                    if epoch_algorithm_results:
                        # 保存每种算法的代表性可视化
                        for algorithm_name, results in epoch_algorithm_results.items():
                            trajectories = results['trajectories']
                            
                            # 保存轨迹图
                            traj_path = os.path.join(Config.RESULTS_ROOT, "trajectories", 
                                                   f"{session_key}_epoch{epoch_idx}_{algorithm_name}_trajectories.png")
                            try:
                                title = get_label('traj_title',
                                                f"被试{subject_id} Session{session_id} Epoch{epoch_idx} - {algorithm_name}算法",
                                                f"Subject {subject_id} Session {session_id} Epoch {epoch_idx} - {algorithm_name} Algorithm")
                                visualizer.plot_trajectories(
                                    trajectories, topographies.shape[1:],
                                    title=title,
                                    save_path=traj_path
                                )
                            except Exception as e:
                                logger.warning(f"保存{algorithm_name}轨迹图失败: {e}")
                        
                        # 将epoch结果合并到session结果中
                        for algorithm_name, results in epoch_algorithm_results.items():
                            if algorithm_name not in session_algorithm_results:
                                session_algorithm_results[algorithm_name] = {
                                    'trajectories': {},
                                    'total_computation_time': 0,
                                    'epoch_count': 0,
                                    'metrics_sum': {}
                                }
                            
                            # 合并轨迹（添加epoch前缀）
                            for traj_id, traj_data in results['trajectories'].items():
                                key = f"epoch{epoch_idx}_{traj_id}"
                                session_algorithm_results[algorithm_name]['trajectories'][key] = traj_data
                            
                            # 累计统计
                            session_algorithm_results[algorithm_name]['total_computation_time'] += results['computation_time']
                            session_algorithm_results[algorithm_name]['epoch_count'] += 1
                            
                            # 累计指标
                            for metric, value in results.get('metrics', {}).items():
                                if metric not in session_algorithm_results[algorithm_name]['metrics_sum']:
                                    session_algorithm_results[algorithm_name]['metrics_sum'][metric] = []
                                session_algorithm_results[algorithm_name]['metrics_sum'][metric].append(value)
                    
                    # 内存清理
                    del topographies
                    gc.collect()
                    
                except Exception as e:
                    logger.error(f"    Epoch {epoch_idx+1} 处理失败: {e}")
                    continue
            
            # 处理session级别的结果
            if session_algorithm_results:
                # 计算平均指标
                for algorithm_name in session_algorithm_results:
                    alg_result = session_algorithm_results[algorithm_name]
                    
                    # 计算平均指标
                    avg_metrics = {}
                    for metric, values in alg_result['metrics_sum'].items():
                        if values:
                            avg_metrics[metric] = np.mean(values)
                    
                    # 更新结果
                    alg_result['average_metrics'] = avg_metrics
                    alg_result['total_trajectories'] = len(alg_result['trajectories'])
                    
                    session_algorithm_results[algorithm_name] = alg_result
                
                subject_results[session_id] = session_algorithm_results
                
                session_summary = get_label('session_summary',
                                          f"  Session {session_id}: 算法对比完成",
                                          f"  Session {session_id}: Algorithm comparison completed")
                logger.info(session_summary)
                
                # 显示各算法的简要结果
                for algorithm_name, alg_result in session_algorithm_results.items():
                    logger.info(f"    {algorithm_name}: {alg_result['total_trajectories']} 条轨迹, "
                              f"平均耗时 {alg_result['total_computation_time']/alg_result['epoch_count']:.2f}s")
            else:
                logger.warning(f"  Session {session_id}: 所有算法均未找到有效轨迹")
                
        except Exception as e:
            logger.error(f"  处理session {session_id} 时出错: {e}")
            continue
    
    return subject_results if subject_results else None

def create_algorithm_comparison_report(all_results, logger):
    """创建算法对比报告"""
    logger.info("生成算法对比报告...")
    
    try:
        # 收集所有算法的统计数据
        algorithm_stats = {}
        
        for subject_id, sessions in all_results.items():
            for session_id, session_data in sessions.items():
                for algorithm_name, alg_data in session_data.items():
                    if algorithm_name not in algorithm_stats:
                        algorithm_stats[algorithm_name] = {
                            'total_trajectories': [],
                            'computation_times': [],
                            'trajectory_lengths': [],
                            'trajectory_qualities': []
                        }
                    
                    # 收集统计数据
                    algorithm_stats[algorithm_name]['total_trajectories'].append(alg_data['total_trajectories'])
                    algorithm_stats[algorithm_name]['computation_times'].append(alg_data['total_computation_time'])
                    
                    # 收集轨迹统计
                    for traj_data in alg_data['trajectories'].values():
                        algorithm_stats[algorithm_name]['trajectory_lengths'].append(traj_data['length'])
                        algorithm_stats[algorithm_name]['trajectory_qualities'].append(traj_data.get('quality_score', 0))
        
        # 生成报告
        report = []
        report.append("=" * 80)
        report.append("EEG轨迹跟踪算法对比报告")
        report.append("=" * 80)
        report.append(f"实验时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"对比算法数量: {len(algorithm_stats)}")
        report.append(f"处理被试数量: {len(all_results)}")
        report.append("")
        
        # 算法性能汇总
        report.append("算法性能汇总:")
        report.append("-" * 50)
        
        performance_ranking = []
        
        for algorithm_name, stats in algorithm_stats.items():
            if not stats['total_trajectories']:
                continue
            
            avg_trajectories = np.mean(stats['total_trajectories'])
            avg_time = np.mean(stats['computation_times'])
            avg_length = np.mean(stats['trajectory_lengths']) if stats['trajectory_lengths'] else 0
            avg_quality = np.mean(stats['trajectory_qualities']) if stats['trajectory_qualities'] else 0
            
            # 计算综合性能分数
            performance_score = (avg_trajectories * 0.3 + 
                               avg_length * 0.25 + 
                               avg_quality * 0.25 + 
                               (10 / max(avg_time, 0.1)) * 0.2)  # 时间越短分数越高
            
            performance_ranking.append((algorithm_name, performance_score, {
                'avg_trajectories': avg_trajectories,
                'avg_time': avg_time,
                'avg_length': avg_length,
                'avg_quality': avg_quality
            }))
            
            report.append(f"\n{algorithm_name.upper()} 算法:")
            report.append(f"  平均轨迹数量: {avg_trajectories:.2f}")
            report.append(f"  平均计算时间: {avg_time:.3f}s")
            report.append(f"  平均轨迹长度: {avg_length:.1f} 帧")
            report.append(f"  平均轨迹质量: {avg_quality:.3f}")
            report.append(f"  综合性能分数: {performance_score:.3f}")
        
        # 算法排名
        performance_ranking.sort(key=lambda x: x[1], reverse=True)
        
        report.append("\n算法性能排名:")
        report.append("-" * 30)
        
        for i, (algorithm_name, score, details) in enumerate(performance_ranking, 1):
            report.append(f"{i}. {algorithm_name.upper()}: {score:.3f}")
            if i == 1:
                report.append("   🏆 综合性能最佳")
        
        # 算法特色分析
        report.append("\n算法特色分析:")
        report.append("-" * 30)
        
        if performance_ranking:
            # 最多轨迹
            max_traj_alg = max(performance_ranking, key=lambda x: x[2]['avg_trajectories'])
            report.append(f"检测能力最强: {max_traj_alg[0]} ({max_traj_alg[2]['avg_trajectories']:.1f} 条平均轨迹)")
            
            # 最快速度
            min_time_alg = min(performance_ranking, key=lambda x: x[2]['avg_time'])
            report.append(f"计算速度最快: {min_time_alg[0]} ({min_time_alg[2]['avg_time']:.3f}s 平均时间)")
            
            # 最高质量
            max_quality_alg = max(performance_ranking, key=lambda x: x[2]['avg_quality'])
            report.append(f"轨迹质量最高: {max_quality_alg[0]} ({max_quality_alg[2]['avg_quality']:.3f} 平均质量)")
            
            # 最长轨迹
            max_length_alg = max(performance_ranking, key=lambda x: x[2]['avg_length'])
            report.append(f"跟踪持续最长: {max_length_alg[0]} ({max_length_alg[2]['avg_length']:.1f} 帧平均长度)")
        
        # 使用建议
        report.append("\n使用建议:")
        report.append("-" * 20)
        
        if performance_ranking:
            best_overall = performance_ranking[0][0]
            report.append(f"• 综合推荐: {best_overall} (综合性能最佳)")
            
            # 针对不同需求的推荐
            if len(performance_ranking) > 1:
                fastest = min(performance_ranking, key=lambda x: x[2]['avg_time'])[0]
                highest_quality = max(performance_ranking, key=lambda x: x[2]['avg_quality'])[0]
                most_trajectories = max(performance_ranking, key=lambda x: x[2]['avg_trajectories'])[0]
                
                report.append(f"• 实时处理推荐: {fastest} (速度优先)")
                report.append(f"• 高精度分析推荐: {highest_quality} (质量优先)")
                report.append(f"• 复杂场景推荐: {most_trajectories} (检测能力优先)")
        
        # 保存报告
        report_text = "\n".join(report)
        report_path = os.path.join(Config.RESULTS_ROOT, "algorithm_comparison", "comparison_report.txt")
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"算法对比报告已保存: {report_path}")
        
        # 显示报告预览
        print("\n" + "="*60)
        print("算法对比报告预览:")
        print("="*60)
        preview_lines = report_text.split('\n')[:30]  # 显示前30行
        print('\n'.join(preview_lines))
        if len(report) > 30:
            print("\n... (完整报告请查看文件)")
        print("="*60)
        
        return algorithm_stats, performance_ranking
        
    except Exception as e:
        logger.error(f"生成算法对比报告失败: {e}")
        return {}, []

def create_algorithm_comparison_visualizations(all_results, algorithm_stats, performance_ranking, visualizer, logger):
    """创建算法对比可视化"""
    logger.info("生成算法对比可视化...")
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        comparison_dir = os.path.join(Config.RESULTS_ROOT, "algorithm_comparison")
        os.makedirs(comparison_dir, exist_ok=True)
        
        # 1. 算法性能对比柱状图
        if algorithm_stats and performance_ranking:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('算法性能对比分析', fontsize=16, fontweight='bold')
            
            algorithms = [item[0] for item in performance_ranking]
            
            # 轨迹数量对比
            ax = axes[0, 0]
            traj_counts = [np.mean(algorithm_stats[alg]['total_trajectories']) for alg in algorithms]
            bars = ax.bar(algorithms, traj_counts, color='skyblue', alpha=0.7)
            ax.set_title('平均轨迹数量对比')
            ax.set_ylabel('轨迹数量')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, count in zip(bars, traj_counts):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                       f'{count:.1f}', ha='center', va='bottom')
            
            # 计算时间对比
            ax = axes[0, 1]
            comp_times = [np.mean(algorithm_stats[alg]['computation_times']) for alg in algorithms]
            bars = ax.bar(algorithms, comp_times, color='lightgreen', alpha=0.7)
            ax.set_title('平均计算时间对比')
            ax.set_ylabel('时间 (秒)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, time in zip(bars, comp_times):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{time:.2f}s', ha='center', va='bottom')
            
            # 轨迹长度对比
            ax = axes[1, 0]
            traj_lengths = [np.mean(algorithm_stats[alg]['trajectory_lengths']) if algorithm_stats[alg]['trajectory_lengths'] else 0 
                           for alg in algorithms]
            bars = ax.bar(algorithms, traj_lengths, color='orange', alpha=0.7)
            ax.set_title('平均轨迹长度对比')
            ax.set_ylabel('长度 (帧)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, length in zip(bars, traj_lengths):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                       f'{length:.1f}', ha='center', va='bottom')
            
            # 综合性能分数
            ax = axes[1, 1]
            performance_scores = [item[1] for item in performance_ranking]
            bars = ax.bar(algorithms, performance_scores, color='coral', alpha=0.7)
            ax.set_title('综合性能分数')
            ax.set_ylabel('性能分数')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, score in zip(bars, performance_scores):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                       f'{score:.2f}', ha='center', va='bottom')
            
            plt.tight_layout()
            comparison_path = os.path.join(comparison_dir, "algorithm_performance_comparison.png")
            plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"性能对比图已保存: {comparison_path}")
            
        # 2. 算法特征雷达图
        if len(performance_ranking) >= 2:
            fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
            
            # 准备数据
            metrics = ['轨迹数量', '计算速度', '轨迹长度', '轨迹质量']
            colors = ['red', 'blue', 'green', 'orange', 'purple']
            
            for i, (algorithm_name, _, details) in enumerate(performance_ranking[:5]):  # 最多显示5个算法
                values = [
                    details['avg_trajectories'] / max([d[2]['avg_trajectories'] for d in performance_ranking]),  # 标准化
                    (10 / max(details['avg_time'], 0.1)) / max([10 / max(d[2]['avg_time'], 0.1) for d in performance_ranking]),  # 速度越快越好
                    details['avg_length'] / max([d[2]['avg_length'] for d in performance_ranking]),
                    details['avg_quality'] / max([d[2]['avg_quality'] for d in performance_ranking]) if max([d[2]['avg_quality'] for d in performance_ranking]) > 0 else 0
                ]
                
                angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                values += values[:1]  # 闭合图形
                angles += angles[:1]
                
                ax.plot(angles, values, 'o-', linewidth=2, label=algorithm_name, color=colors[i % len(colors)])
                ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
            
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            ax.set_ylim(0, 1)
            ax.set_title('算法性能雷达图', size=16, fontweight='bold', pad=20)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            
            radar_path = os.path.join(comparison_dir, "algorithm_radar_chart.png")
            plt.savefig(radar_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"算法雷达图已保存: {radar_path}")
        
        logger.info("算法对比可视化完成")
        
    except Exception as e:
        logger.error(f"生成可视化失败: {e}")

def cleanup_memory():
    """清理内存"""
    gc.collect()
    
    try:
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / (1024 * 1024)
        
        if memory_mb > Config.MEMORY_LIMIT_MB:
            logging.getLogger(__name__).warning(
                f"内存使用量过高: {memory_mb:.1f} MB (限制: {Config.MEMORY_LIMIT_MB} MB)"
            )
            return False
    except ImportError:
        pass
    
    return True

def print_final_summary(all_results, algorithm_stats):
    """打印最终总结"""
    print("\n" + "="*70)
    print("算法对比实验完成总结")
    print("="*70)
    
    # 基本统计
    n_subjects = len(all_results)
    total_sessions = sum(len(sessions) for sessions in all_results.values())
    
    print(f"✓ 成功处理被试数量: {n_subjects}")
    print(f"✓ 总session数量: {total_sessions}")
    print(f"✓ 对比算法数量: {len(algorithm_stats)}")
    print(f"✓ 算法列表: {', '.join(algorithm_stats.keys())}")
    
    # 显示各算法的总体表现
    print(f"\n各算法总体表现:")
    for algorithm_name, stats in algorithm_stats.items():
        if stats['total_trajectories']:
            avg_trajectories = np.mean(stats['total_trajectories'])
            avg_time = np.mean(stats['computation_times'])
            print(f"  {algorithm_name}: 平均{avg_trajectories:.1f}条轨迹, 平均耗时{avg_time:.2f}s")
    
    # 输出路径
    print(f"\n结果保存位置:")
    print(f"  📁 轨迹图对比: {os.path.join(Config.RESULTS_ROOT, 'trajectories')}")
    print(f"  📊 算法对比图: {os.path.join(Config.RESULTS_ROOT, 'algorithm_comparison')}")
    print(f"  📄 对比报告: {os.path.join(Config.RESULTS_ROOT, 'algorithm_comparison', 'comparison_report.txt')}")
    
    print("="*70)
    print("🎉 算法对比实验成功完成！")
    print("="*70)

def main():
    """主实验流程"""
    parser = argparse.ArgumentParser(description='EEG Trajectory Analysis System with Algorithm Comparison')
    parser.add_argument('--subjects', type=int, default=None, 
                       help='Maximum number of subjects to process')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Maximum epochs per subject')
    parser.add_argument('--algorithms', nargs='+', default=None,
                       help='Algorithms to compare', choices=Config.COMPARISON_ALGORITHMS)
    parser.add_argument('--disable-comparison', action='store_true',
                       help='Disable algorithm comparison (use greedy only)')
    
    args = parser.parse_args()
    
    # 打印系统信息
    print_system_info()
    
    # 检查依赖
    if not check_dependencies():
        return 1
    
    # 设置日志
    logger = setup_logging()
    start_msg = get_label('start_experiment',
                         "开始EEG脑电地形图运动轨迹分析实验 - 算法对比版",
                         "Starting EEG topography motion trajectory analysis experiment - Algorithm Comparison Edition")
    logger.info(start_msg)
    
    try:
        # 验证配置
        if not validate_config():
            return 1
        
        # 应用命令行参数
        if args.subjects:
            Config.MAX_SUBJECTS = args.subjects
        if args.epochs:
            Config.MAX_EPOCHS_PER_SUBJECT = args.epochs
        if args.algorithms:
            Config.COMPARISON_ALGORITHMS = args.algorithms
        if args.disable_comparison:
            Config.ENABLE_ALGORITHM_COMPARISON = False
            Config.COMPARISON_ALGORITHMS = ['greedy']
        
        # 初始化组件
        init_msg = get_label('init_components', 
                            "初始化分析组件...",
                            "Initializing analysis components...")
        logger.info(init_msg)
        
        data_loader = EEGDataLoader(Config.DATA_ROOT, Config)
        topo_generator = TopographyGenerator(Config)
        analyzer = TrajectoryAnalyzer(Config)
        visualizer = Visualizer(Config)
        
        # 加载数据
        load_msg = get_label('load_data',
                            "开始加载EEG数据...",
                            "Starting to load EEG data...")
        logger.info(load_msg)
        all_data = data_loader.load_all_subjects(Config.MAX_SUBJECTS)
        
        if not all_data:
            error_msg = get_label('no_data',
                                 "未能加载任何EEG数据，请检查数据路径和格式",
                                 "Failed to load any EEG data, please check data path and format")
            logger.error(error_msg)
            print(f"\n❌ {error_msg}")
            return 1
        
        success_msg = get_label('load_success',
                               f"成功加载 {len(all_data)} 个被试的数据",
                               f"Successfully loaded data from {len(all_data)} subjects")
        logger.info(success_msg)
        
        # 存储所有结果
        all_results = {}
        
        # 处理每个被试
        total_subjects = len(all_data)
        processed_subjects = 0
        
        for subject_id, sessions in tqdm(all_data.items(), desc="Processing subjects"):
            try:
                if Config.ENABLE_ALGORITHM_COMPARISON:
                    subject_results = process_subject_with_multiple_algorithms(
                        data_loader, topo_generator, analyzer, visualizer,
                        subject_id, sessions, logger
                    )
                else:
                    # 使用原有的单算法处理逻辑（这里可以调用原来的process_subject_data函数）
                    logger.info("使用单算法模式（greedy）")
                    # 这里可以添加原有的处理逻辑
                    subject_results = None
                
                if subject_results:
                    all_results[subject_id] = subject_results
                    processed_subjects += 1
                    
                    # 定期清理内存
                    if processed_subjects % 2 == 0:
                        cleanup_memory()
                        progress_msg = get_label('progress',
                                               f"已处理 {processed_subjects}/{total_subjects} 个被试",
                                               f"Processed {processed_subjects}/{total_subjects} subjects")
                        logger.info(progress_msg)
                else:
                    no_result_msg = get_label('no_result',
                                            f"被试 {subject_id} 未产生有效结果",
                                            f"Subject {subject_id} produced no valid results")
                    logger.warning(no_result_msg)
                    
            except Exception as e:
                logger.error(f"处理被试 {subject_id} 时出现严重错误: {e}")
                continue
        
        if processed_subjects == 0:
            no_subjects_msg = get_label('no_subjects',
                                       "没有成功处理任何被试数据",
                                       "No subject data was successfully processed")
            logger.error(no_subjects_msg)
            print(f"\n❌ {no_subjects_msg}")
            return 1
        
        complete_msg = get_label('data_complete',
                                f"数据处理完成，成功处理 {processed_subjects} 个被试",
                                f"Data processing complete, successfully processed {processed_subjects} subjects")
        logger.info(complete_msg)
        
        # 生成算法对比报告和可视化
        if Config.ENABLE_ALGORITHM_COMPARISON and all_results:
            algorithm_stats, performance_ranking = create_algorithm_comparison_report(all_results, logger)
            create_algorithm_comparison_visualizations(all_results, algorithm_stats, performance_ranking, visualizer, logger)
            
            # 打印最终总结
            print_final_summary(all_results, algorithm_stats)
        else:
            logger.info("算法对比已禁用或无有效结果")
        
        # 保存完整结果
        results_path = os.path.join(Config.RESULTS_ROOT, "algorithm_comparison_results.pkl")
        try:
            with open(results_path, 'wb') as f:
                pickle.dump(all_results, f, protocol=pickle.HIGHEST_PROTOCOL)
            logger.info(f"完整结果已保存: {results_path}")
        except Exception as e:
            logger.error(f"保存完整结果失败: {e}")
        
        success_final = get_label('success_final',
                                 "算法对比实验成功完成!",
                                 "Algorithm comparison experiment completed successfully!")
        logger.info(success_final)
        return 0
        
    except KeyboardInterrupt:
        interrupt_msg = get_label('interrupted',
                                 "用户中断了实验",
                                 "Experiment was interrupted by user")
        logger.info(interrupt_msg)
        print(f"\n🛑 {interrupt_msg}")
        return 130
        
    except Exception as e:
        error_final = get_label('unexpected_error',
                               f"实验过程中发生未预期的错误: {e}",
                               f"Unexpected error during experiment: {e}")
        logger.error(error_final)
        print(f"\n❌ {error_final}")
        return 1
        
    finally:
        cleanup_memory()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

# ========== quick_test.py ==========
# 相对路径: quick_test.py
# 在项目中的相对位置: ./quick_test.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EEG轨迹跟踪算法对比系统 - 快速测试脚本
用于验证系统安装和基本功能
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import logging

def test_dependencies():
    """测试依赖库安装"""
    print("🔍 测试依赖库安装...")
    
    required_packages = {
        'numpy': 'NumPy',
        'scipy': 'SciPy', 
        'matplotlib': 'Matplotlib',
        'sklearn': 'Scikit-learn',
        'cv2': 'OpenCV',
        'tqdm': 'tqdm',
        'mne': 'MNE-Python'
    }
    
    missing_packages = []
    installed_packages = []
    
    for package, name in required_packages.items():
        try:
            __import__(package)
            installed_packages.append(name)
            print(f"  ✓ {name}")
        except ImportError:
            missing_packages.append(name)
            print(f"  ❌ {name} - 未安装")
    
    print(f"\n安装状态: {len(installed_packages)}/{len(required_packages)} 个包已安装")
    
    if missing_packages:
        print(f"\n❌ 缺少依赖: {', '.join(missing_packages)}")
        print("请运行: pip install -r requirements.txt")
        return False
    else:
        print("✅ 所有依赖库已正确安装!")
        return True

def test_tracker_factory():
    """测试跟踪器工厂"""
    print("\n🏭 测试跟踪器工厂...")
    
    try:
        # 添加路径
        sys.path.append('trackers')
        from trackers import TrackerFactory
        from config import Config
        
        # 测试获取可用算法
        algorithms = TrackerFactory.get_available_algorithms()
        print(f"  ✓ 可用算法: {', '.join(algorithms)}")
        
        # 测试创建跟踪器
        success_count = 0
        for algorithm in algorithms:
            try:
                tracker = TrackerFactory.create_tracker(algorithm, Config)
                if tracker is not None:
                    print(f"  ✓ {algorithm} 跟踪器创建成功")
                    success_count += 1
                else:
                    print(f"  ❌ {algorithm} 跟踪器创建失败")
            except Exception as e:
                print(f"  ❌ {algorithm} 跟踪器创建异常: {e}")
        
        print(f"\n跟踪器创建状态: {success_count}/{len(algorithms)} 个算法可用")
        return success_count > 0
        
    except Exception as e:
        print(f"  ❌ 跟踪器工厂测试失败: {e}")
        return False

def test_synthetic_data():
    """测试合成数据处理"""
    print("\n🧪 测试合成数据处理...")
    
    try:
        sys.path.append('src')
        sys.path.append('trackers')
        
        from src.topography import TopographyGenerator
        from trackers import TrackerFactory
        from config import Config
        
        # 创建合成地形图数据
        n_frames = 50
        size = (64, 64)  # 使用较小尺寸以加快测试
        
        print(f"  🔧 生成 {n_frames} 帧 {size} 尺寸的合成地形图...")
        
        # 创建简单的移动激活区域
        topographies = np.zeros((n_frames, size[0], size[1]))
        
        for i in range(n_frames):
            # 创建移动的高斯激活
            center_x = 20 + int(15 * np.sin(2 * np.pi * i / 30))
            center_y = 20 + int(10 * np.cos(2 * np.pi * i / 20))
            
            y, x = np.ogrid[:size[0], :size[1]]
            activation = np.exp(-((x - center_x)**2 + (y - center_y)**2) / (2 * 5**2))
            topographies[i] = activation
        
        print("  ✓ 合成地形图生成完成")
        
        # 测试跟踪算法
        test_algorithms = ['greedy', 'hungarian']  # 测试主要算法
        
        for algorithm in test_algorithms:
            try:
                print(f"  🎯 测试 {algorithm} 算法...")
                
                tracker = TrackerFactory.create_tracker(algorithm, Config)
                if tracker is None:
                    print(f"    ❌ {algorithm} 跟踪器创建失败")
                    continue
                
                result = tracker.track_sequence(topographies)
                
                if result and 'trajectories' in result:
                    trajectories = result['trajectories']
                    metrics = result.get('metrics', {})
                    
                    print(f"    ✓ {algorithm}: {len(trajectories)} 条轨迹")
                    print(f"    ✓ 计算时间: {metrics.get('computation_time', 0):.3f}s")
                    
                    if len(trajectories) > 0:
                        first_traj = list(trajectories.values())[0]
                        print(f"    ✓ 轨迹长度: {first_traj['length']} 帧")
                else:
                    print(f"    ⚠️  {algorithm}: 未检测到轨迹")
                
            except Exception as e:
                print(f"    ❌ {algorithm} 测试失败: {e}")
        
        return True
        
    except Exception as e:
        print(f"  ❌ 合成数据测试失败: {e}")
        return False

def test_visualization():
    """测试可视化功能"""
    print("\n🎨 测试可视化功能...")
    
    try:
        # 测试matplotlib设置
        import matplotlib
        matplotlib.use('Agg')  # 使用非交互式后端
        
        # 创建简单测试图
        fig, ax = plt.subplots(figsize=(6, 4))
        
        # 测试中文字体
        try:
            ax.text(0.5, 0.7, '测试中文字体', ha='center', va='center', fontsize=14)
            ax.text(0.5, 0.5, 'Test English Font', ha='center', va='center', fontsize=12)
            chinese_support = True
        except:
            ax.text(0.5, 0.6, 'Font Test (English Only)', ha='center', va='center', fontsize=12)
            chinese_support = False
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.set_title('EEG Trajectory Analysis - Font Test')
        ax.axis('off')
        
        # 保存测试图
        test_dir = './test_results'
        os.makedirs(test_dir, exist_ok=True)
        
        test_path = os.path.join(test_dir, 'font_test.png')
        plt.savefig(test_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  ✓ 测试图保存至: {test_path}")
        print(f"  {'✓' if chinese_support else '⚠️'} 中文字体支持: {'是' if chinese_support else '否'}")
        
        # 测试复杂可视化
        fig, axes = plt.subplots(2, 2, figsize=(10, 8))
        fig.suptitle('Algorithm Comparison Test Charts', fontsize=14)
        
        # 模拟数据
        algorithms = ['Greedy', 'Hungarian', 'Kalman', 'Overlap', 'Hybrid']
        metrics = {
            'trajectory_count': [4.2, 4.5, 3.8, 3.9, 4.3],
            'computation_time': [0.15, 0.45, 0.25, 0.35, 0.55],
            'trajectory_quality': [0.72, 0.85, 0.78, 0.74, 0.82],
            'memory_usage': [50, 80, 65, 70, 95]
        }
        
        # 柱状图测试
        for i, (metric, values) in enumerate(metrics.items()):
            ax = axes[i//2, i%2]
            bars = ax.bar(algorithms, values, alpha=0.7)
            ax.set_title(metric.replace('_', ' ').title())
            ax.tick_params(axis='x', rotation=45)
            
            # 添加数值标签
            for bar, value in zip(bars, values):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{value:.2f}', ha='center', va='bottom', fontsize=8)
        
        plt.tight_layout()
        
        chart_path = os.path.join(test_dir, 'comparison_charts_test.png')
        plt.savefig(chart_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  ✓ 对比图表保存至: {chart_path}")
        
        return True
        
    except Exception as e:
        print(f"  ❌ 可视化测试失败: {e}")
        return False

def test_config():
    """测试配置文件"""
    print("\n⚙️ 测试配置文件...")
    
    try:
        from config import Config
        
        # 测试基本配置
        print(f"  ✓ 数据路径: {Config.DATA_ROOT}")
        print(f"  ✓ 结果路径: {Config.RESULTS_ROOT}")
        print(f"  ✓ 最大被试数: {Config.MAX_SUBJECTS}")
        print(f"  ✓ 算法对比: {'启用' if Config.ENABLE_ALGORITHM_COMPARISON else '禁用'}")
        print(f"  ✓ 对比算法: {', '.join(Config.COMPARISON_ALGORITHMS)}")
        
        # 测试配置方法
        summary = Config.get_experiment_summary()
        print(f"  ✓ 实验摘要: {summary['algorithms_count']} 种算法, {summary['total_subjects']} 个被试")
        
        # 测试算法配置
        for algorithm in Config.COMPARISON_ALGORITHMS:
            alg_config = Config.get_algorithm_config(algorithm)
            print(f"  ✓ {algorithm} 配置: {len(alg_config)} 个参数")
        
        return True
        
    except Exception as e:
        print(f"  ❌ 配置测试失败: {e}")
        return False

def generate_test_report(results):
    """生成测试报告"""
    print("\n" + "="*60)
    print("🎯 EEG轨迹跟踪系统 - 快速测试报告")
    print("="*60)
    print(f"测试时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("")
    
    test_items = [
        ('依赖库检查', results.get('dependencies', False)),
        ('跟踪器工厂', results.get('tracker_factory', False)),
        ('合成数据处理', results.get('synthetic_data', False)),
        ('可视化功能', results.get('visualization', False)),
        ('配置文件', results.get('config', False))
    ]
    
    passed_tests = sum(1 for _, result in test_items if result)
    total_tests = len(test_items)
    
    print("测试项目:")
    for item_name, passed in test_items:
        status = "✅ 通过" if passed else "❌ 失败"
        print(f"  {item_name}: {status}")
    
    print(f"\n总体结果: {passed_tests}/{total_tests} 项测试通过")
    
    if passed_tests == total_tests:
        print("🎉 恭喜！系统安装完成，所有功能正常！")
        print("\n下一步:")
        print("  1. 准备您的EEG数据（参考README.md中的数据格式）")
        print("  2. 运行: python main.py --subjects 3  (快速测试)")
        print("  3. 运行: python main.py  (完整实验)")
    elif passed_tests >= 3:
        print("✅ 系统基本功能正常，可以开始使用！")
        print("⚠️  部分功能可能受限，请检查失败的测试项。")
    else:
        print("❌ 系统存在严重问题，建议重新安装。")
        print("\n建议:")
        print("  1. 检查Python版本（需要3.8+）")
        print("  2. 重新安装依赖: pip install -r requirements.txt")
        print("  3. 检查系统兼容性")
    
    print("\n📁 测试文件保存在: ./test_results/")
    print("📋 如需帮助，请查看README.md或联系维护者")
    print("="*60)

def main():
    """主测试函数"""
    print("🚀 EEG轨迹跟踪算法对比系统 - 快速功能测试")
    print("="*60)
    print("此测试将验证系统安装和基本功能")
    print("预计耗时: 1-2分钟")
    print("")
    
    # 抑制部分日志
    logging.getLogger().setLevel(logging.WARNING)
    
    # 运行各项测试
    results = {}
    
    results['dependencies'] = test_dependencies()
    results['config'] = test_config()
    results['tracker_factory'] = test_tracker_factory()
    results['synthetic_data'] = test_synthetic_data()
    results['visualization'] = test_visualization()
    
    # 生成测试报告
    generate_test_report(results)
    
    return 0 if all(results.values()) else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

# ========== src/__init__.py ==========
# 相对路径: src/__init__.py
# 在项目中的相对位置: ./src/__init__.py

# EEG脑电地形图运动轨迹分析包
__version__ = "1.0.0"
__author__ = "EEG Research Team"

from .data_loader import EEGDataLoader
from .topography import TopographyGenerator
from .tracker import RegionTracker
from .trajectory_analysis import TrajectoryAnalyzer
from .visualization import Visualizer

__all__ = [
    'EEGDataLoader',
    'TopographyGenerator', 
    'RegionTracker',
    'TrajectoryAnalyzer',
    'Visualizer'
]

# ========== src/data_loader.py ==========
# 相对路径: src/data_loader.py
# 在项目中的相对位置: ./src/data_loader.py

import os
import mne
import numpy as np
import pandas as pd
from tqdm import tqdm
import logging
import gc
from typing import List, Tuple, Dict, Optional

class EEGDataLoader:
    def __init__(self, data_root: str, config):
        self.data_root = data_root
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # 设置MNE日志级别
        mne.set_log_level('WARNING')
        
    def get_subject_sessions(self, subject_id: str) -> List[str]:
        """获取指定被试的所有session"""
        subject_dir = os.path.join(self.data_root, f"sub-{subject_id}")
        if not os.path.exists(subject_dir):
            self.logger.warning(f"Subject directory not found: {subject_dir}")
            return []
        
        sessions = []
        try:
            for item in os.listdir(subject_dir):
                if item.startswith("ses-") and os.path.isdir(os.path.join(subject_dir, item)):
                    session_num = item.split("-")[1]
                    sessions.append(session_num)
        except Exception as e:
            self.logger.error(f"Error reading subject directory {subject_dir}: {e}")
            return []
        
        return sorted(sessions, key=lambda x: int(x) if x.isdigit() else 0)
    
    def find_eeg_files(self, subject_id: str, session_id: str) -> Optional[str]:
        """查找EEG文件，支持多种格式"""
        eeg_dir = os.path.join(self.data_root, f"sub-{subject_id}", f"ses-{session_id}", "eeg")
        
        if not os.path.exists(eeg_dir):
            self.logger.warning(f"EEG directory not found: {eeg_dir}")
            return None
        
        # 按优先级搜索不同格式的文件
        file_patterns = [
            f"sub-{subject_id}_ses-{session_id}_task-innerspeech_eeg.vhdr",
            f"sub-{subject_id}_ses-{session_id}_task-innerspeech_eeg.edf",
            f"sub-{subject_id}_ses-{session_id}_task-innerspeech_eeg.fif",
            f"sub-{subject_id}_ses-{session_id}_eeg.vhdr",
            f"sub-{subject_id}_ses-{session_id}_eeg.edf",
            f"sub-{subject_id}_ses-{session_id}_eeg.fif"
        ]
        
        for pattern in file_patterns:
            file_path = os.path.join(eeg_dir, pattern)
            if os.path.exists(file_path):
                return file_path
        
        # 如果找不到特定模式，列出所有EEG文件
        try:
            eeg_files = [f for f in os.listdir(eeg_dir) 
                        if f.endswith(('.vhdr', '.edf', '.fif', '.set', '.cnt'))]
            if eeg_files:
                return os.path.join(eeg_dir, eeg_files[0])
        except Exception as e:
            self.logger.error(f"Error listing EEG directory {eeg_dir}: {e}")
        
        return None
    
    def load_raw_eeg(self, subject_id: str, session_id: str) -> Optional[mne.io.Raw]:
        """加载原始EEG数据，支持多种格式"""
        eeg_file = self.find_eeg_files(subject_id, session_id)
        
        if not eeg_file:
            self.logger.warning(f"No EEG file found for subject {subject_id}, session {session_id}")
            return None
        
        try:
            # 根据文件扩展名选择加载方法
            file_ext = os.path.splitext(eeg_file)[1].lower()
            
            if file_ext == '.vhdr':
                raw = mne.io.read_raw_brainvision(eeg_file, preload=True, verbose=False)
            elif file_ext == '.edf':
                raw = mne.io.read_raw_edf(eeg_file, preload=True, verbose=False)
            elif file_ext == '.fif':
                raw = mne.io.read_raw_fif(eeg_file, preload=True, verbose=False)
            elif file_ext == '.set':
                raw = mne.io.read_raw_eeglab(eeg_file, preload=True, verbose=False)
            elif file_ext == '.cnt':
                raw = mne.io.read_raw_cnt(eeg_file, preload=True, verbose=False)
            else:
                self.logger.error(f"Unsupported file format: {file_ext}")
                return None
            
            self.logger.info(f"Successfully loaded {eeg_file}")
            return raw
            
        except Exception as e:
            self.logger.error(f"Error loading EEG data from {eeg_file}: {e}")
            return None
    
    def preprocess_eeg(self, raw: mne.io.Raw) -> Optional[mne.io.Raw]:
        """预处理EEG数据"""
        try:
            # 创建副本避免修改原始数据
            raw_copy = raw.copy()
            
            # 检查并设置电极类型
            if len(raw_copy.info['chs']) > 0:
                # 自动检测EEG通道
                raw_copy.set_channel_types({ch: 'eeg' for ch in raw_copy.ch_names 
                                          if not ch.startswith(('EOG', 'ECG', 'EMG', 'TRIG', 'STIM'))})
            
            # 选择EEG通道
            raw_copy.pick_types(eeg=True, exclude='bads')
            
            if len(raw_copy.ch_names) == 0:
                self.logger.error("No EEG channels found after preprocessing")
                return None
            
            # 设置参考电极
            try:
                raw_copy.set_eeg_reference('average', projection=True, verbose=False)
                raw_copy.apply_proj(verbose=False)
            except Exception as e:
                self.logger.warning(f"Failed to set average reference: {e}")
            
            # 滤波
            try:
                raw_copy.filter(l_freq=self.config.LOW_FREQ, h_freq=self.config.HIGH_FREQ, 
                              fir_design='firwin', verbose=False)
            except Exception as e:
                self.logger.warning(f"Filtering failed: {e}")
            
            # 重采样（如果需要）
            if raw_copy.info['sfreq'] != self.config.SAMPLING_RATE:
                try:
                    raw_copy.resample(self.config.SAMPLING_RATE, verbose=False)
                except Exception as e:
                    self.logger.warning(f"Resampling failed: {e}")
            
            return raw_copy
            
        except Exception as e:
            self.logger.error(f"Preprocessing failed: {e}")
            return None
    
    def extract_epochs(self, raw: mne.io.Raw, epoch_length: float = None, 
                      overlap: float = 0.5, max_epochs: int = None) -> Optional[mne.Epochs]:
        """提取固定长度的epoch"""
        if epoch_length is None:
            epoch_length = self.config.TIME_WINDOW
        
        if max_epochs is None:
            max_epochs = self.config.MAX_EPOCHS_PER_SUBJECT
        
        try:
            # 创建虚拟事件
            duration = epoch_length
            interval = duration * (1 - overlap)
            
            n_samples = int(duration * raw.info['sfreq'])
            step_samples = int(interval * raw.info['sfreq'])
            
            if n_samples >= len(raw.times):
                self.logger.warning("Epoch length longer than recording, using full recording")
                n_samples = len(raw.times) - 1
            
            events = []
            event_id = 1
            
            # 限制epoch数量以节省内存
            max_start_sample = len(raw.times) - n_samples
            epoch_count = 0
            
            for start_sample in range(0, max_start_sample, step_samples):
                if epoch_count >= max_epochs:
                    break
                events.append([start_sample, 0, event_id])
                epoch_count += 1
            
            if not events:
                self.logger.error("No epochs could be created")
                return None
            
            events = np.array(events)
            
            epochs = mne.Epochs(raw, events, event_id={'epoch': event_id}, 
                               tmin=0, tmax=duration-1/raw.info['sfreq'], 
                               baseline=None, preload=True, verbose=False)
            
            # 检查epoch质量
            if len(epochs) == 0:
                self.logger.error("No valid epochs after creation")
                return None
            
            self.logger.info(f"Created {len(epochs)} epochs of {duration}s each")
            return epochs
            
        except Exception as e:
            self.logger.error(f"Epoch extraction failed: {e}")
            return None
    
    def get_electrode_positions(self, ch_names: List[str]) -> Dict[str, Tuple[float, float]]:
        """获取电极位置，改进的匹配算法"""
        positions = {}
        unmatched_channels = []
        
        for ch_name in ch_names:
            # 清理通道名称
            clean_name = ch_name.strip()
            position_found = False
            
            # 尝试多种匹配方式
            search_variants = [
                clean_name,
                clean_name.upper(),
                clean_name.lower(),
                clean_name.capitalize(),
                clean_name.replace(' ', ''),
                clean_name.replace('-', ''),
                clean_name.replace('_', '')
            ]
            
            for variant in search_variants:
                if variant in self.config.ELECTRODE_POSITIONS:
                    positions[ch_name] = self.config.ELECTRODE_POSITIONS[variant]
                    position_found = True
                    break
            
            if not position_found:
                unmatched_channels.append(ch_name)
        
        # 为未匹配的电极分配默认位置
        if unmatched_channels:
            self.logger.warning(f"Using default positions for electrodes: {unmatched_channels}")
            for i, ch_name in enumerate(unmatched_channels):
                default_pos = self.config.get_default_electrode_position(
                    ch_name, len(ch_names), len(positions) + i
                )
                positions[ch_name] = default_pos
        
        return positions
    
    def check_memory_usage(self) -> bool:
        """检查内存使用情况"""
        try:
            import psutil
            memory_usage = psutil.virtual_memory().percent
            if memory_usage > 85:  # 超过85%内存使用率
                self.logger.warning(f"High memory usage: {memory_usage:.1f}%")
                gc.collect()  # 强制垃圾回收
                return False
            return True
        except ImportError:
            return True  # 如果没有psutil，假设内存充足
    
    def load_all_subjects(self, max_subjects: Optional[int] = None) -> Dict:
        """加载所有被试数据，改进内存管理"""
        all_data = {}
        
        if max_subjects is None:
            max_subjects = self.config.MAX_SUBJECTS
        
        # 获取所有被试ID
        subject_ids = []
        try:
            for item in os.listdir(self.data_root):
                if item.startswith("sub-") and os.path.isdir(os.path.join(self.data_root, item)):
                    subject_id = item.split("-")[1]
                    subject_ids.append(subject_id)
        except Exception as e:
            self.logger.error(f"Error reading data directory {self.data_root}: {e}")
            return {}
        
        subject_ids = sorted(subject_ids, key=lambda x: int(x) if x.isdigit() else 0)
        if max_subjects:
            subject_ids = subject_ids[:max_subjects]
        
        self.logger.info(f"Found {len(subject_ids)} subjects to process")
        
        successful_loads = 0
        for subject_id in tqdm(subject_ids, desc="Loading subjects"):
            if not self.check_memory_usage():
                self.logger.warning("Memory usage too high, stopping data loading")
                break
            
            sessions = self.get_subject_sessions(subject_id)
            if not sessions:
                continue
            
            all_data[subject_id] = {}
            
            for session_id in sessions:
                try:
                    raw = self.load_raw_eeg(subject_id, session_id)
                    if raw is not None:
                        raw = self.preprocess_eeg(raw)
                        if raw is not None:
                            epochs = self.extract_epochs(raw)
                            if epochs is not None:
                                all_data[subject_id][session_id] = {
                                    'raw': raw,
                                    'epochs': epochs,
                                    'positions': self.get_electrode_positions(raw.ch_names)
                                }
                                successful_loads += 1
                                self.logger.info(f"Successfully loaded subject {subject_id}, session {session_id}")
                            else:
                                self.logger.warning(f"Failed to extract epochs for subject {subject_id}, session {session_id}")
                        else:
                            self.logger.warning(f"Failed to preprocess data for subject {subject_id}, session {session_id}")
                    else:
                        self.logger.warning(f"Failed to load raw data for subject {subject_id}, session {session_id}")
                        
                except Exception as e:
                    self.logger.error(f"Error processing subject {subject_id}, session {session_id}: {e}")
                    continue
            
            # 如果这个被试没有成功加载任何session，移除它
            if not all_data[subject_id]:
                del all_data[subject_id]
                
            # 定期清理内存
            if successful_loads % 5 == 0:
                gc.collect()
        
        self.logger.info(f"Successfully loaded data from {len(all_data)} subjects")
        return all_data

# ========== src/topography.py ==========
# 相对路径: src/topography.py
# 在项目中的相对位置: ./src/topography.py

import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
from scipy.ndimage import gaussian_filter
import cv2
from typing import Dict, Tuple, List, Optional
import logging
import warnings

# 抑制插值警告
warnings.filterwarnings('ignore', category=RuntimeWarning)

class TopographyGenerator:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # 预计算头部掩码以提高效率
        self._head_mask = self.create_head_mask(config.TOPO_SIZE)
        
    def create_head_mask(self, size: Tuple[int, int]) -> np.ndarray:
        """创建头部轮廓掩码"""
        h, w = size
        center = (w // 2, h // 2)
        radius = min(w, h) // 2 - 5
        
        # 创建圆形掩码
        y, x = np.ogrid[:h, :w]
        mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2
        
        return mask.astype(bool)
    
    def electrode_to_pixel(self, pos: Tuple[float, float], size: Tuple[int, int]) -> Tuple[int, int]:
        """将电极位置转换为像素坐标"""
        x, y = pos
        h, w = size
        
        # 标准化坐标到像素坐标
        # 电极坐标范围 [-1, 1] 映射到像素坐标
        pixel_x = int((x + 1) * w / 2)
        pixel_y = int((1 - y) * h / 2)  # Y轴翻转
        
        # 确保在边界内
        pixel_x = max(0, min(w - 1, pixel_x))
        pixel_y = max(0, min(h - 1, pixel_y))
        
        return pixel_x, pixel_y
    
    def validate_electrode_data(self, eeg_data: np.ndarray, 
                               electrode_positions: Dict[str, Tuple[float, float]],
                               ch_names: List[str]) -> Tuple[np.ndarray, List[Tuple[float, float]], List[str]]:
        """验证和清理电极数据"""
        valid_positions = []
        valid_values = []
        valid_names = []
        
        for i, ch_name in enumerate(ch_names):
            if ch_name in electrode_positions:
                pos = electrode_positions[ch_name]
                
                # 检查位置是否合理
                if abs(pos[0]) <= 1.2 and abs(pos[1]) <= 1.2:  # 允许略微超出标准范围
                    # 检查数据是否有效
                    if i < len(eeg_data) and not np.isnan(eeg_data[i]) and not np.isinf(eeg_data[i]):
                        valid_positions.append(pos)
                        valid_values.append(eeg_data[i])
                        valid_names.append(ch_name)
                    else:
                        self.logger.warning(f"Invalid data for electrode {ch_name}: {eeg_data[i] if i < len(eeg_data) else 'missing'}")
                else:
                    self.logger.warning(f"Invalid position for electrode {ch_name}: {pos}")
            else:
                self.logger.warning(f"No position found for electrode {ch_name}")
        
        return np.array(valid_values), valid_positions, valid_names
    
    def generate_topography(self, eeg_data: np.ndarray, 
                          electrode_positions: Dict[str, Tuple[float, float]],
                          ch_names: List[str]) -> np.ndarray:
        """生成脑电地形图"""
        size = self.config.TOPO_SIZE
        
        # 验证和清理数据
        values, positions, valid_names = self.validate_electrode_data(
            eeg_data, electrode_positions, ch_names
        )
        
        if len(positions) < 3:
            self.logger.warning(f"Not enough valid electrode positions for interpolation: {len(positions)}")
            return np.zeros(size)
        
        positions = np.array(positions)
        
        # 创建插值网格
        xi = np.linspace(-1.2, 1.2, size[1])  # 稍微扩大范围以获得更好的边界效果
        yi = np.linspace(-1.2, 1.2, size[0])
        xi_grid, yi_grid = np.meshgrid(xi, yi)
        
        # 执行插值
        try:
            # 首先尝试cubic插值
            topography = griddata(positions, values, (xi_grid, yi_grid), 
                                method=self.config.INTERPOLATION_METHOD, 
                                fill_value=0)
            
            # 检查插值结果
            if np.all(np.isnan(topography)):
                raise ValueError("Cubic interpolation failed")
                
        except (ValueError, Exception) as e:
            self.logger.warning(f"Cubic interpolation failed: {e}, trying linear interpolation")
            try:
                topography = griddata(positions, values, (xi_grid, yi_grid), 
                                    method='linear', fill_value=0)
            except Exception as e2:
                self.logger.warning(f"Linear interpolation failed: {e2}, trying nearest neighbor")
                topography = griddata(positions, values, (xi_grid, yi_grid), 
                                    method='nearest', fill_value=0)
        
        # 处理NaN值
        topography = np.nan_to_num(topography, nan=0.0, posinf=0.0, neginf=0.0)
        
        # 应用头部掩码
        topography[~self._head_mask] = 0
        
        # 平滑处理
        try:
            sigma = max(1.0, min(size) / 64.0)  # 自适应平滑参数
            topography = gaussian_filter(topography, sigma=sigma)
        except Exception as e:
            self.logger.warning(f"Gaussian filtering failed: {e}")
        
        return topography
    
    def generate_time_series_topographies(self, epochs_data: np.ndarray,
                                        electrode_positions: Dict[str, Tuple[float, float]],
                                        ch_names: List[str]) -> np.ndarray:
        """为时间序列数据生成地形图序列"""
        n_epochs, n_channels, n_times = epochs_data.shape
        size = self.config.TOPO_SIZE
        
        self.logger.info(f"生成地形图序列: {n_epochs} epochs, {n_channels} channels, {n_times} time points")
        
        # 初始化输出数组
        topographies = np.zeros((n_epochs, n_times, size[0], size[1]))
        
        # 预处理电极位置信息
        valid_electrode_indices = []
        valid_positions = []
        
        for i, ch_name in enumerate(ch_names):
            if ch_name in electrode_positions:
                pos = electrode_positions[ch_name]
                if abs(pos[0]) <= 1.2 and abs(pos[1]) <= 1.2:
                    valid_electrode_indices.append(i)
                    valid_positions.append(pos)
        
        if len(valid_positions) < 3:
            self.logger.error("Not enough valid electrodes for topography generation")
            return topographies
        
        self.logger.info(f"Using {len(valid_positions)} valid electrodes for interpolation")
        
        # 生成地形图
        for epoch in range(n_epochs):
            for time_point in range(n_times):
                # 提取有效电极的数据
                valid_data = epochs_data[epoch, valid_electrode_indices, time_point]
                
                # 检查数据质量
                if np.any(np.isfinite(valid_data)) and np.std(valid_data) > 1e-10:
                    try:
                        topo = self._generate_single_topography(valid_data, valid_positions)
                        topographies[epoch, time_point] = topo
                    except Exception as e:
                        self.logger.warning(f"Failed to generate topography for epoch {epoch}, time {time_point}: {e}")
                        topographies[epoch, time_point] = np.zeros(size)
                else:
                    # 数据质量不好或全为零，使用零地形图
                    topographies[epoch, time_point] = np.zeros(size)
        
        return topographies
    
    def _generate_single_topography(self, eeg_data: np.ndarray, 
                                   positions: List[Tuple[float, float]]) -> np.ndarray:
        """生成单个地形图（内部方法，已知数据有效）"""
        size = self.config.TOPO_SIZE
        positions = np.array(positions)
        
        # 创建插值网格
        xi = np.linspace(-1.2, 1.2, size[1])
        yi = np.linspace(-1.2, 1.2, size[0])
        xi_grid, yi_grid = np.meshgrid(xi, yi)
        
        # 执行插值
        try:
            topography = griddata(positions, eeg_data, (xi_grid, yi_grid), 
                                method=self.config.INTERPOLATION_METHOD, 
                                fill_value=0)
            
            if np.all(np.isnan(topography)):
                topography = griddata(positions, eeg_data, (xi_grid, yi_grid), 
                                    method='linear', fill_value=0)
        except:
            topography = griddata(positions, eeg_data, (xi_grid, yi_grid), 
                                method='nearest', fill_value=0)
        
        # 处理异常值
        topography = np.nan_to_num(topography, nan=0.0, posinf=0.0, neginf=0.0)
        
        # 应用头部掩码
        topography[~self._head_mask] = 0
        
        # 平滑处理
        sigma = max(1.0, min(size) / 64.0)
        topography = gaussian_filter(topography, sigma=sigma)
        
        return topography
    
    def normalize_topography(self, topography: np.ndarray, 
                           method: str = 'minmax') -> np.ndarray:
        """标准化地形图"""
        # 只考虑头部区域内的值
        masked_topo = topography[self._head_mask]
        
        if len(masked_topo) == 0 or np.all(masked_topo == 0):
            return topography
        
        if method == 'minmax':
            topo_min = np.min(masked_topo)
            topo_max = np.max(masked_topo)
            
            if topo_max > topo_min:
                # 只标准化头部区域
                normalized = topography.copy()
                normalized[self._head_mask] = (masked_topo - topo_min) / (topo_max - topo_min)
                return normalized
            else:
                return topography
                
        elif method == 'zscore':
            mean_val = np.mean(masked_topo)
            std_val = np.std(masked_topo)
            
            if std_val > 1e-10:
                normalized = topography.copy()
                normalized[self._head_mask] = (masked_topo - mean_val) / std_val
                return normalized
            else:
                return topography
                
        elif method == 'robust':
            # 使用中位数和四分位距进行robust标准化
            median_val = np.median(masked_topo)
            q75, q25 = np.percentile(masked_topo, [75, 25])
            iqr = q75 - q25
            
            if iqr > 1e-10:
                normalized = topography.copy()
                normalized[self._head_mask] = (masked_topo - median_val) / iqr
                return normalized
            else:
                return topography
        else:
            return topography
    
    def enhance_topography(self, topography: np.ndarray, 
                          enhancement_factor: float = 1.5) -> np.ndarray:
        """增强地形图对比度"""
        enhanced = topography.copy()
        
        # 只处理头部区域
        masked_data = enhanced[self._head_mask]
        
        if len(masked_data) > 0:
            # 使用sigmoid函数增强对比度
            mean_val = np.mean(masked_data)
            enhanced_data = mean_val + (masked_data - mean_val) * enhancement_factor
            
            # 使用tanh函数平滑截断
            enhanced_data = np.tanh(enhanced_data)
            
            enhanced[self._head_mask] = enhanced_data
        
        return enhanced
    
    def save_topography(self, topography: np.ndarray, filepath: str,
                       colormap: str = None, title: str = "") -> None:
        """保存地形图"""
        if colormap is None:
            colormap = self.config.COLORMAP
        
        plt.figure(figsize=(8, 8))
        
        # 创建地形图
        im = plt.imshow(topography, cmap=colormap, interpolation='bilinear', origin='upper')
        
        # 添加颜色条
        cbar = plt.colorbar(im, shrink=0.8)
        cbar.set_label('激活强度 (μV)', fontsize=12)
        
        # 添加头部轮廓
        center = (topography.shape[1]//2, topography.shape[0]//2)
        radius = min(topography.shape)//2 - 5
        circle = plt.Circle(center, radius, fill=False, color='black', linewidth=2)
        plt.gca().add_patch(circle)
        
        plt.title(title if title else 'EEG Topography', fontsize=14, fontweight='bold')
        plt.axis('off')
        plt.tight_layout()
        
        try:
            plt.savefig(filepath, dpi=150, bbox_inches='tight')
            self.logger.info(f"Topography saved to {filepath}")
        except Exception as e:
            self.logger.error(f"Failed to save topography to {filepath}: {e}")
        finally:
            plt.close()
    
    def get_electrode_contributions(self, topography: np.ndarray,
                                  electrode_positions: Dict[str, Tuple[float, float]],
                                  ch_names: List[str]) -> Dict[str, float]:
        """计算各电极对地形图的贡献度"""
        contributions = {}
        size = self.config.TOPO_SIZE
        
        for ch_name in ch_names:
            if ch_name in electrode_positions:
                pos = electrode_positions[ch_name]
                pixel_x, pixel_y = self.electrode_to_pixel(pos, size)
                
                # 提取电极周围区域的平均值作为贡献度
                radius = 5
                y_min = max(0, pixel_y - radius)
                y_max = min(size[0], pixel_y + radius + 1)
                x_min = max(0, pixel_x - radius)
                x_max = min(size[1], pixel_x + radius + 1)
                
                region = topography[y_min:y_max, x_min:x_max]
                contributions[ch_name] = float(np.mean(region)) if region.size > 0 else 0.0
        
        return contributions
    
    def create_difference_topography(self, topo1: np.ndarray, topo2: np.ndarray) -> np.ndarray:
        """创建差异地形图"""
        if topo1.shape != topo2.shape:
            self.logger.error("Topographies must have the same shape for difference calculation")
            return np.zeros_like(topo1)
        
        difference = topo1 - topo2
        
        # 只保留头部区域的差异
        difference[~self._head_mask] = 0
        
        return difference

# ========== src/trajectory_analysis.py ==========
# 相对路径: src/trajectory_analysis.py
# 在项目中的相对位置: ./src/trajectory_analysis.py

import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform, euclidean
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import logging

# 使用fastdtw替代dtw
try:
    from fastdtw import fastdtw
    DTW_AVAILABLE = True
except ImportError:
    DTW_AVAILABLE = False

class TrajectoryAnalyzer:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        if not DTW_AVAILABLE:
            self.logger.warning("FastDTW not available, using Euclidean distance for trajectory comparison")
    
    def compute_trajectory_features(self, trajectory: np.ndarray) -> Dict:
        """计算轨迹特征"""
        if len(trajectory) < 2:
            return {}
        
        try:
            # 基本几何特征
            length = len(trajectory)
            
            # 计算逐步距离
            step_distances = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
            total_distance = np.sum(step_distances)
            displacement = np.linalg.norm(trajectory[-1] - trajectory[0])
            
            # 速度特征
            mean_velocity = np.mean(step_distances) if len(step_distances) > 0 else 0
            max_velocity = np.max(step_distances) if len(step_distances) > 0 else 0
            velocity_std = np.std(step_distances) if len(step_distances) > 0 else 0
            
            # 方向特征
            if length > 2:
                directions = np.diff(trajectory, axis=0)
                angles = np.arctan2(directions[:, 1], directions[:, 0])
                angle_changes = np.diff(angles)
                # 处理角度跳跃
                angle_changes = np.mod(angle_changes + np.pi, 2*np.pi) - np.pi
                mean_angle_change = np.mean(np.abs(angle_changes))
                tortuosity = total_distance / (displacement + 1e-8)
            else:
                mean_angle_change = 0
                tortuosity = 1
            
            # 覆盖区域
            if length > 2:
                min_coords = np.min(trajectory, axis=0)
                max_coords = np.max(trajectory, axis=0)
                ranges = max_coords - min_coords
                bounding_area = np.prod(ranges) if np.all(ranges > 0) else 0
            else:
                bounding_area = 0
            
            # 质心和散布
            centroid = np.mean(trajectory, axis=0)
            distances_to_centroid = np.linalg.norm(trajectory - centroid, axis=1)
            mean_spread = np.mean(distances_to_centroid)
            max_spread = np.max(distances_to_centroid)
            
            # 轨迹复杂度
            complexity = np.sum(np.abs(np.diff(step_distances))) if len(step_distances) > 1 else 0
            
            return {
                'length': length,
                'total_distance': total_distance,
                'displacement': displacement,
                'mean_velocity': mean_velocity,
                'max_velocity': max_velocity,
                'velocity_std': velocity_std,
                'mean_angle_change': mean_angle_change,
                'tortuosity': tortuosity,
                'bounding_area': bounding_area,
                'straightness': displacement / (total_distance + 1e-8),
                'mean_spread': mean_spread,
                'max_spread': max_spread,
                'complexity': complexity
            }
            
        except Exception as e:
            self.logger.error(f"Error computing trajectory features: {e}")
            return {}
    
    def compute_dtw_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """计算两条轨迹之间的DTW距离"""
        try:
            if DTW_AVAILABLE:
                distance, _ = fastdtw(traj1, traj2, dist=euclidean)
                return distance
            else:
                # 使用形状匹配的欧几里得距离作为后备
                return self.compute_shape_distance(traj1, traj2)
        except Exception as e:
            self.logger.warning(f"DTW computation failed: {e}, using shape distance")
            return self.compute_shape_distance(traj1, traj2)
    
    def compute_shape_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """计算轨迹形状距离（当DTW不可用时的后备方案）"""
        try:
            # 标准化轨迹长度
            from scipy.interpolate import interp1d
            
            # 选择较短轨迹的长度作为标准长度
            target_length = min(len(traj1), len(traj2), 50)  # 限制最大长度以提高效率
            
            if len(traj1) < 2 or len(traj2) < 2:
                return np.linalg.norm(traj1.flatten() - traj2.flatten())
            
            # 创建插值函数
            t1 = np.linspace(0, 1, len(traj1))
            t2 = np.linspace(0, 1, len(traj2))
            t_new = np.linspace(0, 1, target_length)
            
            # 对每个维度进行插值
            traj1_interp = np.zeros((target_length, traj1.shape[1]))
            traj2_interp = np.zeros((target_length, traj2.shape[1]))
            
            for dim in range(traj1.shape[1]):
                f1 = interp1d(t1, traj1[:, dim], kind='linear', bounds_error=False, fill_value='extrapolate')
                f2 = interp1d(t2, traj2[:, dim], kind='linear', bounds_error=False, fill_value='extrapolate')
                traj1_interp[:, dim] = f1(t_new)
                traj2_interp[:, dim] = f2(t_new)
            
            # 计算欧几里得距离
            return np.linalg.norm(traj1_interp - traj2_interp)
            
        except Exception as e:
            self.logger.warning(f"Shape distance computation failed: {e}")
            # 最后的后备方案：简单的端点距离
            return euclidean(traj1[-1], traj2[-1]) + euclidean(traj1[0], traj2[0])
    
    def compute_trajectory_similarity_matrix(self, trajectories: Dict) -> np.ndarray:
        """计算轨迹相似性矩阵"""
        trajectory_list = list(trajectories.values())
        n_trajectories = len(trajectory_list)
        
        if n_trajectories < 2:
            return np.array([[1.0]])
        
        # 计算距离矩阵
        distance_matrix = np.zeros((n_trajectories, n_trajectories))
        
        self.logger.info(f"Computing similarity matrix for {n_trajectories} trajectories")
        
        for i in range(n_trajectories):
            for j in range(i+1, n_trajectories):
                traj1 = trajectory_list[i]['trajectory']
                traj2 = trajectory_list[j]['trajectory']
                
                try:
                    distance = self.compute_dtw_distance(traj1, traj2)
                    distance_matrix[i, j] = distance
                    distance_matrix[j, i] = distance
                except Exception as e:
                    self.logger.warning(f"Distance computation failed for trajectories {i}, {j}: {e}")
                    distance_matrix[i, j] = float('inf')
                    distance_matrix[j, i] = float('inf')
        
        # 转换为相似性矩阵
        max_distance = np.max(distance_matrix[distance_matrix != float('inf')])
        if max_distance > 0:
            # 处理无穷大值
            distance_matrix[distance_matrix == float('inf')] = max_distance * 2
            similarity_matrix = 1 - distance_matrix / (max_distance * 2)
        else:
            similarity_matrix = np.ones_like(distance_matrix)
        
        # 确保对角线为1
        np.fill_diagonal(similarity_matrix, 1.0)
        
        return similarity_matrix
    
    def cluster_trajectories(self, trajectories: Dict, method: str = 'hierarchical',
                           n_clusters: Optional[int] = None) -> Dict:
        """聚类轨迹"""
        if len(trajectories) < 2:
            return {'labels': [0], 'n_clusters': 1, 'trajectory_ids': list(trajectories.keys())}
        
        # 计算特征矩阵
        features = []
        trajectory_ids = []
        
        for traj_id, traj_data in trajectories.items():
            traj_features = self.compute_trajectory_features(traj_data['trajectory'])
            if traj_features:  # 确保特征不为空
                feature_vector = [
                    traj_features.get('total_distance', 0),
                    traj_features.get('displacement', 0),
                    traj_features.get('mean_velocity', 0),
                    traj_features.get('tortuosity', 1),
                    traj_features.get('straightness', 0),
                    traj_features.get('bounding_area', 0),
                    traj_features.get('mean_spread', 0),
                    traj_features.get('complexity', 0)
                ]
                features.append(feature_vector)
                trajectory_ids.append(traj_id)
        
        if len(features) < 2:
            return {'labels': [0], 'n_clusters': 1, 'trajectory_ids': trajectory_ids}
        
        features = np.array(features)
        
        # 标准化特征
        try:
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
        except Exception as e:
            self.logger.warning(f"Feature scaling failed: {e}, using original features")
            features_scaled = features
        
        # 处理NaN和无穷大值
        features_scaled = np.nan_to_num(features_scaled, nan=0.0, posinf=1.0, neginf=-1.0)
        
        try:
            if method == 'hierarchical':
                labels, n_clusters = self._hierarchical_clustering(features_scaled, n_clusters)
            elif method == 'kmeans':
                labels, n_clusters = self._kmeans_clustering(features_scaled, n_clusters)
            elif method == 'dbscan':
                labels, n_clusters = self._dbscan_clustering(features_scaled)
            else:
                self.logger.error(f"Unknown clustering method: {method}")
                return {'labels': [0] * len(features), 'n_clusters': 1, 'trajectory_ids': trajectory_ids}
            
        except Exception as e:
            self.logger.error(f"Clustering failed: {e}")
            return {'labels': [0] * len(features), 'n_clusters': 1, 'trajectory_ids': trajectory_ids}
        
        return {
            'labels': labels,
            'n_clusters': n_clusters,
            'trajectory_ids': trajectory_ids,
            'features': features_scaled
        }
    
    def _hierarchical_clustering(self, features: np.ndarray, n_clusters: Optional[int]) -> Tuple[np.ndarray, int]:
        """层次聚类"""
        linkage_matrix = linkage(features, method='ward')
        
        if n_clusters is None:
            # 自动确定聚类数
            max_clusters = min(len(features) // 2, 5)
            best_score = -1
            best_n_clusters = 2
            
            for n in range(2, max_clusters + 1):
                try:
                    labels = fcluster(linkage_matrix, n, criterion='maxclust')
                    if len(np.unique(labels)) > 1:
                        score = silhouette_score(features, labels)
                        if score > best_score:
                            best_score = score
                            best_n_clusters = n
                except Exception as e:
                    self.logger.warning(f"Silhouette score computation failed for n={n}: {e}")
                    continue
            
            n_clusters = best_n_clusters
        
        labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')
        return labels, n_clusters
    
    def _kmeans_clustering(self, features: np.ndarray, n_clusters: Optional[int]) -> Tuple[np.ndarray, int]:
        """K-means聚类"""
        if n_clusters is None:
            n_clusters = min(len(features) // 2, 3)
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(features)
        return labels, n_clusters
    
    def _dbscan_clustering(self, features: np.ndarray) -> Tuple[np.ndarray, int]:
        """DBSCAN聚类"""
        dbscan = DBSCAN(eps=0.5, min_samples=2)
        labels = dbscan.fit_predict(features)
        n_clusters = len(np.unique(labels[labels >= 0]))
        return labels, n_clusters
    
    def analyze_subject_consistency(self, subject_trajectories: Dict) -> Dict:
        """分析单个被试的轨迹一致性"""
        results = {}
        
        for subject_id, sessions in subject_trajectories.items():
            self.logger.info(f"Analyzing consistency for subject {subject_id}")
            subject_results = {}
            
            # 收集该被试的所有轨迹
            all_trajectories = {}
            for session_id, trajectories in sessions.items():
                for traj_id, traj_data in trajectories.items():
                    key = f"{session_id}_{traj_id}"
                    all_trajectories[key] = traj_data
            
            if len(all_trajectories) > 1:
                try:
                    # 计算相似性矩阵
                    similarity_matrix = self.compute_trajectory_similarity_matrix(all_trajectories)
                    
                    # 聚类分析
                    clustering_results = self.cluster_trajectories(all_trajectories)
                    
                    # 计算一致性指标
                    upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
                    mean_similarity = np.mean(upper_triangle)
                    consistency_score = mean_similarity
                    
                    subject_results = {
                        'n_trajectories': len(all_trajectories),
                        'mean_similarity': float(mean_similarity),
                        'consistency_score': float(consistency_score),
                        'n_clusters': clustering_results['n_clusters'],
                        'clustering': clustering_results,
                        'similarity_matrix': similarity_matrix
                    }
                    
                except Exception as e:
                    self.logger.error(f"Consistency analysis failed for subject {subject_id}: {e}")
                    subject_results = {
                        'n_trajectories': len(all_trajectories),
                        'error': str(e)
                    }
            else:
                subject_results = {
                    'n_trajectories': len(all_trajectories),
                    'note': 'Insufficient trajectories for analysis'
                }
            
            results[subject_id] = subject_results
        
        return results
    
    def compare_subjects(self, subject_trajectories: Dict) -> Dict:
        """比较不同被试之间的差异"""
        # 计算每个被试的总体特征
        subject_features = {}
        
        for subject_id, sessions in subject_trajectories.items():
            all_trajectories = {}
            for session_id, trajectories in sessions.items():
                for traj_id, traj_data in trajectories.items():
                    key = f"{session_id}_{traj_id}"
                    all_trajectories[key] = traj_data
            
            if all_trajectories:
                # 计算平均特征
                features_list = []
                for traj_data in all_trajectories.values():
                    traj_features = self.compute_trajectory_features(traj_data['trajectory'])
                    if traj_features:
                        features_list.append(traj_features)
                
                if features_list:
                    # 计算平均特征
                    mean_features = {}
                    for key in features_list[0].keys():
                        values = [f[key] for f in features_list if key in f and not np.isnan(f[key])]
                        mean_features[key] = np.mean(values) if values else 0
                    
                    subject_features[subject_id] = mean_features
        
        # 计算被试间距离矩阵
        inter_subject_similarity = {}
        if len(subject_features) > 1:
            subject_ids = list(subject_features.keys())
            n_subjects = len(subject_ids)
            distance_matrix = np.zeros((n_subjects, n_subjects))
            
            # 提取特征向量
            feature_keys = list(subject_features[subject_ids[0]].keys())
            subject_vectors = []
            
            for subject_id in subject_ids:
                vector = [subject_features[subject_id][key] for key in feature_keys]
                subject_vectors.append(vector)
            
            subject_vectors = np.array(subject_vectors)
            
            # 计算距离矩阵
            for i in range(n_subjects):
                for j in range(i+1, n_subjects):
                    dist = euclidean(subject_vectors[i], subject_vectors[j])
                    distance_matrix[i, j] = dist
                    distance_matrix[j, i] = dist
            
            # 转换为相似性
            max_dist = np.max(distance_matrix)
            if max_dist > 0:
                similarity_matrix = 1 - distance_matrix / max_dist
            else:
                similarity_matrix = np.ones_like(distance_matrix)
            
            np.fill_diagonal(similarity_matrix, 1.0)
            inter_subject_similarity = {
                'similarity_matrix': similarity_matrix,
                'subject_ids': subject_ids
            }
        
        return {
            'subject_features': subject_features,
            'n_subjects': len(subject_features),
            'inter_subject_similarity': inter_subject_similarity
        }
    
    def generate_summary_report(self, analysis_results: Dict) -> str:
        """生成分析报告"""
        report = []
        report.append("=" * 60)
        report.append("EEG轨迹分析报告")
        report.append("=" * 60)
        report.append("")
        
        # 总体统计
        if 'subject_consistency' in analysis_results:
            consistency_results = analysis_results['subject_consistency']
            n_subjects = len(consistency_results)
            report.append(f"分析被试数量: {n_subjects}")
            
            # 一致性统计
            consistency_scores = []
            trajectory_counts = []
            
            for subject_id, results in consistency_results.items():
                if 'consistency_score' in results:
                    consistency_scores.append(results['consistency_score'])
                    trajectory_counts.append(results['n_trajectories'])
            
            if consistency_scores:
                mean_consistency = np.mean(consistency_scores)
                std_consistency = np.std(consistency_scores)
                min_consistency = np.min(consistency_scores)
                max_consistency = np.max(consistency_scores)
                
                report.append(f"平均一致性得分: {mean_consistency:.3f} ± {std_consistency:.3f}")
                report.append(f"一致性得分范围: {min_consistency:.3f} - {max_consistency:.3f}")
                
                total_trajectories = sum(trajectory_counts)
                mean_trajectories = np.mean(trajectory_counts)
                report.append(f"总轨迹数量: {total_trajectories}")
                report.append(f"平均每被试轨迹数: {mean_trajectories:.1f}")
            
            report.append("")
        
        # 各被试详细结果
        if 'subject_consistency' in analysis_results:
            report.append("各被试一致性分析详情:")
            report.append("-" * 40)
            
            for subject_id, results in consistency_results.items():
                report.append(f"被试 {subject_id}:")
                
                if 'error' in results:
                    report.append(f"  分析出错: {results['error']}")
                elif 'note' in results:
                    report.append(f"  {results['note']}")
                    report.append(f"  轨迹数量: {results['n_trajectories']}")
                else:
                    report.append(f"  轨迹数量: {results['n_trajectories']}")
                    if 'consistency_score' in results:
                        report.append(f"  一致性得分: {results['consistency_score']:.3f}")
                        report.append(f"  聚类数量: {results['n_clusters']}")
                        
                        # 如果有聚类信息，显示聚类分布
                        if 'clustering' in results and 'labels' in results['clustering']:
                            labels = results['clustering']['labels']
                            unique_labels, counts = np.unique(labels, return_counts=True)
                            cluster_info = [f"簇{label}: {count}条轨迹" for label, count in zip(unique_labels, counts)]
                            report.append(f"  聚类分布: {', '.join(cluster_info)}")
                
                report.append("")
        
        # 被试间比较
        if 'subject_comparison' in analysis_results:
            comparison_results = analysis_results['subject_comparison']
            report.append("被试间比较:")
            report.append("-" * 40)
            
            if 'inter_subject_similarity' in comparison_results:
                similarity_info = comparison_results['inter_subject_similarity']
                if 'similarity_matrix' in similarity_info:
                    similarity_matrix = similarity_info['similarity_matrix']
                    # 计算平均被试间相似性（排除对角线）
                    upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
                    mean_inter_similarity = np.mean(upper_triangle)
                    report.append(f"平均被试间相似性: {mean_inter_similarity:.3f}")
                    
                    # 找出最相似和最不相似的被试对
                    subject_ids = similarity_info['subject_ids']
                    max_idx = np.unravel_index(np.argmax(upper_triangle), 
                                             (len(subject_ids), len(subject_ids)))
                    min_idx = np.unravel_index(np.argmin(upper_triangle), 
                                             (len(subject_ids), len(subject_ids)))
                    
                    if len(subject_ids) > 1:
                        # 重新计算上三角矩阵的索引
                        triu_indices = np.triu_indices_from(similarity_matrix, k=1)
                        max_pos = np.argmax(upper_triangle)
                        min_pos = np.argmin(upper_triangle)
                        
                        max_i, max_j = triu_indices[0][max_pos], triu_indices[1][max_pos]
                        min_i, min_j = triu_indices[0][min_pos], triu_indices[1][min_pos]
                        
                        report.append(f"最相似被试对: {subject_ids[max_i]} - {subject_ids[max_j]} "
                                    f"(相似性: {similarity_matrix[max_i, max_j]:.3f})")
                        report.append(f"最不相似被试对: {subject_ids[min_i]} - {subject_ids[min_j]} "
                                    f"(相似性: {similarity_matrix[min_i, min_j]:.3f})")
            
            report.append("")
        
        # 添加方法说明
        report.append("分析方法说明:")
        report.append("-" * 40)
        report.append("• 轨迹特征: 包括总距离、位移、速度、弯曲度、直线度等")
        if DTW_AVAILABLE:
            report.append("• 相似性计算: 使用动态时间规整(DTW)算法")
        else:
            report.append("• 相似性计算: 使用形状匹配的欧几里得距离")
        report.append("• 聚类方法: 层次聚类，自动确定最佳聚类数")
        report.append("• 一致性评分: 基于轨迹间平均相似性计算")
        
        return "\n".join(report)

# ========== src/visualization.py ==========
# 相对路径: src/visualization.py
# 在项目中的相对位置: ./src/visualization.py

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import seaborn as sns
import cv2
from typing import Dict, List, Tuple, Optional
import os
import logging
import warnings
import platform
import matplotlib.font_manager as fm

# 抑制警告
warnings.filterwarnings('ignore', category=UserWarning)

# 检查字体支持
def check_chinese_font_support():
    """检查是否支持中文字体"""
    try:
        # 获取系统可用字体
        available_fonts = [f.name for f in fm.fontManager.ttflist]
        
        system = platform.system()
        chinese_fonts = []
        
        if system == "Windows":
            chinese_fonts = ['Microsoft YaHei', 'SimHei', 'SimSun']
        elif system == "Darwin":  # macOS
            chinese_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti']
        else:  # Linux
            chinese_fonts = ['Noto Sans CJK SC', 'WenQuanYi Micro Hei', 'Droid Sans Fallback']
        
        for font in chinese_fonts:
            if font in available_fonts:
                return True, font
        
        return False, None
    except:
        return False, None

# 全局字体设置
CHINESE_FONT_AVAILABLE, FONT_NAME = check_chinese_font_support()

# 多语言标签字典
LABELS = {
    'activation_intensity': '激活强度 (μV)' if CHINESE_FONT_AVAILABLE else 'Activation Intensity (μV)',
    'x_coordinate': 'X坐标 (像素)' if CHINESE_FONT_AVAILABLE else 'X Coordinate (pixels)',
    'y_coordinate': 'Y坐标 (像素)' if CHINESE_FONT_AVAILABLE else 'Y Coordinate (pixels)',
    'trajectory': '轨迹' if CHINESE_FONT_AVAILABLE else 'Trajectory',
    'frame': '帧' if CHINESE_FONT_AVAILABLE else 'Frame',
    'length': '长度' if CHINESE_FONT_AVAILABLE else 'Length',
    'intensity': '强度' if CHINESE_FONT_AVAILABLE else 'Intensity',
    'points': '点' if CHINESE_FONT_AVAILABLE else 'points',
    'total_trajectories': '轨迹总数' if CHINESE_FONT_AVAILABLE else 'Total Trajectories',
    'average_length': '平均长度' if CHINESE_FONT_AVAILABLE else 'Average Length',
    'length_range': '长度范围' if CHINESE_FONT_AVAILABLE else 'Length Range',
    'topography': '地形图' if CHINESE_FONT_AVAILABLE else 'Topography',
    'cumulative_trajectories': '累积轨迹' if CHINESE_FONT_AVAILABLE else 'Cumulative Trajectories',
    'progress': '进度' if CHINESE_FONT_AVAILABLE else 'Progress',
    'similarity_score': '相似性分数' if CHINESE_FONT_AVAILABLE else 'Similarity Score',
    'trajectory_id': '轨迹ID' if CHINESE_FONT_AVAILABLE else 'Trajectory ID',
    'clustering_results': '轨迹聚类结果' if CHINESE_FONT_AVAILABLE else 'Trajectory Clustering Results',
    'cluster': '聚类' if CHINESE_FONT_AVAILABLE else 'Cluster',
    'trajectories_count': '条轨迹' if CHINESE_FONT_AVAILABLE else 'trajectories',
    'cluster_distribution': '聚类分布' if CHINESE_FONT_AVAILABLE else 'Cluster Distribution',
    'cluster_id': '聚类ID' if CHINESE_FONT_AVAILABLE else 'Cluster ID',
    'num_trajectories': '轨迹数量' if CHINESE_FONT_AVAILABLE else 'Number of Trajectories',
    'mean': '均值' if CHINESE_FONT_AVAILABLE else 'Mean',
    'median': '中位数' if CHINESE_FONT_AVAILABLE else 'Median',
    'frequency': '频次' if CHINESE_FONT_AVAILABLE else 'Frequency',
    'value': '值' if CHINESE_FONT_AVAILABLE else 'Value',
    'feature_analysis': '轨迹特征分析' if CHINESE_FONT_AVAILABLE else 'Trajectory Feature Analysis',
    'total_distance': '总距离' if CHINESE_FONT_AVAILABLE else 'Total Distance',
    'displacement': '位移' if CHINESE_FONT_AVAILABLE else 'Displacement',
    'mean_velocity': '平均速度' if CHINESE_FONT_AVAILABLE else 'Mean Velocity',
    'tortuosity': '弯曲度' if CHINESE_FONT_AVAILABLE else 'Tortuosity',
    'straightness': '直线度' if CHINESE_FONT_AVAILABLE else 'Straightness',
    'complexity': '复杂度' if CHINESE_FONT_AVAILABLE else 'Complexity',
    'bounding_area': '覆盖面积' if CHINESE_FONT_AVAILABLE else 'Bounding Area',
    'mean_spread': '平均散布' if CHINESE_FONT_AVAILABLE else 'Mean Spread'
}

class Visualizer:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # 设置颜色主题
        self.colors = plt.cm.Set1(np.linspace(0, 1, 10))
        self.background_color = '#f8f9fa'
        self.grid_color = '#e9ecef'
        
        # 设置matplotlib字体
        self._setup_matplotlib_font()
        
        # 确认字体设置状态
        if CHINESE_FONT_AVAILABLE:
            self.logger.info(f"使用中文字体: {FONT_NAME}")
        else:
            self.logger.info("使用英文标签")
    
    def _setup_matplotlib_font(self):
        """设置matplotlib字体"""
        try:
            if CHINESE_FONT_AVAILABLE and FONT_NAME:
                plt.rcParams['font.sans-serif'] = [FONT_NAME] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
            else:
                # 使用安全的英文字体
                plt.rcParams['font.family'] = 'DejaVu Sans'
                plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
        except Exception as e:
            self.logger.warning(f"字体设置失败: {e}")
            plt.rcParams['font.family'] = 'DejaVu Sans'
    
    def setup_figure_style(self, fig, title: str = ""):
        """统一设置图形样式"""
        fig.patch.set_facecolor(self.background_color)
        if title:
            fig.suptitle(title, fontsize=16, fontweight='bold', y=0.95)
    
    def add_head_outline(self, ax, center: Tuple[float, float], radius: float, 
                        color: str = 'black', linewidth: float = 2):
        """添加头部轮廓"""
        circle = plt.Circle(center, radius, fill=False, color=color, 
                           linewidth=linewidth, alpha=0.8)
        ax.add_patch(circle)
        
        # 添加鼻子标记
        nose_x, nose_y = center[0], center[1] + radius * 0.1
        ax.plot([nose_x], [nose_y], 'k^', markersize=8, alpha=0.8)
        
        # 添加耳朵标记
        ear_y = center[1]
        left_ear_x = center[0] - radius * 1.1
        right_ear_x = center[0] + radius * 1.1
        ax.plot([left_ear_x, right_ear_x], [ear_y, ear_y], 'k-', 
                linewidth=3, alpha=0.6)
    
    def plot_topography(self, topography: np.ndarray, title: str = "", 
                       save_path: Optional[str] = None, show_colorbar: bool = True,
                       electrode_positions: Optional[Dict] = None) -> None:
        """绘制单个地形图"""
        try:
            fig, ax = plt.subplots(figsize=(10, 8))
            self.setup_figure_style(fig, title)
            
            # 创建地形图
            im = ax.imshow(topography, cmap=self.config.COLORMAP, 
                          interpolation='bilinear', origin='upper',
                          extent=[0, topography.shape[1], topography.shape[0], 0])
            
            # 添加颜色条
            if show_colorbar:
                cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.02)
                cbar.set_label(LABELS['activation_intensity'], fontsize=12, fontweight='bold')
                cbar.ax.tick_params(labelsize=10)
            
            # 添加头部轮廓
            center = (topography.shape[1]//2, topography.shape[0]//2)
            radius = min(topography.shape)//2 - 5
            self.add_head_outline(ax, center, radius)
            
            # 如果提供了电极位置，标记电极
            if electrode_positions:
                self.plot_electrode_positions(ax, electrode_positions, topography.shape)
            
            # 设置坐标轴
            ax.set_xlim(0, topography.shape[1])
            ax.set_ylim(topography.shape[0], 0)
            ax.set_aspect('equal')
            ax.axis('off')
            
            # 添加标尺
            self.add_scale_bar(ax, topography.shape)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight', 
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Topography saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot topography: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def plot_electrode_positions(self, ax, electrode_positions: Dict, 
                               topography_shape: Tuple[int, int]):
        """在地形图上标记电极位置"""
        for ch_name, (x, y) in electrode_positions.items():
            # 转换坐标
            pixel_x = (x + 1) * topography_shape[1] / 2
            pixel_y = (1 - y) * topography_shape[0] / 2
            
            # 绘制电极点
            ax.plot(pixel_x, pixel_y, 'wo', markersize=6, 
                   markeredgecolor='black', markeredgewidth=1)
            
            # 添加电极标签（仅显示部分以避免拥挤）
            if ch_name in ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2']:
                ax.text(pixel_x, pixel_y-8, ch_name, ha='center', va='top',
                       fontsize=8, fontweight='bold', color='white',
                       bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7))
    
    def add_scale_bar(self, ax, shape: Tuple[int, int]):
        """添加比例尺"""
        scale_length = shape[1] // 10
        scale_x = shape[1] * 0.05
        scale_y = shape[0] * 0.95
        
        ax.plot([scale_x, scale_x + scale_length], [scale_y, scale_y], 
               'k-', linewidth=3)
        ax.text(scale_x + scale_length/2, scale_y - 5, f'{scale_length}px',
               ha='center', va='top', fontsize=10, fontweight='bold')
    
    def plot_trajectories(self, trajectories: Dict, topography_shape: Tuple[int, int],
                         title: str = "", save_path: Optional[str] = None,
                         show_legend: bool = True, alpha: float = 0.8) -> None:
        """绘制轨迹"""
        try:
            fig, ax = plt.subplots(figsize=(12, 10))
            self.setup_figure_style(fig, title)
            
            # 创建背景
            background = np.zeros(topography_shape)
            ax.imshow(background, cmap='gray', alpha=0.2, origin='upper',
                     extent=[0, topography_shape[1], topography_shape[0], 0])
            
            # 添加网格
            ax.grid(True, alpha=0.3, color=self.grid_color, linewidth=0.5)
            
            # 绘制轨迹
            legend_elements = []
            
            for i, (traj_id, traj_data) in enumerate(trajectories.items()):
                trajectory = traj_data['trajectory']
                color = self.colors[i % len(self.colors)]
                
                # 绘制轨迹线
                line = ax.plot(trajectory[:, 1], trajectory[:, 0], 
                              color=color, linewidth=3, alpha=alpha, 
                              label=f'{LABELS["trajectory"]} {traj_id}')[0]
                
                # 标记起点
                start_point = ax.scatter(trajectory[0, 1], trajectory[0, 0], 
                                       color=color, s=150, marker='o', 
                                       edgecolors='white', linewidth=2, 
                                       zorder=5, alpha=0.9)
                
                # 标记终点
                end_point = ax.scatter(trajectory[-1, 1], trajectory[-1, 0], 
                                     color=color, s=150, marker='s', 
                                     edgecolors='white', linewidth=2, 
                                     zorder=5, alpha=0.9)
                
                # 添加方向箭头
                self.add_direction_arrows(ax, trajectory, color, alpha)
                
                # 添加轨迹信息
                trajectory_info = f"{LABELS['trajectory']} {traj_id}\n{LABELS['length']}: {len(trajectory)}{LABELS['points']}"
                
                # 安全地获取强度信息
                intensity_value = None
                for key in ['mean_intensity', 'final_intensity', 'intensity']:
                    if key in traj_data:
                        intensity_value = traj_data[key]
                        break
                
                if intensity_value is not None:
                    trajectory_info += f"\n{LABELS['intensity']}: {intensity_value:.2f}"
                
                legend_elements.append((line, trajectory_info))
            
            # 添加头部轮廓
            center = (topography_shape[1]//2, topography_shape[0]//2)
            radius = min(topography_shape)//2 - 5
            self.add_head_outline(ax, center, radius)
            
            # 设置坐标轴
            ax.set_xlim(0, topography_shape[1])
            ax.set_ylim(topography_shape[0], 0)
            ax.set_aspect('equal')
            ax.set_xlabel(LABELS['x_coordinate'], fontsize=12, fontweight='bold')
            ax.set_ylabel(LABELS['y_coordinate'], fontsize=12, fontweight='bold')
            
            # 添加图例
            if show_legend and legend_elements:
                legend_lines = [elem[0] for elem in legend_elements]
                legend_labels = [elem[1] for elem in legend_elements]
                ax.legend(legend_lines, legend_labels, bbox_to_anchor=(1.05, 1), 
                         loc='upper left', fontsize=10)
            
            # 添加统计信息
            self.add_trajectory_stats(ax, trajectories, topography_shape)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight',
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Trajectories plot saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot trajectories: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def add_direction_arrows(self, ax, trajectory: np.ndarray, color, alpha: float):
        """添加轨迹方向箭头"""
        if len(trajectory) < 3:
            return
        
        # 在轨迹的几个关键点添加箭头
        arrow_positions = [len(trajectory)//4, len(trajectory)//2, 3*len(trajectory)//4]
        
        for pos in arrow_positions:
            if pos < len(trajectory) - 1:
                start = trajectory[pos]
                end = trajectory[pos + 1]
                
                dx = end[1] - start[1]
                dy = end[0] - start[0]
                
                # 只有移动足够大时才显示箭头
                if abs(dx) > 0.5 or abs(dy) > 0.5:
                    try:
                        ax.arrow(start[1], start[0], dx, dy, 
                                head_width=3, head_length=4, fc=color, ec=color,
                                alpha=alpha*0.7, length_includes_head=True)
                    except:
                        pass  # 忽略箭头绘制错误
    
    def add_trajectory_stats(self, ax, trajectories: Dict, shape: Tuple[int, int]):
        """添加轨迹统计信息"""
        stats_text = f"{LABELS['total_trajectories']}: {len(trajectories)}\n"
        
        if trajectories:
            lengths = [len(traj_data['trajectory']) for traj_data in trajectories.values()]
            stats_text += f"{LABELS['average_length']}: {np.mean(lengths):.1f}{LABELS['points']}\n"
            stats_text += f"{LABELS['length_range']}: {min(lengths)}-{max(lengths)}{LABELS['points']}"
        
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
               fontsize=10, verticalalignment='top',
               bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))
    
    def create_trajectory_animation(self, topographies: np.ndarray, 
                                  tracking_results: Dict,
                                  save_path: str, fps: int = None) -> None:
        """创建轨迹动画"""
        if fps is None:
            fps = self.config.FPS
        
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
            self.setup_figure_style(fig, f"EEG {LABELS['trajectory']} Animation")
            
            n_frames = min(len(topographies), 300)  # 限制帧数以节省时间和空间
            
            def animate(frame):
                ax1.clear()
                ax2.clear()
                
                # 左侧：当前地形图
                im1 = ax1.imshow(topographies[frame], cmap=self.config.COLORMAP, 
                               interpolation='bilinear', origin='upper')
                ax1.set_title(f'{LABELS["topography"]} - {LABELS["frame"]} {frame+1}/{n_frames}', 
                             fontweight='bold')
                
                # 添加头部轮廓
                center = (topographies.shape[2]//2, topographies.shape[1]//2)
                radius = min(topographies.shape[1:])//2 - 5
                self.add_head_outline(ax1, center, radius)
                
                # 右侧：累积轨迹
                ax2.imshow(np.zeros_like(topographies[0]), cmap='gray', 
                          alpha=0.3, origin='upper')
                
                # 绘制到目前为止的轨迹
                if frame < len(tracking_results['frame_results']):
                    frame_result = tracking_results['frame_results'][frame]
                    
                    colors = plt.cm.Set1(np.linspace(0, 1, 10))
                    color_idx = 0
                    
                    for region in frame_result.get('tracked_regions', []):
                        trajectory = np.array(region.trajectory)
                        color = colors[color_idx % len(colors)]
                        
                        if len(trajectory) > 1:
                            # 绘制轨迹线
                            ax1.plot(trajectory[:, 1], trajectory[:, 0], 
                                   color=color, linewidth=2, alpha=0.8)
                            ax2.plot(trajectory[:, 1], trajectory[:, 0], 
                                   color=color, linewidth=2, alpha=0.8)
                        
                        # 标记当前位置
                        if len(trajectory) > 0:
                            current_pos = trajectory[-1]
                            ax1.scatter(current_pos[1], current_pos[0], 
                                      s=100, c=color, marker='o', 
                                      edgecolors='white', linewidth=2, zorder=5)
                            ax2.scatter(current_pos[1], current_pos[0], 
                                      s=100, c=color, marker='o', 
                                      edgecolors='white', linewidth=2, zorder=5)
                        
                        color_idx += 1
                
                # 添加头部轮廓到右侧图
                self.add_head_outline(ax2, center, radius)
                
                ax2.set_title(LABELS['cumulative_trajectories'], fontweight='bold')
                
                # 设置坐标轴
                for ax in [ax1, ax2]:
                    ax.set_xlim(0, topographies.shape[2])
                    ax.set_ylim(topographies.shape[1], 0)
                    ax.set_aspect('equal')
                    ax.axis('off')
                
                # 添加进度条
                progress = frame / (n_frames - 1)
                ax2.text(0.5, -0.05, f'{LABELS["progress"]}: {progress*100:.1f}%', 
                        transform=ax2.transAxes, ha='center', fontweight='bold')
            
            # 创建动画
            anim = animation.FuncAnimation(fig, animate, frames=n_frames, 
                                         interval=1000//fps, blit=False, repeat=True)
            
            # 保存动画
            Writer = animation.writers['pillow']
            writer = Writer(fps=fps, metadata=dict(artist='EEG Tracker'), bitrate=1800)
            anim.save(save_path, writer=writer)
            
            plt.close()
            self.logger.info(f"Animation saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to save animation: {e}")
            # 尝试保存静态图像序列作为后备
            self.save_frame_sequence(topographies, tracking_results, 
                                   os.path.splitext(save_path)[0])
            if 'fig' in locals():
                plt.close(fig)
    
    def save_frame_sequence(self, topographies: np.ndarray, tracking_results: Dict,
                           base_path: str):
        """保存帧序列作为动画的后备方案"""
        frame_dir = f"{base_path}_frames"
        os.makedirs(frame_dir, exist_ok=True)
        
        n_frames_to_save = min(50, len(topographies))  # 限制帧数
        
        for frame in range(n_frames_to_save):
            try:
                fig, ax = plt.subplots(figsize=(10, 8))
                
                # 显示地形图
                ax.imshow(topographies[frame], cmap=self.config.COLORMAP, 
                         interpolation='bilinear', origin='upper')
                
                # 绘制轨迹
                if frame < len(tracking_results['frame_results']):
                    frame_result = tracking_results['frame_results'][frame]
                    
                    for i, region in enumerate(frame_result.get('tracked_regions', [])):
                        trajectory = np.array(region.trajectory)
                        color = self.colors[i % len(self.colors)]
                        
                        if len(trajectory) > 1:
                            ax.plot(trajectory[:, 1], trajectory[:, 0], 
                                   color=color, linewidth=2, alpha=0.8)
                        
                        if len(trajectory) > 0:
                            current_pos = trajectory[-1]
                            ax.scatter(current_pos[1], current_pos[0], 
                                      s=100, c=color, marker='o', 
                                      edgecolors='white', linewidth=2)
                
                # 添加头部轮廓
                center = (topographies.shape[2]//2, topographies.shape[1]//2)
                radius = min(topographies.shape[1:])//2 - 5
                self.add_head_outline(ax, center, radius)
                
                ax.set_title(f'{LABELS["frame"]} {frame+1}/{len(topographies)}', fontweight='bold')
                ax.axis('off')
                
                frame_path = os.path.join(frame_dir, f"frame_{frame:04d}.png")
                plt.savefig(frame_path, dpi=150, bbox_inches='tight')
                plt.close()
                
            except Exception as e:
                self.logger.warning(f"Failed to save frame {frame}: {e}")
                if 'fig' in locals():
                    plt.close(fig)
                continue
        
        self.logger.info(f"Frame sequence saved to {frame_dir}")
    
    def plot_similarity_matrix(self, similarity_matrix: np.ndarray,
                              labels: Optional[List[str]] = None,
                              title: str = "",
                              save_path: Optional[str] = None) -> None:
        """绘制相似性矩阵热图"""
        if not title:
            title = f"{LABELS['trajectory']} Similarity Matrix"
        
        try:
            fig, ax = plt.subplots(figsize=(12, 10))
            self.setup_figure_style(fig, title)
            
            # 创建热图
            mask = np.triu(np.ones_like(similarity_matrix, dtype=bool), k=1)
            
            sns.heatmap(similarity_matrix, 
                       annot=True, 
                       fmt='.3f',
                       cmap='RdYlBu_r', 
                       center=0.5,
                       square=True,
                       mask=mask,
                       xticklabels=labels, 
                       yticklabels=labels,
                       cbar_kws={"shrink": .8, "label": LABELS['similarity_score']},
                       ax=ax)
            
            ax.set_xlabel(LABELS['trajectory_id'], fontsize=12, fontweight='bold')
            ax.set_ylabel(LABELS['trajectory_id'], fontsize=12, fontweight='bold')
            
            # 添加统计信息
            upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
            if len(upper_triangle) > 0:
                stats_text = f"Average similarity: {np.mean(upper_triangle):.3f}\n"
                stats_text += f"Range: {np.min(upper_triangle):.3f} - {np.max(upper_triangle):.3f}"
                
                ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
                       fontsize=10, verticalalignment='top',
                       bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight',
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Similarity matrix saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot similarity matrix: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def plot_clustering_results(self, trajectories: Dict, clustering_results: Dict,
                               topography_shape: Tuple[int, int],
                               title: str = "",
                               save_path: Optional[str] = None) -> None:
        """绘制聚类结果"""
        if not title:
            title = LABELS['clustering_results']
        
        try:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))
            self.setup_figure_style(fig, title)
            
            # 获取聚类信息
            labels = clustering_results['labels']
            trajectory_ids = clustering_results['trajectory_ids']
            n_clusters = clustering_results['n_clusters']
            
            # 为每个聚类分配颜色
            cluster_colors = plt.cm.Set1(np.linspace(0, 1, max(n_clusters, 2)))
            
            # 左侧：聚类轨迹图
            background = np.zeros(topography_shape)
            ax1.imshow(background, cmap='gray', alpha=0.2, origin='upper')
            
            # 记录每个聚类的轨迹数量
            cluster_counts = {}
            
            for i, traj_id in enumerate(trajectory_ids):
                if traj_id in trajectories:
                    trajectory = trajectories[traj_id]['trajectory']
                    cluster_id = labels[i]
                    color = cluster_colors[cluster_id % len(cluster_colors)]
                    
                    # 更新聚类计数
                    cluster_counts[cluster_id] = cluster_counts.get(cluster_id, 0) + 1
                    
                    # 绘制轨迹
                    line = ax1.plot(trajectory[:, 1], trajectory[:, 0], 
                                   color=color, linewidth=2, alpha=0.7)[0]
                    
                    # 标记起点和终点
                    ax1.scatter(trajectory[0, 1], trajectory[0, 0], 
                               color=color, s=80, marker='o', 
                               edgecolors='black', linewidth=1, alpha=0.8)
                    ax1.scatter(trajectory[-1, 1], trajectory[-1, 0], 
                               color=color, s=80, marker='s', 
                               edgecolors='black', linewidth=1, alpha=0.8)
            
            # 添加头部轮廓
            center = (topography_shape[1]//2, topography_shape[0]//2)
            radius = min(topography_shape)//2 - 5
            self.add_head_outline(ax1, center, radius)
            
            ax1.set_xlim(0, topography_shape[1])
            ax1.set_ylim(topography_shape[0], 0)
            ax1.set_aspect('equal')
            ax1.set_title(f'Clustered Trajectories ({n_clusters} clusters)', fontweight='bold')
            ax1.axis('off')
            
            # 创建图例
            legend_elements = []
            for cluster_id, count in cluster_counts.items():
                color = cluster_colors[cluster_id % len(cluster_colors)]
                legend_elements.append(plt.Line2D([0], [0], color=color, lw=3,
                                                label=f'{LABELS["cluster"]} {cluster_id} ({count} {LABELS["trajectories_count"]})'))
            
            if legend_elements:
                ax1.legend(handles=legend_elements, loc='center left', 
                          bbox_to_anchor=(1, 0.5), fontsize=10)
            
            # 右侧：聚类统计图
            if cluster_counts:
                cluster_ids = list(cluster_counts.keys())
                counts = list(cluster_counts.values())
                
                bars = ax2.bar(cluster_ids, counts, 
                              color=[cluster_colors[i % len(cluster_colors)] for i in cluster_ids], 
                              alpha=0.7)
                
                ax2.set_xlabel(LABELS['cluster_id'], fontsize=12, fontweight='bold')
                ax2.set_ylabel(LABELS['num_trajectories'], fontsize=12, fontweight='bold')
                ax2.set_title(LABELS['cluster_distribution'], fontweight='bold')
                ax2.grid(True, alpha=0.3)
                
                # 在柱子上添加数值标签
                for bar, count in zip(bars, counts):
                    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                            str(count), ha='center', va='bottom', fontweight='bold')
                
                # 添加统计信息
                total_trajectories = sum(counts)
                avg_per_cluster = total_trajectories / len(counts) if len(counts) > 0 else 0
                stats_text = f"Total: {total_trajectories}\n"
                stats_text += f"Average per cluster: {avg_per_cluster:.1f}\n"
                stats_text += f"Largest cluster: {max(counts) if counts else 0}\n"
                stats_text += f"Smallest cluster: {min(counts) if counts else 0}"
                
                ax2.text(0.02, 0.98, stats_text, transform=ax2.transAxes, 
                        fontsize=10, verticalalignment='top',
                        bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.9))
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight',
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Clustering results saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot clustering results: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def plot_trajectory_features(self, feature_data: Dict,
                               title: str = "",
                               save_path: Optional[str] = None) -> None:
        """绘制轨迹特征统计"""
        if not feature_data:
            self.logger.warning("No feature data provided for visualization")
            return
        
        if not title:
            title = LABELS['feature_analysis']
        
        try:
            # 准备数据
            features_df = []
            for traj_id, features in feature_data.items():
                row = features.copy()
                row['trajectory_id'] = traj_id
                features_df.append(row)
            
            if not features_df:
                self.logger.warning("No valid feature data")
                return
            
            try:
                import pandas as pd
                df = pd.DataFrame(features_df)
            except ImportError:
                self.logger.error("Pandas not available for feature plotting")
                return
            
            # 选择主要特征进行可视化
            feature_cols = ['total_distance', 'displacement', 'mean_velocity', 
                           'tortuosity', 'straightness', 'complexity']
            feature_cols = [col for col in feature_cols if col in df.columns]
            
            if not feature_cols:
                self.logger.warning("No valid feature columns found")
                return
            
            # 创建子图
            n_features = len(feature_cols)
            n_cols = 3
            n_rows = (n_features + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))
            self.setup_figure_style(fig, title)
            
            if n_rows == 1:
                axes = axes.reshape(1, -1)
            
            axes = axes.flatten()
            
            for i, feature in enumerate(feature_cols):
                if i < len(axes):
                    ax = axes[i]
                    data = df[feature].dropna()
                    
                    if len(data) > 0:
                        # 绘制直方图
                        n_bins = min(20, max(5, len(data)//3)) if len(data) > 1 else 1
                        ax.hist(data, bins=n_bins, alpha=0.7, color='skyblue', 
                               edgecolor='black', linewidth=0.5)
                        
                        # 添加统计线
                        if len(data) > 1:
                            mean_val = data.mean()
                            median_val = data.median()
                            ax.axvline(mean_val, color='red', linestyle='--', 
                                      linewidth=2, label=f'{LABELS["mean"]}: {mean_val:.3f}')
                            ax.axvline(median_val, color='orange', linestyle='--', 
                                      linewidth=2, label=f'{LABELS["median"]}: {median_val:.3f}')
                            ax.legend(fontsize=8)
                        
                        feature_name = LABELS.get(feature, feature.replace('_', ' ').title())
                        ax.set_title(feature_name, fontweight='bold')
                        ax.set_xlabel(LABELS['value'])
                        ax.set_ylabel(LABELS['frequency'])
                        ax.grid(True, alpha=0.3)
                        
                        # 添加统计信息
                        if len(data) > 1:
                            stats_text = f'N={len(data)}\nStd={data.std():.3f}'
                        else:
                            stats_text = f'N={len(data)}'
                            
                        ax.text(0.98, 0.98, stats_text, transform=ax.transAxes,
                               ha='right', va='top', fontsize=8,
                               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
            
            # 隐藏多余的子图
            for i in range(len(feature_cols), len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight',
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Feature analysis saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot trajectory features: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def create_comparison_plot(self, subject_trajectories: Dict, 
                             save_path: Optional[str] = None):
        """创建被试间比较图"""
        n_subjects = len(subject_trajectories)
        if n_subjects < 2:
            self.logger.warning("Need at least 2 subjects for comparison")
            return
        
        try:
            # 创建子图网格
            n_cols = min(3, n_subjects)
            n_rows = (n_subjects + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(6*n_cols, 6*n_rows))
            self.setup_figure_style(fig, "Subject Trajectory Comparison")
            
            if n_rows == 1:
                axes = axes.reshape(1, -1) if n_subjects > 1 else [axes]
            axes = axes.flatten()
            
            # 为每个被试绘制轨迹
            for i, (subject_id, sessions) in enumerate(subject_trajectories.items()):
                if i >= len(axes):
                    break
                    
                ax = axes[i]
                
                # 收集该被试的所有轨迹
                all_trajectories = {}
                for session_id, trajectories in sessions.items():
                    for traj_id, traj_data in trajectories.items():
                        key = f"{session_id}_{traj_id}"
                        all_trajectories[key] = traj_data
                
                if all_trajectories:
                    # 假设所有轨迹有相同的地形图尺寸
                    topo_shape = (128, 128)  # 默认尺寸
                    
                    # 创建背景
                    background = np.zeros(topo_shape)
                    ax.imshow(background, cmap='gray', alpha=0.2, origin='upper')
                    
                    # 绘制轨迹
                    colors = plt.cm.Set1(np.linspace(0, 1, len(all_trajectories)))
                    
                    for j, (traj_id, traj_data) in enumerate(all_trajectories.items()):
                        trajectory = traj_data['trajectory']
                        color = colors[j % len(colors)]
                        
                        ax.plot(trajectory[:, 1], trajectory[:, 0], 
                               color=color, linewidth=2, alpha=0.7)
                        
                        # 标记起点
                        ax.scatter(trajectory[0, 1], trajectory[0, 0], 
                                 color=color, s=60, marker='o', 
                                 edgecolors='white', linewidth=1)
                    
                    # 添加头部轮廓
                    center = (topo_shape[1]//2, topo_shape[0]//2)
                    radius = min(topo_shape)//2 - 5
                    self.add_head_outline(ax, center, radius)
                    
                    ax.set_title(f'Subject {subject_id}\n({len(all_trajectories)} trajectories)', 
                               fontweight='bold')
                    ax.set_xlim(0, topo_shape[1])
                    ax.set_ylim(topo_shape[0], 0)
                    ax.set_aspect('equal')
                    ax.axis('off')
            
            # 隐藏多余的子图
            for i in range(n_subjects, len(axes)):
                axes[i].set_visible(False)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight',
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Comparison plot saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to create comparison plot: {e}")
            if 'fig' in locals():
                plt.close(fig)

# ========== trackers/__init__.py ==========
# 相对路径: trackers/__init__.py
# 在项目中的相对位置: ./trackers/__init__.py

# 跟踪算法模块
"""
EEG轨迹跟踪算法集合
包含多种不同的轨迹跟踪算法实现
"""

from .base_tracker import BaseTracker
from .greedy_tracker import GreedyTracker
from .hungarian_tracker import HungarianTracker
from .kalman_tracker import KalmanTracker
from .overlap_tracker import OverlapTracker
from .hybrid_tracker import HybridTracker
from .tracker_factory import TrackerFactory

__all__ = [
    'BaseTracker',
    'GreedyTracker', 
    'HungarianTracker',
    'KalmanTracker',
    'OverlapTracker',
    'HybridTracker',
    'TrackerFactory'
]

__version__ = "2.0.0"

# ========== trackers/base_tracker.py ==========
# 相对路径: trackers/base_tracker.py
# 在项目中的相对位置: ./trackers/base_tracker.py

import numpy as np
import cv2
from scipy.ndimage import label, center_of_mass
import logging
import time
from typing import List, Tuple, Dict, Optional
from abc import ABC, abstractmethod

class Region:
    """轨迹区域类"""
    def __init__(self, center: Tuple[float, float], area: float, intensity: float, id: int):
        self.center = center
        self.area = area
        self.intensity = intensity
        self.id = id
        self.trajectory = [center]
        self.active = True
        self.inactive_frames = 0
        self.max_inactive_frames = 25
        self.velocity_history = []
        self.predicted_position = None
        self.last_mask = None
        self.quality_score = 0.0

class BaseTracker(ABC):
    """基础跟踪器抽象类"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.regions = []
        self.next_region_id = 0
        
        # 算法名称（由子类设置）
        self.algorithm_name = "base"
        
        # 性能统计
        self.performance_stats = {
            'total_frames': 0,
            'total_detections': 0,
            'total_matches': 0,
            'computation_times': [],
            'memory_usage': []
        }
    
    def detect_high_activation_regions(self, topography: np.ndarray, frame_idx: int = 0) -> List[Dict]:
        """检测高激活区域 - 通用实现"""
        try:
            # 只考虑非零区域（头部内部）
            valid_mask = topography != 0
            if not np.any(valid_mask):
                return []
            
            valid_values = topography[valid_mask]
            
            # 自适应阈值计算
            threshold = np.percentile(valid_values, self.config.THRESHOLD_PERCENTILE)
            
            # 二值化
            binary = (topography > threshold) & valid_mask
            
            if not np.any(binary):
                # 降低阈值重试
                threshold = np.percentile(valid_values, max(70, self.config.THRESHOLD_PERCENTILE - 15))
                binary = (topography > threshold) & valid_mask
                
                if not np.any(binary):
                    return []
            
            # 形态学操作清理噪声
            try:
                kernel = np.ones((3, 3), np.uint8)
                binary = cv2.morphologyEx(binary.astype(np.uint8), cv2.MORPH_OPEN, kernel)
                binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            except:
                pass
            
            # 连通域分析
            labeled_array, num_features = label(binary)
            
            regions = []
            for i in range(1, num_features + 1):
                region_mask = labeled_array == i
                area = np.sum(region_mask)
                
                if area < self.config.MIN_REGION_SIZE:
                    continue
                
                # 计算质心
                center = center_of_mass(region_mask)
                
                # 计算强度统计
                region_values = topography[region_mask]
                intensity = np.mean(region_values)
                max_intensity = np.max(region_values)
                
                regions.append({
                    'center': center,
                    'area': area,
                    'intensity': intensity,
                    'max_intensity': max_intensity,
                    'mask': region_mask,
                    'threshold_used': threshold
                })
            
            # 按强度和面积的组合排序
            regions.sort(key=lambda x: x['intensity'] * np.sqrt(x['area']), reverse=True)
            selected_regions = regions[:self.config.MAX_REGIONS]
            
            return selected_regions
            
        except Exception as e:
            self.logger.error(f"区域检测失败: {e}")
            return []
    
    @abstractmethod
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = 20.0, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """匹配区域 - 由子类实现具体算法"""
        pass
    
    def update_tracker(self, topography: np.ndarray, frame_idx: int = 0) -> Dict:
        """更新跟踪器 - 通用框架"""
        start_time = time.time()
        
        try:
            # 检测当前帧的区域
            current_regions = self.detect_high_activation_regions(topography, frame_idx)
            
            # 使用具体算法进行匹配
            matches = self.match_regions(current_regions, frame_idx=frame_idx)
            
            # 更新匹配的区域
            active_regions = [r for r in self.regions if r.active]
            matched_tracked = set()
            matched_current = set()
            
            for tracked_idx, current_idx in matches:
                if tracked_idx < len(active_regions) and current_idx < len(current_regions):
                    region = active_regions[tracked_idx]
                    current_region = current_regions[current_idx]
                    
                    # 更新轨迹
                    region.trajectory.append(current_region['center'])
                    region.area = current_region['area']
                    region.intensity = current_region['intensity']
                    region.inactive_frames = 0
                    region.last_mask = current_region.get('mask')
                    
                    matched_tracked.add(tracked_idx)
                    matched_current.add(current_idx)
            
            # 处理未匹配的跟踪区域
            for i, region in enumerate(active_regions):
                if i not in matched_tracked:
                    region.inactive_frames += 1
                    
                    if region.inactive_frames >= region.max_inactive_frames:
                        region.active = False
            
            # 为未匹配的当前区域创建新的跟踪区域
            for i, current_region in enumerate(current_regions):
                if i not in matched_current:
                    new_region = Region(
                        center=current_region['center'],
                        area=current_region['area'],
                        intensity=current_region['intensity'],
                        id=self.next_region_id
                    )
                    new_region.last_mask = current_region.get('mask')
                    self.regions.append(new_region)
                    self.next_region_id += 1
            
            # 更新性能统计
            computation_time = time.time() - start_time
            self.performance_stats['computation_times'].append(computation_time)
            self.performance_stats['total_frames'] += 1
            self.performance_stats['total_detections'] += len(current_regions)
            self.performance_stats['total_matches'] += len(matches)
            
            return {
                'current_regions': current_regions,
                'tracked_regions': [r for r in self.regions if r.active],
                'all_regions': self.regions,
                'frame_idx': frame_idx,
                'matches': matches,
                'algorithm': self.algorithm_name
            }
            
        except Exception as e:
            self.logger.error(f"跟踪更新失败: {e}")
            return {
                'current_regions': [],
                'tracked_regions': [],
                'all_regions': self.regions,
                'frame_idx': frame_idx,
                'error': str(e),
                'algorithm': self.algorithm_name
            }
    
    def track_sequence(self, topographies: np.ndarray) -> Dict:
        """跟踪整个序列"""
        n_frames = topographies.shape[0]
        tracking_results = []
        
        # 重置跟踪器
        self.reset_tracker()
        
        self.logger.info(f"开始使用{self.algorithm_name}算法跟踪{n_frames}帧")
        
        start_time = time.time()
        
        try:
            for frame_idx in range(n_frames):
                topography = topographies[frame_idx]
                result = self.update_tracker(topography, frame_idx)
                result['frame'] = frame_idx
                tracking_results.append(result)
            
            total_time = time.time() - start_time
            
            # 提取轨迹
            trajectories = self.extract_trajectories()
            
            # 计算性能指标
            metrics = self.calculate_performance_metrics(trajectories, total_time)
            
            self.logger.info(f"{self.algorithm_name}算法完成: {len(trajectories)}条轨迹, "
                           f"耗时{total_time:.2f}秒")
            
            return {
                'algorithm': self.algorithm_name,
                'frame_results': tracking_results,
                'trajectories': trajectories,
                'metrics': metrics,
                'summary': {
                    'total_regions': len(self.regions),
                    'tracked_regions': len(trajectories),
                    'total_frames': n_frames,
                    'total_time': total_time
                }
            }
            
        except Exception as e:
            self.logger.error(f"{self.algorithm_name}算法跟踪失败: {e}")
            return {
                'algorithm': self.algorithm_name,
                'frame_results': tracking_results,
                'trajectories': {},
                'metrics': {},
                'summary': {
                    'total_regions': 0,
                    'tracked_regions': 0,
                    'total_frames': n_frames,
                    'error': str(e)
                }
            }
    
    def reset_tracker(self):
        """重置跟踪器状态"""
        self.regions = []
        self.next_region_id = 0
        self.performance_stats = {
            'total_frames': 0,
            'total_detections': 0,
            'total_matches': 0,
            'computation_times': [],
            'memory_usage': []
        }
    
    def extract_trajectories(self) -> Dict:
        """提取有效轨迹"""
        trajectories = {}
        
        for region in self.regions:
            if len(region.trajectory) > 2:  # 至少跟踪了3帧
                try:
                    trajectory_array = np.array(region.trajectory)
                    
                    # 计算基本统计
                    distances = np.linalg.norm(np.diff(trajectory_array, axis=0), axis=1)
                    total_distance = np.sum(distances)
                    avg_velocity = np.mean(distances) if len(distances) > 0 else 0
                    
                    # 计算轨迹质量分数
                    quality_score = self.compute_trajectory_quality(region)
                    
                    trajectories[region.id] = {
                        'trajectory': trajectory_array,
                        'length': len(region.trajectory),
                        'mean_intensity': float(getattr(region, 'intensity', 0.0)),
                        'area': float(getattr(region, 'area', 0.0)),
                        'total_distance': float(total_distance),
                        'avg_velocity': float(avg_velocity),
                        'inactive_frames': getattr(region, 'inactive_frames', 0),
                        'quality_score': float(quality_score)
                    }
                except Exception as e:
                    self.logger.warning(f"提取轨迹{region.id}失败: {e}")
                    continue
        
        return trajectories
    
    def compute_trajectory_quality(self, region: Region) -> float:
        """计算轨迹质量分数"""
        try:
            trajectory = np.array(region.trajectory)
            
            if len(trajectory) < 2:
                return 0.0
            
            # 长度分数
            length_score = min(1.0, len(trajectory) / 50.0)
            
            # 连续性分数
            continuity_score = max(0.0, 1.0 - region.inactive_frames / region.max_inactive_frames)
            
            # 运动平滑性分数
            if len(trajectory) >= 3:
                velocities = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
                if len(velocities) > 1:
                    velocity_var = np.var(velocities)
                    smoothness_score = max(0.0, 1.0 - velocity_var / 50.0)
                else:
                    smoothness_score = 0.8
            else:
                smoothness_score = 0.5
            
            # 强度一致性分数
            intensity_score = min(1.0, getattr(region, 'intensity', 0) / 0.5)
            
            # 综合质量分数
            quality_score = (length_score * 0.3 + 
                           continuity_score * 0.3 + 
                           smoothness_score * 0.25 + 
                           intensity_score * 0.15)
            
            return quality_score
            
        except Exception as e:
            self.logger.warning(f"质量计算失败: {e}")
            return 0.0
    
    def calculate_performance_metrics(self, trajectories: Dict, total_time: float) -> Dict:
        """计算性能指标"""
        metrics = {}
        
        try:
            # 基本指标
            metrics['trajectory_count'] = len(trajectories)
            metrics['computation_time'] = total_time
            
            if trajectories:
                lengths = [traj['length'] for traj in trajectories.values()]
                qualities = [traj['quality_score'] for traj in trajectories.values()]
                
                metrics['average_trajectory_length'] = np.mean(lengths)
                metrics['max_trajectory_length'] = np.max(lengths)
                metrics['trajectory_quality'] = np.mean(qualities)
                
                # 连续性指标
                metrics['tracking_continuity'] = self._calculate_continuity(trajectories)
                metrics['trajectory_smoothness'] = self._calculate_smoothness(trajectories)
            else:
                metrics['average_trajectory_length'] = 0
                metrics['max_trajectory_length'] = 0
                metrics['trajectory_quality'] = 0
                metrics['tracking_continuity'] = 0
                metrics['trajectory_smoothness'] = 0
            
            # 检测稳定性
            if self.performance_stats['computation_times']:
                avg_time = np.mean(self.performance_stats['computation_times'])
                time_std = np.std(self.performance_stats['computation_times'])
                metrics['detection_stability'] = 1.0 / (1.0 + time_std / avg_time) if avg_time > 0 else 0
            else:
                metrics['detection_stability'] = 0
            
            # 内存使用（简化版）
            try:
                import psutil
                metrics['memory_usage'] = psutil.Process().memory_info().rss / 1024 / 1024
            except ImportError:
                metrics['memory_usage'] = 0
            
        except Exception as e:
            self.logger.error(f"性能指标计算失败: {e}")
            for metric in self.config.EVALUATION_METRICS:
                metrics[metric] = 0.0
        
        return metrics
    
    def _calculate_continuity(self, trajectories: Dict) -> float:
        """计算跟踪连续性"""
        if not trajectories:
            return 0.0
        
        continuity_scores = []
        for traj_data in trajectories.values():
            trajectory = traj_data['trajectory']
            if len(trajectory) > 2:
                velocities = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
                if len(velocities) > 1:
                    velocity_stability = 1.0 / (1.0 + np.std(velocities))
                    continuity_scores.append(velocity_stability)
        
        return np.mean(continuity_scores) if continuity_scores else 0.0
    
    def _calculate_smoothness(self, trajectories: Dict) -> float:
        """计算轨迹平滑度"""
        if not trajectories:
            return 0.0
        
        smoothness_scores = []
        for traj_data in trajectories.values():
            trajectory = traj_data['trajectory']
            if len(trajectory) > 3:
                velocities = np.diff(trajectory, axis=0)
                accelerations = np.diff(velocities, axis=0)
                if len(accelerations) > 0:
                    acceleration_magnitude = np.linalg.norm(accelerations, axis=1)
                    if len(acceleration_magnitude) > 1:
                        smoothness = 1.0 / (1.0 + np.std(acceleration_magnitude))
                        smoothness_scores.append(smoothness)
        
        return np.mean(smoothness_scores) if smoothness_scores else 0.0
    
    def get_algorithm_info(self) -> Dict:
        """获取算法信息"""
        return {
            'name': self.algorithm_name,
            'description': getattr(self, 'description', '基础跟踪算法'),
            'parameters': getattr(self, 'algorithm_params', {}),
            'performance_stats': self.performance_stats
        }

# ========== trackers/greedy_tracker.py ==========
# 相对路径: trackers/greedy_tracker.py
# 在项目中的相对位置: ./trackers/greedy_tracker.py

import numpy as np
from scipy.spatial.distance import cdist
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class GreedyTracker(BaseTracker):
    """贪婪匹配跟踪算法"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "greedy"
        self.description = "贪婪匹配算法 - 快速局部最优解"
        
        # 获取算法特定配置
        alg_config = config.get_algorithm_config('greedy')
        self.distance_threshold = alg_config.get('distance_threshold', 25.0)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 25)
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """贪婪匹配算法实现"""
        if not current_regions:
            return []
        
        # 获取活跃区域
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # 使用配置的距离阈值
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            # 获取当前区域中心点和已跟踪区域的最新位置
            current_centers = np.array([r['center'] for r in current_regions])
            tracked_centers = np.array([r.trajectory[-1] for r in active_regions])
            
            # 计算距离矩阵
            distances = cdist(tracked_centers, current_centers)
            
            # 贪婪匹配：按距离从小到大进行匹配
            matches = []
            used_current = set()
            used_tracked = set()
            
            # 获取所有距离的排序索引
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for tracked_idx, current_idx in zip(dist_indices[0], dist_indices[1]):
                # 跳过已使用的区域
                if tracked_idx in used_tracked or current_idx in used_current:
                    continue
                
                # 检查距离是否在阈值内
                if distances[tracked_idx, current_idx] < distance_threshold:
                    matches.append((tracked_idx, current_idx))
                    used_tracked.add(tracked_idx)
                    used_current.add(current_idx)
                else:
                    # 由于按距离排序，后续距离只会更大，可以提前结束
                    break
            
            # 如果启用重连，尝试为未匹配的区域进行重连
            if self.enable_reconnection and len(matches) < len(active_regions):
                reconnection_matches = self._attempt_reconnection(
                    current_regions, active_regions, used_current, used_tracked, frame_idx
                )
                matches.extend(reconnection_matches)
            
            self.logger.debug(f"贪婪算法第{frame_idx}帧: 匹配{len(matches)}对区域")
            
            return matches
            
        except Exception as e:
            self.logger.error(f"贪婪匹配算法失败: {e}")
            return []
    
    def _attempt_reconnection(self, current_regions: List[Dict], 
                            active_regions: List, 
                            used_current: set, used_tracked: set, 
                            frame_idx: int) -> List[Tuple[int, int]]:
        """尝试重新连接断开的轨迹"""
        reconnection_matches = []
        
        try:
            # 扩大搜索距离进行重连
            reconnection_threshold = self.distance_threshold * 1.5
            
            # 获取未匹配的区域
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in used_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in used_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # 计算未匹配区域间的距离
            unmatched_tracked_centers = np.array([active_regions[i].trajectory[-1] for i in unmatched_tracked])
            unmatched_current_centers = np.array([current_regions[i]['center'] for i in unmatched_current])
            
            distances = cdist(unmatched_tracked_centers, unmatched_current_centers)
            
            # 贪婪重连匹配
            reconnection_used_current = set()
            reconnection_used_tracked = set()
            
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for rel_tracked_idx, rel_current_idx in zip(dist_indices[0], dist_indices[1]):
                if rel_tracked_idx in reconnection_used_tracked or rel_current_idx in reconnection_used_current:
                    continue
                
                if distances[rel_tracked_idx, rel_current_idx] < reconnection_threshold:
                    # 转换回原始索引
                    original_tracked_idx = unmatched_tracked[rel_tracked_idx]
                    original_current_idx = unmatched_current[rel_current_idx]
                    
                    # 检查非活跃帧数，如果太多则不重连
                    region = active_regions[original_tracked_idx]
                    if region.inactive_frames < self.max_inactive_frames:
                        reconnection_matches.append((original_tracked_idx, original_current_idx))
                        reconnection_used_tracked.add(rel_tracked_idx)
                        reconnection_used_current.add(rel_current_idx)
                        
                        self.logger.debug(f"重连轨迹{region.id}: 距离{distances[rel_tracked_idx, rel_current_idx]:.2f}")
            
        except Exception as e:
            self.logger.warning(f"重连尝试失败: {e}")
        
        return reconnection_matches
    
    def compute_match_quality(self, tracked_region, current_region, distance: float) -> float:
        """计算匹配质量分数"""
        try:
            # 距离分数（距离越小分数越高）
            distance_score = max(0, 1.0 - distance / self.distance_threshold)
            
            # 强度相似性分数
            intensity_diff = abs(tracked_region.intensity - current_region.get('intensity', 0))
            max_intensity = max(tracked_region.intensity, current_region.get('intensity', 0), 0.1)
            intensity_score = 1.0 - min(1.0, intensity_diff / max_intensity)
            
            # 面积相似性分数
            area_diff = abs(tracked_region.area - current_region.get('area', 0))
            max_area = max(tracked_region.area, current_region.get('area', 0), 1.0)
            area_score = 1.0 - min(1.0, area_diff / max_area)
            
            # 综合分数
            quality_score = (distance_score * 0.6 + 
                           intensity_score * 0.25 + 
                           area_score * 0.15)
            
            return quality_score
            
        except Exception as e:
            self.logger.warning(f"匹配质量计算失败: {e}")
            return distance_score if 'distance_score' in locals() else 0.0
    
    def get_algorithm_info(self) -> Dict:
        """获取算法信息"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "快速计算",
                "局部最优解",
                "贪婪策略",
                "支持轨迹重连"
            ],
            'advantages': [
                "计算效率高",
                "实现简单",
                "内存消耗低",
                "适合实时处理"
            ],
            'disadvantages': [
                "可能陷入局部最优",
                "对噪声敏感",
                "匹配质量不如全局算法"
            ],
            'best_for': [
                "实时处理场景",
                "计算资源有限场景",
                "轨迹数量较少场景"
            ]
        })
        return info

# ========== trackers/hungarian_tracker.py ==========
# 相对路径: trackers/hungarian_tracker.py
# 在项目中的相对位置: ./trackers/hungarian_tracker.py

 import numpy as np
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class HungarianTracker(BaseTracker):
    """匈牙利算法跟踪器 - 全局最优匹配"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "hungarian"
        self.description = "匈牙利算法 - 全局最优匹配解"
        
        # 获取算法特定配置
        alg_config = config.get_algorithm_config('hungarian')
        self.distance_threshold = alg_config.get('distance_threshold', 25.0)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 25)
        
        # 匈牙利算法特定参数
        self.cost_threshold = self.distance_threshold * 2  # 成本阈值
        self.use_weighted_cost = True  # 是否使用加权成本
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'cost_threshold': self.cost_threshold,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames,
            'use_weighted_cost': self.use_weighted_cost
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """匈牙利算法匹配实现"""
        if not current_regions:
            return []
        
        # 获取活跃区域
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # 使用配置的距离阈值
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            # 构建成本矩阵
            cost_matrix = self._build_cost_matrix(current_regions, active_regions)
            
            if cost_matrix.size == 0:
                return []
            
            # 使用匈牙利算法求解最优分配
            try:
                row_indices, col_indices = linear_sum_assignment(cost_matrix)
            except Exception as e:
                self.logger.warning(f"匈牙利算法求解失败: {e}, 使用备用贪婪算法")
                return self._fallback_greedy_matching(current_regions, active_regions, distance_threshold)
            
            # 筛选有效匹配
            matches = []
            for row_idx, col_idx in zip(row_indices, col_indices):
                cost = cost_matrix[row_idx, col_idx]
                
                # 只接受成本低于阈值的匹配
                if cost < self.cost_threshold:
                    matches.append((row_idx, col_idx))
                    self.logger.debug(f"匹配轨迹{active_regions[row_idx].id}与区域{col_idx}, 成本: {cost:.2f}")
            
            # 如果启用重连，尝试重连未匹配的区域
            if self.enable_reconnection and len(matches) < len(active_regions):
                reconnection_matches = self._attempt_hungarian_reconnection(
                    current_regions, active_regions, matches, frame_idx
                )
                matches.extend(reconnection_matches)
            
            self.logger.debug(f"匈牙利算法第{frame_idx}帧: 匹配{len(matches)}对区域")
            
            return matches
            
        except Exception as e:
            self.logger.error(f"匈牙利算法匹配失败: {e}")
            return self._fallback_greedy_matching(current_regions, active_regions, distance_threshold)
    
    def _build_cost_matrix(self, current_regions: List[Dict], active_regions: List) -> np.ndarray:
        """构建成本矩阵"""
        try:
            n_tracked = len(active_regions)
            n_current = len(current_regions)
            
            # 初始化成本矩阵
            cost_matrix = np.full((n_tracked, n_current), np.inf)
            
            # 计算各种成本组件
            for i, tracked_region in enumerate(active_regions):
                tracked_center = np.array(tracked_region.trajectory[-1])
                
                for j, current_region in enumerate(current_regions):
                    current_center = np.array(current_region['center'])
                    
                    # 计算综合成本
                    cost = self._calculate_assignment_cost(tracked_region, current_region, 
                                                         tracked_center, current_center)
                    cost_matrix[i, j] = cost
            
            return cost_matrix
            
        except Exception as e:
            self.logger.error(f"成本矩阵构建失败: {e}")
            return np.array([])
    
    def _calculate_assignment_cost(self, tracked_region, current_region, 
                                 tracked_center: np.ndarray, current_center: np.ndarray) -> float:
        """计算分配成本"""
        try:
            # 1. 基础距离成本
            distance = np.linalg.norm(tracked_center - current_center)
            distance_cost = distance
            
            # 如果距离超过阈值，返回极高成本
            if distance > self.distance_threshold:
                return np.inf
            
            if not self.use_weighted_cost:
                return distance_cost
            
            # 2. 强度差异成本
            intensity_diff = abs(tracked_region.intensity - current_region.get('intensity', 0))
            max_intensity = max(tracked_region.intensity, current_region.get('intensity', 0), 0.1)
            intensity_cost = intensity_diff / max_intensity * 10  # 权重为10
            
            # 3. 面积差异成本
            area_diff = abs(tracked_region.area - current_region.get('area', 0))
            max_area = max(tracked_region.area, current_region.get('area', 0), 1.0)
            area_cost = area_diff / max_area * 5  # 权重为5
            
            # 4. 速度一致性成本
            velocity_cost = 0
            if len(tracked_region.trajectory) >= 2:
                prev_velocity = np.array(tracked_region.trajectory[-1]) - np.array(tracked_region.trajectory[-2])
                current_velocity = current_center - tracked_center
                velocity_diff = np.linalg.norm(current_velocity - prev_velocity)
                velocity_cost = velocity_diff * 2  # 权重为2
            
            # 5. 非活跃帧数惩罚
            inactive_penalty = tracked_region.inactive_frames * 3  # 权重为3
            
            # 综合成本
            total_cost = (distance_cost + intensity_cost + area_cost + 
                         velocity_cost + inactive_penalty)
            
            return total_cost
            
        except Exception as e:
            self.logger.warning(f"成本计算失败: {e}")
            return distance if 'distance' in locals() else np.inf
    
    def _attempt_hungarian_reconnection(self, current_regions: List[Dict], 
                                      active_regions: List,
                                      existing_matches: List[Tuple[int, int]], 
                                      frame_idx: int) -> List[Tuple[int, int]]:
        """使用匈牙利算法尝试重连"""
        reconnection_matches = []
        
        try:
            # 获取未匹配的区域索引
            matched_tracked = set([m[0] for m in existing_matches])
            matched_current = set([m[1] for m in existing_matches])
            
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in matched_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in matched_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # 构建重连成本矩阵（使用更宽松的阈值）
            reconnection_threshold = self.distance_threshold * 2.0
            cost_matrix = np.full((len(unmatched_tracked), len(unmatched_current)), np.inf)
            
            for i, tracked_idx in enumerate(unmatched_tracked):
                tracked_region = active_regions[tracked_idx]
                
                # 只为非活跃帧数不太多的区域尝试重连
                if tracked_region.inactive_frames >= self.max_inactive_frames:
                    continue
                
                tracked_center = np.array(tracked_region.trajectory[-1])
                
                for j, current_idx in enumerate(unmatched_current):
                    current_region = current_regions[current_idx]
                    current_center = np.array(current_region['center'])
                    
                    distance = np.linalg.norm(tracked_center - current_center)
                    
                    if distance < reconnection_threshold:
                        # 重连时的成本包含非活跃惩罚
                        reconnection_cost = distance + tracked_region.inactive_frames * 5
                        cost_matrix[i, j] = reconnection_cost
            
            # 如果有可重连的组合，使用匈牙利算法
            if np.any(cost_matrix < np.inf):
                try:
                    row_indices, col_indices = linear_sum_assignment(cost_matrix)
                    
                    for row_idx, col_idx in zip(row_indices, col_indices):
                        if cost_matrix[row_idx, col_idx] < np.inf:
                            original_tracked_idx = unmatched_tracked[row_idx]
                            original_current_idx = unmatched_current[col_idx]
                            
                            reconnection_matches.append((original_tracked_idx, original_current_idx))
                            
                            region = active_regions[original_tracked_idx]
                            self.logger.debug(f"重连轨迹{region.id}: 成本{cost_matrix[row_idx, col_idx]:.2f}")
                            
                except Exception as e:
                    self.logger.warning(f"重连匈牙利算法失败: {e}")
            
        except Exception as e:
            self.logger.warning(f"重连尝试失败: {e}")
        
        return reconnection_matches
    
    def _fallback_greedy_matching(self, current_regions: List[Dict], 
                                active_regions: List, 
                                distance_threshold: float) -> List[Tuple[int, int]]:
        """备用贪婪匹配算法"""
        try:
            current_centers = np.array([r['center'] for r in current_regions])
            tracked_centers = np.array([r.trajectory[-1] for r in active_regions])
            
            distances = cdist(tracked_centers, current_centers)
            
            matches = []
            used_current = set()
            used_tracked = set()
            
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for tracked_idx, current_idx in zip(dist_indices[0], dist_indices[1]):
                if tracked_idx in used_tracked or current_idx in used_current:
                    continue
                
                if distances[tracked_idx, current_idx] < distance_threshold:
                    matches.append((tracked_idx, current_idx))
                    used_tracked.add(tracked_idx)
                    used_current.add(current_idx)
            
            return matches
            
        except Exception as e:
            self.logger.error(f"备用贪婪算法失败: {e}")
            return []
    
    def get_algorithm_info(self) -> Dict:
        """获取算法信息"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "全局最优解",
                "成本矩阵优化",
                "多因素权衡",
                "高精度匹配"
            ],
            'advantages': [
                "全局最优匹配",
                "考虑多种特征",
                "匹配质量高",
                "数学基础扎实"
            ],
            'disadvantages': [
                "计算复杂度较高",
                "内存消耗较大",
                "参数调优复杂"
            ],
            'best_for': [
                "高精度要求场景",
                "轨迹质量优先场景",
                "复杂匹配问题",
                "离线分析场景"
            ]
        })
        return info

# ========== trackers/tracker_factory.py ==========
# 相对路径: trackers/tracker_factory.py
# 在项目中的相对位置: ./trackers/tracker_factory.py

"""
跟踪器工厂类
用于创建和管理不同类型的跟踪算法
"""

import logging
from typing import Dict, List, Optional
from .base_tracker import BaseTracker
from .greedy_tracker import GreedyTracker
from .hungarian_tracker import HungarianTracker

# 简化版本的其他跟踪器
class KalmanTracker(BaseTracker):
    """卡尔曼预测跟踪器（简化版）"""
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "kalman"
        self.description = "卡尔曼预测算法 - 基于运动预测"
        alg_config = config.get_algorithm_config('kalman')
        self.distance_threshold = alg_config.get('distance_threshold', 30.0)
        self.prediction_weight = alg_config.get('prediction_weight', 0.4)
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'prediction_weight': self.prediction_weight
        }
    
    def match_regions(self, current_regions, distance_threshold=None, frame_idx=0):
        # 简化的卡尔曼预测匹配
        if not current_regions:
            return []
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        try:
            from scipy.spatial.distance import cdist
            from scipy.optimize import linear_sum_assignment
            import numpy as np
            
            # 预测位置
            predicted_centers = []
            for region in active_regions:
                if len(region.trajectory) >= 2:
                    velocity = np.array(region.trajectory[-1]) - np.array(region.trajectory[-2])
                    predicted = np.array(region.trajectory[-1]) + velocity * self.prediction_weight
                    predicted_centers.append(predicted)
                else:
                    predicted_centers.append(region.trajectory[-1])
            
            if not predicted_centers:
                return []
            
            predicted_centers = np.array(predicted_centers)
            current_centers = np.array([r['center'] for r in current_regions])
            distances = cdist(predicted_centers, current_centers)
            
            try:
                row_indices, col_indices = linear_sum_assignment(distances)
                matches = []
                threshold = distance_threshold or self.distance_threshold
                for row_idx, col_idx in zip(row_indices, col_indices):
                    if distances[row_idx, col_idx] < threshold:
                        matches.append((row_idx, col_idx))
                return matches
            except Exception as e:
                self.logger.warning(f"Hungarian assignment failed: {e}")
                return []
        except Exception as e:
            self.logger.error(f"Kalman matching failed: {e}")
            return []

class OverlapTracker(BaseTracker):
    """重叠度匹配跟踪器（简化版）"""
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "overlap"
        self.description = "重叠度匹配 - 基于区域重叠"
        alg_config = config.get_algorithm_config('overlap')
        self.overlap_threshold = alg_config.get('overlap_threshold', 0.3)
        self.distance_threshold = alg_config.get('distance_threshold', 35.0)
        
        self.algorithm_params = {
            'overlap_threshold': self.overlap_threshold,
            'distance_threshold': self.distance_threshold
        }
    
    def match_regions(self, current_regions, distance_threshold=None, frame_idx=0):
        # 简化的重叠度匹配
        if not current_regions:
            return []
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        try:
            import numpy as np
            
            matches = []
            used_current = set()
            used_tracked = set()
            
            for i, tracked_region in enumerate(active_regions):
                if i in used_tracked:
                    continue
                
                best_match = -1
                best_score = 0
                
                for j, current_region in enumerate(current_regions):
                    if j in used_current:
                        continue
                    
                    # 计算距离得分
                    distance = np.linalg.norm(
                        np.array(tracked_region.trajectory[-1]) - np.array(current_region['center'])
                    )
                    threshold = distance_threshold or self.distance_threshold
                    if distance > threshold:
                        continue
                    
                    distance_score = 1.0 - distance / threshold
                    
                    # 简化的重叠计算（基于距离）
                    overlap_score = max(0, 1.0 - distance / 20.0)
                    
                    total_score = distance_score * 0.6 + overlap_score * 0.4
                    
                    if total_score > best_score and total_score > 0.3:
                        best_score = total_score
                        best_match = j
                
                if best_match >= 0:
                    matches.append((i, best_match))
                    used_tracked.add(i)
                    used_current.add(best_match)
            
            return matches
        except Exception as e:
            self.logger.error(f"Overlap matching failed: {e}")
            return []

class HybridTracker(BaseTracker):
    """混合跟踪器（简化版）"""
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "hybrid"
        self.description = "混合算法 - 综合多种特征"
        alg_config = config.get_algorithm_config('hybrid')
        self.distance_threshold = alg_config.get('distance_threshold', 25.0)
        self.overlap_weight = alg_config.get('overlap_weight', 0.4)
        self.intensity_weight = alg_config.get('intensity_weight', 0.1)
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'overlap_weight': self.overlap_weight,
            'intensity_weight': self.intensity_weight
        }
    
    def match_regions(self, current_regions, distance_threshold=None, frame_idx=0):
        # 简化的混合匹配
        if not current_regions:
            return []
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        try:
            from scipy.spatial.distance import cdist
            from scipy.optimize import linear_sum_assignment
            import numpy as np
            
            # 计算综合得分矩阵
            score_matrix = np.zeros((len(active_regions), len(current_regions)))
            
            for i, tracked_region in enumerate(active_regions):
                tracked_center = np.array(tracked_region.trajectory[-1])
                
                for j, current_region in enumerate(current_regions):
                    current_center = np.array(current_region['center'])
                    
                    # 距离得分
                    distance = np.linalg.norm(tracked_center - current_center)
                    threshold = distance_threshold or self.distance_threshold
                    if distance < threshold:
                        distance_score = 1.0 - (distance / threshold)
                        
                        # 强度得分
                        intensity_diff = abs(tracked_region.intensity - current_region.get('intensity', 0))
                        max_intensity = max(tracked_region.intensity, current_region.get('intensity', 0), 0.1)
                        intensity_score = 1.0 - min(1.0, intensity_diff / max_intensity)
                        
                        # 面积得分
                        area_diff = abs(tracked_region.area - current_region.get('area', 0))
                        max_area = max(tracked_region.area, current_region.get('area', 0), 1.0)
                        area_score = 1.0 - min(1.0, area_diff / max_area)
                        
                        # 综合得分
                        total_score = (distance_score * 0.5 + 
                                     intensity_score * self.intensity_weight + 
                                     area_score * 0.1 + 
                                     0.3)  # 基础分
                        
                        score_matrix[i, j] = total_score
            
            # 使用匈牙利算法
            try:
                cost_matrix = 1.0 - score_matrix
                row_indices, col_indices = linear_sum_assignment(cost_matrix)
                matches = []
                for row_idx, col_idx in zip(row_indices, col_indices):
                    if score_matrix[row_idx, col_idx] > 0.3:
                        matches.append((row_idx, col_idx))
                return matches
            except Exception as e:
                self.logger.warning(f"Hungarian assignment failed: {e}")
                return []
        except Exception as e:
            self.logger.error(f"Hybrid matching failed: {e}")
            return []

class TrackerFactory:
    """跟踪器工厂类"""
    
    # 注册的跟踪器类
    _trackers = {
        'greedy': GreedyTracker,
        'hungarian': HungarianTracker, 
        'kalman': KalmanTracker,
        'overlap': OverlapTracker,
        'hybrid': HybridTracker
    }
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @classmethod
    def create_tracker(cls, algorithm_name: str, config) -> Optional[BaseTracker]:
        """创建指定类型的跟踪器"""
        logger = logging.getLogger(__name__)
        
        if algorithm_name not in cls._trackers:
            logger.error(f"未知的跟踪算法: {algorithm_name}")
            return None
        
        try:
            tracker_class = cls._trackers[algorithm_name]
            tracker = tracker_class(config)
            logger.info(f"成功创建{algorithm_name}跟踪器")
            return tracker
        except Exception as e:
            logger.error(f"创建{algorithm_name}跟踪器失败: {e}")
            import traceback
            logger.debug(f"详细错误信息: {traceback.format_exc()}")
            return None
    
    @classmethod
    def get_available_algorithms(cls) -> List[str]:
        """获取可用的算法列表"""
        return list(cls._trackers.keys())
    
    @classmethod
    def get_algorithm_info(cls, algorithm_name: str) -> Dict:
        """获取算法信息"""
        if algorithm_name not in cls._trackers:
            return {'error': f'未知算法: {algorithm_name}'}
        
        try:
            # 创建临时实例获取信息
            from config import Config
            temp_tracker = cls._trackers[algorithm_name](Config)
            return temp_tracker.get_algorithm_info()
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"获取{algorithm_name}算法信息失败: {e}")
            return {
                'name': algorithm_name,
                'description': cls._trackers[algorithm_name].__doc__ or f'{algorithm_name} 跟踪算法',
                'error': f'获取算法信息失败: {e}'
            }
    
    @classmethod
    def compare_algorithms(cls, config) -> Dict:
        """比较所有可用算法"""
        comparison = {
            'available_algorithms': cls.get_available_algorithms(),
            'algorithm_details': {}
        }
        
        for algorithm in cls.get_available_algorithms():
            info = cls.get_algorithm_info(algorithm)
            comparison['algorithm_details'][algorithm] = info
        
        return comparison
    
    @classmethod
    def register_tracker(cls, algorithm_name: str, tracker_class):
        """注册新的跟踪器类"""
        if not issubclass(tracker_class, BaseTracker):
            raise ValueError("跟踪器类必须继承自BaseTracker")
        
        cls._trackers[algorithm_name] = tracker_class
        logging.getLogger(__name__).info(f"注册新跟踪器: {algorithm_name}")
    
    @classmethod
    def create_all_trackers(cls, config, algorithms: Optional[List[str]] = None) -> Dict[str, BaseTracker]:
        """创建多个跟踪器"""
        if algorithms is None:
            algorithms = config.COMPARISON_ALGORITHMS
        
        trackers = {}
        
        for algorithm in algorithms:
            tracker = cls.create_tracker(algorithm, config)
            if tracker is not None:
                trackers[algorithm] = tracker
            else:
                logging.getLogger(__name__).warning(f"跳过创建失败的跟踪器: {algorithm}")
        
        return trackers
    
    @classmethod
    def validate_algorithm_config(cls, config) -> Dict[str, bool]:
        """验证算法配置"""
        validation_results = {}
        
        for algorithm in config.COMPARISON_ALGORITHMS:
            try:
                if algorithm in cls._trackers:
                    # 检查配置是否存在
                    alg_config = config.get_algorithm_config(algorithm)
                    validation_results[algorithm] = bool(alg_config)
                else:
                    validation_results[algorithm] = False
            except Exception as e:
                logging.getLogger(__name__).error(f"验证{algorithm}配置失败: {e}")
                validation_results[algorithm] = False
        
        return validation_results

