# é¡¹ç›®ç»“æž„ (tree /f æˆ– tree -af)
# ===============================
.
â”œâ”€â”€ ./.datalad
â”‚Â Â  â”œâ”€â”€ ./.datalad/.gitattributes
â”‚Â Â  â””â”€â”€ ./.datalad/config
â”œâ”€â”€ ./.git
â”‚Â Â  â”œâ”€â”€ ./.git/COMMIT_EDITMSG
â”‚Â Â  â”œâ”€â”€ ./.git/FETCH_HEAD
â”‚Â Â  â”œâ”€â”€ ./.git/HEAD
â”‚Â Â  â”œâ”€â”€ ./.git/annex
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/fsck
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/fsck/fsck.lck
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/annex/fsck/fsckdb
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ ./.git/annex/fsck/fsckdb/db
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/index
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/index.lck
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/journal
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/journal.lck
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/keysdb
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/annex/keysdb/db
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/keysdb.cache
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/keysdb.lck
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/mergedrefs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/othertmp
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/othertmp.lck
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/sentinal
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/sentinal.cache
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/annex/smudge.lck
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/annex/smudge.log
â”‚Â Â  â”œâ”€â”€ ./.git/branches
â”‚Â Â  â”œâ”€â”€ ./.git/config
â”‚Â Â  â”œâ”€â”€ ./.git/config.dataladlock
â”‚Â Â  â”œâ”€â”€ ./.git/description
â”‚Â Â  â”œâ”€â”€ ./.git/hooks
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/applypatch-msg.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/commit-msg.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/fsmonitor-watchman.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/post-checkout
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/post-merge
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/post-receive
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/post-update.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-applypatch.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-commit
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-commit.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-merge-commit.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-push.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-rebase.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/pre-receive.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/prepare-commit-msg.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/push-to-checkout.sample
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/hooks/sendemail-validate.sample
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/hooks/update.sample
â”‚Â Â  â”œâ”€â”€ ./.git/index
â”‚Â Â  â”œâ”€â”€ ./.git/info
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/info/attributes
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/info/exclude
â”‚Â Â  â”œâ”€â”€ ./.git/logs
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/logs/HEAD
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/logs/refs
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ ./.git/logs/refs/heads
â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ./.git/logs/refs/heads/git-annex
â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ ./.git/logs/refs/heads/main
â”‚Â Â  â”‚Â Â      â””â”€â”€ ./.git/logs/refs/remotes
â”‚Â Â  â”‚Â Â          â””â”€â”€ ./.git/logs/refs/remotes/origin
â”‚Â Â  â”‚Â Â              â”œâ”€â”€ ./.git/logs/refs/remotes/origin/HEAD
â”‚Â Â  â”‚Â Â              â””â”€â”€ ./.git/logs/refs/remotes/origin/main
â”‚Â Â  â”œâ”€â”€ ./.git/objects
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/0b
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/0b/0076e57c553ea9056c17ace9f16363e9900d65
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/0c
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/0c/f8e75b6fb6db990e90248106d6b8770a027e7b
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/0f
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/0f/1f79e38845e57f347552ea93d1f92e977a11c2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/1f
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/1f/0bd672157052c9887dcc3fb761b1159e2e9e58
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/24
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/24/bed0bdb41d2220f129a401af09cb65c7ee5de6
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/41
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/41/07637f7768c9a09d2297b6b9c708602ec3cbd0
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/42
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/42/d26db99b9b8d5f7e4d1079985ab31037cb3945
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/45
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/45/af84fcbea02e00d3659d37e5e420da39e46494
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/46
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/46/3ac0fad1d88cbf59f38cd6884c824ade4ca5a8
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/49
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/49/4612250187fc87654237b55540499cce09a9c9
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/4b
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/4b/825dc642cb6eb9a060e54bf8d69288fbee4904
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/4d
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/4d/58d446cfaf130a730751ff21ba1e517a0552a0
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/4e
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/4e/d05926487eb2bd84a73eac648e6b8678ddeee2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/59
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/59/28185081410e652fcb8b4446fae9caf43288e5
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/5a
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/5a/aba8aabfb55002fb1e3b56b40c1ec77aa91d59
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/5f
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/5f/84c346beb09d2eed54a14446d1b175c1857af0
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/6b
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/6b/38ca9dfe215971a9e329782ea0c3fdbf8a31b6
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/6c
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/6c/0b1c14df216291dae3cb585617d288d4bf48a2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/70
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/70/49eff93c7d4ed219919bce4b74370c152bd6d3
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/77
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/77/4154243c5ee80449641d9dfa77d8318c9943be
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/80
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/80/5770d083b03913ddcc30815b012d4a4fe91fd6
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/81
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/81/9ff3af34b2d388e3f76dda5d1ff53289897dda
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/82
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/82/ccb5d829f624f42c5994df967cec904ff2469c
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/86
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/86/726e91bc765cd628401aed13f1bc139134d156
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/9c
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/9c/e00ea2ccb3c6997a8d3004bd41ebfa96168c73
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/af
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/af/926ef0c359556ac1d36d71f7e173d97b893ff2
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/b6
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/b6/334bd9af8e300fd7f33f42869172e6a2cbe2d8
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/c2
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/c2/c0ea6f3921fd03337a9592657eaa200a780e7b
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/c5
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/c5/30856915c260cbfdf71c72974db3004aebd759
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/c8
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/c8/368359f3e7633965252fbd3b8047b37a45a325
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/d4
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/d4/9f3251d735c3e3775dd1c930e0c3fa5e867869
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/d5
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/d5/9574abe8fbaa810502881ab4ccfb738c1d705a
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/d9
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/d9/9cd4afa8955f94d4fde4a3846e121d77a7c87d
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/da
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/da/ad4c4a0fb4474a8408c7c763b1395a6e039d40
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/dc
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/dc/dfe8aaa7536290e92da3461a610c1917445078
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/e0
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/e0/ae8d49527e460a86a9e640b24cef3ca1d41ac0
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/e3
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/e3/9584b67a3ef09500ac880be852816b8e821d7d
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/e6
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/e6/9de29bb2d1d6434b8b29ae775ad8c2e48c5391
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/f2
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/f2/08d7457372a48b3e5e89ec9e4576b6145f98af
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/f8
â”‚Â Â  â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/f8/5b791bee41050ad7b29374397788189cb002f8
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/f8/f95e78c08d4309190cf8300a139062e32c9c6f
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/fd
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/fd/0aadefc75e3ad402b9a161f68980d4d940f9fa
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/ff
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/ff/818832bd2164da8f6f3de864d88535ed08928e
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./.git/objects/info
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./.git/objects/pack
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ ./.git/objects/pack/pack-44261f52b16e2554a4dac2d17f6c4669e26c8bc1.idx
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ ./.git/objects/pack/pack-44261f52b16e2554a4dac2d17f6c4669e26c8bc1.pack
â”‚Â Â  â”‚Â Â      â””â”€â”€ ./.git/objects/pack/pack-44261f52b16e2554a4dac2d17f6c4669e26c8bc1.rev
â”‚Â Â  â”œâ”€â”€ ./.git/packed-refs
â”‚Â Â  â””â”€â”€ ./.git/refs
â”‚Â Â      â”œâ”€â”€ ./.git/refs/annex
â”‚Â Â      â”‚Â Â  â””â”€â”€ ./.git/refs/annex/last-index
â”‚Â Â      â”œâ”€â”€ ./.git/refs/heads
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ ./.git/refs/heads/git-annex
â”‚Â Â      â”‚Â Â  â””â”€â”€ ./.git/refs/heads/main
â”‚Â Â      â”œâ”€â”€ ./.git/refs/remotes
â”‚Â Â      â”‚Â Â  â””â”€â”€ ./.git/refs/remotes/origin
â”‚Â Â      â”‚Â Â      â”œâ”€â”€ ./.git/refs/remotes/origin/HEAD
â”‚Â Â      â”‚Â Â      â””â”€â”€ ./.git/refs/remotes/origin/main
â”‚Â Â      â””â”€â”€ ./.git/refs/tags
â”œâ”€â”€ ./.gitattributes
â”œâ”€â”€ ./.gitignore
â”œâ”€â”€ ./README.md
â”œâ”€â”€ ./__pycache__
â”‚Â Â  â”œâ”€â”€ ./__pycache__/algorithm_comparison.cpython-311.pyc
â”‚Â Â  â””â”€â”€ ./__pycache__/config.cpython-311.pyc
â”œâ”€â”€ ./algorithm_comparison.py
â”œâ”€â”€ ./config.py
â”œâ”€â”€ ./gather_all_py_to_txt.py
â”œâ”€â”€ ./logs
â”‚Â Â  â””â”€â”€ ./logs/enhanced_experiment_20250802_061714.log
â”œâ”€â”€ ./main.py
â”œâ”€â”€ ./quick_test.py
â”œâ”€â”€ ./requirements.txt
â”œâ”€â”€ ./src
â”‚Â Â  â”œâ”€â”€ ./src/__init__.py
â”‚Â Â  â”œâ”€â”€ ./src/__pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./src/__pycache__/__init__.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./src/__pycache__/data_loader.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./src/__pycache__/topography.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./src/__pycache__/tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./src/__pycache__/trajectory_analysis.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./src/__pycache__/visualization.cpython-311.pyc
â”‚Â Â  â”œâ”€â”€ ./src/data_loader.py
â”‚Â Â  â”œâ”€â”€ ./src/topography.py
â”‚Â Â  â”œâ”€â”€ ./src/trajectory_analysis.py
â”‚Â Â  â””â”€â”€ ./src/visualization.py
â”œâ”€â”€ ./test_results
â”‚Â Â  â”œâ”€â”€ ./test_results/algorithm_comparison_test.png
â”‚Â Â  â”œâ”€â”€ ./test_results/comparison_charts_test.png
â”‚Â Â  â”œâ”€â”€ ./test_results/enhanced_visualization_test.png
â”‚Â Â  â””â”€â”€ ./test_results/font_test.png
â”œâ”€â”€ ./trackers
â”‚Â Â  â”œâ”€â”€ ./trackers/__init__.py
â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/__init__.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/base_tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/greedy_tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/hungarian_tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/hybrid_tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/kalman_tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ./trackers/__pycache__/overlap_tracker.cpython-311.pyc
â”‚Â Â  â”‚Â Â  â””â”€â”€ ./trackers/__pycache__/tracker_factory.cpython-311.pyc
â”‚Â Â  â”œâ”€â”€ ./trackers/base_tracker.py
â”‚Â Â  â”œâ”€â”€ ./trackers/greedy_tracker.py
â”‚Â Â  â”œâ”€â”€ ./trackers/hungarian_tracker.py
â”‚Â Â  â”œâ”€â”€ ./trackers/hybrid_tracker.py
â”‚Â Â  â”œâ”€â”€ ./trackers/kalman_tracker.py
â”‚Â Â  â”œâ”€â”€ ./trackers/overlap_tracker.py
â”‚Â Â  â””â”€â”€ ./trackers/tracker_factory.py
â””â”€â”€ ./usage_guide.md

75 directories, 144 files

# ===============================

# ========== algorithm_comparison.py ==========
# ç›¸å¯¹è·¯å¾„: algorithm_comparison.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./algorithm_comparison.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced EEG Trajectory Tracking Algorithm Comparison Module
Comprehensive evaluation and comparison of different tracking algorithms
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, adjusted_rand_score
import time
import pandas as pd
from typing import Dict, List, Tuple, Optional
import logging
import os

# Set matplotlib to use English fonts only
plt.rcParams['font.family'] = 'DejaVu Sans'

class EnhancedAlgorithmComparison:
    """Enhanced algorithm comparison with better visualization and analysis"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
    def calculate_comprehensive_metrics(self, algorithm_results: Dict) -> Dict:
        """Calculate comprehensive performance metrics for each algorithm"""
        comprehensive_metrics = {}
        
        for algorithm_name, results in algorithm_results.items():
            metrics = {
                'trajectory_count': [],
                'computation_times': [],
                'trajectory_lengths': [],
                'quality_scores': [],
                'frames_processed': [],
                'efficiency_scores': [],
                'stability_scores': []
            }
            
            # Collect raw data
            for session_data in results.values():
                if isinstance(session_data, dict) and 'trajectories' in session_data:
                    metrics['trajectory_count'].append(len(session_data['trajectories']))
                    metrics['computation_times'].append(session_data.get('total_computation_time', 0))
                    metrics['frames_processed'].append(session_data.get('total_frames_processed', 0))
                    
                    # Extract trajectory-level metrics
                    for traj_data in session_data['trajectories'].values():
                        metrics['trajectory_lengths'].append(traj_data.get('length', 0))
                        metrics['quality_scores'].append(traj_data.get('quality_score', 0))
                    
                    # Calculate efficiency: trajectories per second
                    time_taken = session_data.get('total_computation_time', 1e-6)
                    traj_count = len(session_data['trajectories'])
                    efficiency = traj_count / max(time_taken, 1e-6)
                    metrics['efficiency_scores'].append(efficiency)
            
            # Calculate summary statistics
            summary = {}
            for metric_name, values in metrics.items():
                if values:
                    summary[f'avg_{metric_name}'] = np.mean(values)
                    summary[f'std_{metric_name}'] = np.std(values)
                    summary[f'min_{metric_name}'] = np.min(values)
                    summary[f'max_{metric_name}'] = np.max(values)
                    summary[f'median_{metric_name}'] = np.median(values)
                else:
                    summary[f'avg_{metric_name}'] = 0
                    summary[f'std_{metric_name}'] = 0
                    summary[f'min_{metric_name}'] = 0
                    summary[f'max_{metric_name}'] = 0
                    summary[f'median_{metric_name}'] = 0
            
            # Calculate composite performance score
            traj_score = min(1.0, summary['avg_trajectory_count'] / 5.0)
            quality_score = summary['avg_quality_scores']
            efficiency_score = min(1.0, summary['avg_efficiency_scores'] / 10.0)
            stability_score = 1.0 / (1.0 + summary['std_computation_times'] / max(summary['avg_computation_times'], 1e-6))
            
            composite_score = (traj_score * 0.3 + quality_score * 0.3 + 
                             efficiency_score * 0.25 + stability_score * 0.15)
            
            summary['composite_performance_score'] = composite_score
            summary['raw_data'] = metrics
            
            comprehensive_metrics[algorithm_name] = summary
        
        return comprehensive_metrics
    
    def generate_detailed_report(self, comprehensive_metrics: Dict) -> str:
        """Generate detailed comparison report with actionable insights"""
        report = []
        report.append("=" * 80)
        report.append("ENHANCED EEG TRAJECTORY TRACKING ALGORITHM COMPARISON REPORT")
        report.append("=" * 80)
        report.append(f"Analysis Date: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Algorithms Analyzed: {len(comprehensive_metrics)}")
        report.append("")
        
        # Executive Summary
        report.append("EXECUTIVE SUMMARY")
        report.append("-" * 40)
        
        # Find best performers in each category
        best_overall = max(comprehensive_metrics.items(), 
                          key=lambda x: x[1]['composite_performance_score'])
        best_speed = min(comprehensive_metrics.items(), 
                        key=lambda x: x[1]['avg_computation_times'])
        best_quality = max(comprehensive_metrics.items(), 
                          key=lambda x: x[1]['avg_quality_scores'])
        best_efficiency = max(comprehensive_metrics.items(), 
                             key=lambda x: x[1]['avg_efficiency_scores'])
        
        report.append(f"â€¢ Best Overall Performance: {best_overall[0].upper()} "
                     f"(Score: {best_overall[1]['composite_performance_score']:.3f})")
        report.append(f"â€¢ Fastest Algorithm: {best_speed[0].upper()} "
                     f"({best_speed[1]['avg_computation_times']:.4f}s avg)")
        report.append(f"â€¢ Highest Quality: {best_quality[0].upper()} "
                     f"(Score: {best_quality[1]['avg_quality_scores']:.3f})")
        report.append(f"â€¢ Most Efficient: {best_efficiency[0].upper()} "
                     f"({best_efficiency[1]['avg_efficiency_scores']:.1f} traj/s)")
        report.append("")
        
        # Detailed Algorithm Analysis
        report.append("DETAILED ALGORITHM ANALYSIS")
        report.append("-" * 50)
        
        for algorithm_name, metrics in comprehensive_metrics.items():
            report.append(f"\n{algorithm_name.upper()} ALGORITHM:")
            report.append("=" * (len(algorithm_name) + 11))
            
            # Performance Metrics
            report.append("Performance Metrics:")
            report.append(f"  â€¢ Average Trajectories Detected: {metrics['avg_trajectory_count']:.2f} Â± {metrics['std_trajectory_count']:.2f}")
            report.append(f"  â€¢ Average Computation Time: {metrics['avg_computation_times']:.4f}s Â± {metrics['std_computation_times']:.4f}s")
            report.append(f"  â€¢ Average Trajectory Quality: {metrics['avg_quality_scores']:.3f} Â± {metrics['std_quality_scores']:.3f}")
            report.append(f"  â€¢ Average Trajectory Length: {metrics['avg_trajectory_lengths']:.1f} Â± {metrics['std_trajectory_lengths']:.1f} frames")
            report.append(f"  â€¢ Processing Efficiency: {metrics['avg_efficiency_scores']:.1f} trajectories/second")
            report.append(f"  â€¢ Composite Performance Score: {metrics['composite_performance_score']:.3f}")
            
            # Reliability Metrics
            report.append("\nReliability Analysis:")
            cv_time = metrics['std_computation_times'] / max(metrics['avg_computation_times'], 1e-6)
            cv_quality = metrics['std_quality_scores'] / max(metrics['avg_quality_scores'], 1e-6)
            
            report.append(f"  â€¢ Time Consistency (CV): {cv_time:.3f} {'(Excellent)' if cv_time < 0.1 else '(Good)' if cv_time < 0.3 else '(Poor)'}")
            report.append(f"  â€¢ Quality Consistency (CV): {cv_quality:.3f} {'(Excellent)' if cv_quality < 0.1 else '(Good)' if cv_quality < 0.3 else '(Poor)'}")
            
            # Performance Range
            report.append("\nPerformance Range:")
            report.append(f"  â€¢ Trajectory Count Range: {metrics['min_trajectory_count']:.0f} - {metrics['max_trajectory_count']:.0f}")
            report.append(f"  â€¢ Time Range: {metrics['min_computation_times']:.4f}s - {metrics['max_computation_times']:.4f}s")
            report.append(f"  â€¢ Quality Range: {metrics['min_quality_scores']:.3f} - {metrics['max_quality_scores']:.3f}")
        
        # Comparative Analysis
        report.append("\n\nCOMPARATIVE ANALYSIS")
        report.append("-" * 50)
        
        # Statistical significance (simplified)
        report.append("Performance Ranking by Category:")
        
        categories = [
            ('Overall Performance', 'composite_performance_score'),
            ('Speed', 'avg_computation_times', True),  # Lower is better
            ('Quality', 'avg_quality_scores'),
            ('Efficiency', 'avg_efficiency_scores'),
            ('Trajectory Count', 'avg_trajectory_count')
        ]
        
        for category_name, metric_key, *reverse in categories:
            is_reverse = len(reverse) > 0 and reverse[0]
            sorted_algorithms = sorted(comprehensive_metrics.items(), 
                                     key=lambda x: x[1][metric_key], 
                                     reverse=not is_reverse)
            
            report.append(f"\n{category_name}:")
            for i, (alg_name, metrics) in enumerate(sorted_algorithms, 1):
                value = metrics[metric_key]
                if 'time' in metric_key:
                    report.append(f"  {i}. {alg_name.upper()}: {value:.4f}s")
                elif 'score' in metric_key:
                    report.append(f"  {i}. {alg_name.upper()}: {value:.3f}")
                else:
                    report.append(f"  {i}. {alg_name.upper()}: {value:.2f}")
        
        # Recommendations
        report.append("\n\nRECOMMENDATIONS")
        report.append("-" * 40)
        
        report.append("Use Case Recommendations:")
        report.append("â€¢ Real-time Processing: Choose the fastest algorithm with acceptable quality")
        report.append("â€¢ High-precision Analysis: Choose the highest quality algorithm")
        report.append("â€¢ Resource-constrained Environments: Choose the most efficient algorithm")
        report.append("â€¢ Batch Processing: Choose the best overall performance algorithm")
        
        report.append("\nSpecific Recommendations:")
        for alg_name, metrics in comprehensive_metrics.items():
            score = metrics['composite_performance_score']
            speed = metrics['avg_computation_times']
            quality = metrics['avg_quality_scores']
            
            if score > 0.7:
                recommendation = "Excellent for most applications"
            elif speed < 0.1 and quality > 0.6:
                recommendation = "Good for real-time applications"
            elif quality > 0.8:
                recommendation = "Ideal for high-precision tasks"
            elif metrics['avg_efficiency_scores'] > 15:
                recommendation = "Best for high-throughput scenarios"
            else:
                recommendation = "Suitable for basic applications"
            
            report.append(f"â€¢ {alg_name.upper()}: {recommendation}")
        
        # Technical Notes
        report.append("\n\nTECHNICAL NOTES")
        report.append("-" * 40)
        report.append("â€¢ Performance scores are normalized to 0-1 scale")
        report.append("â€¢ Composite score weights: Trajectories(30%), Quality(30%), Efficiency(25%), Stability(15%)")
        report.append("â€¢ Coefficient of Variation (CV) indicates consistency: <0.1=Excellent, 0.1-0.3=Good, >0.3=Poor")
        report.append("â€¢ Results may vary with different data characteristics and parameters")
        
        return "\n".join(report)
    
    def create_comprehensive_visualizations(self, comprehensive_metrics: Dict, 
                                          save_dir: str, visualizer):
        """Create comprehensive visualization suite"""
        os.makedirs(save_dir, exist_ok=True)
        
        # 1. Main comparison chart
        main_comparison_path = os.path.join(save_dir, "main_algorithm_comparison.png")
        visualizer.create_algorithm_comparison_plot(comprehensive_metrics, main_comparison_path)
        
        # 2. Performance radar chart  
        radar_path = os.path.join(save_dir, "performance_radar_chart.png")
        visualizer.create_performance_radar_chart(comprehensive_metrics, radar_path)
        
        # 3. Detailed comparison table
        table_path = os.path.join(save_dir, "detailed_comparison_table.png")
        visualizer.create_detailed_comparison_table(comprehensive_metrics, table_path)
        
        # 4. Statistical analysis plots
        self._create_statistical_plots(comprehensive_metrics, save_dir)
        
        # 5. Performance trends
        self._create_performance_trends(comprehensive_metrics, save_dir)
        
        self.logger.info(f"Comprehensive visualizations saved to {save_dir}")
    
    def _create_statistical_plots(self, comprehensive_metrics: Dict, save_dir: str):
        """Create statistical analysis plots"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Statistical Performance Analysis', fontsize=16, fontweight='bold')
            
            algorithms = list(comprehensive_metrics.keys())
            
            # 1. Box plot for trajectory counts
            ax = axes[0, 0]
            traj_data = []
            labels = []
            
            for alg in algorithms:
                raw_data = comprehensive_metrics[alg]['raw_data']
                if raw_data['trajectory_count']:
                    traj_data.append(raw_data['trajectory_count'])
                    labels.append(alg)
            
            if traj_data:
                bp = ax.boxplot(traj_data, labels=labels, patch_artist=True)
                for patch, color in zip(bp['boxes'], plt.cm.Set1(np.linspace(0, 1, len(labels)))):
                    patch.set_facecolor(color)
                ax.set_title('Trajectory Count Distribution')
                ax.set_ylabel('Number of Trajectories')
                ax.tick_params(axis='x', rotation=45)
            
            # 2. Computation time variability
            ax = axes[0, 1]
            time_data = []
            time_labels = []
            
            for alg in algorithms:
                raw_data = comprehensive_metrics[alg]['raw_data']
                if raw_data['computation_times']:
                    time_data.append(raw_data['computation_times'])
                    time_labels.append(alg)
            
            if time_data:
                bp = ax.boxplot(time_data, labels=time_labels, patch_artist=True)
                for patch, color in zip(bp['boxes'], plt.cm.Set1(np.linspace(0, 1, len(time_labels)))):
                    patch.set_facecolor(color)
                ax.set_title('Computation Time Distribution')
                ax.set_ylabel('Time (seconds)')
                ax.tick_params(axis='x', rotation=45)
            
            # 3. Quality score histogram
            ax = axes[1, 0]
            for i, alg in enumerate(algorithms):
                raw_data = comprehensive_metrics[alg]['raw_data']
                if raw_data['quality_scores']:
                    ax.hist(raw_data['quality_scores'], alpha=0.7, 
                           label=alg, bins=20, density=True)
            
            ax.set_title('Quality Score Distribution')
            ax.set_xlabel('Quality Score')
            ax.set_ylabel('Density')
            ax.legend()
            
            # 4. Efficiency comparison
            ax = axes[1, 1]
            efficiencies = []
            eff_labels = []
            
            for alg in algorithms:
                eff = comprehensive_metrics[alg]['avg_efficiency_scores']
                efficiencies.append(eff)
                eff_labels.append(alg)
            
            bars = ax.bar(eff_labels, efficiencies, 
                         color=plt.cm.Set1(np.linspace(0, 1, len(eff_labels))), alpha=0.7)
            ax.set_title('Processing Efficiency Comparison')
            ax.set_ylabel('Trajectories per Second')
            ax.tick_params(axis='x', rotation=45)
            
            # Add value labels on bars
            for bar, eff in zip(bars, efficiencies):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                       f'{eff:.1f}', ha='center', va='bottom')
            
            plt.tight_layout()
            
            save_path = os.path.join(save_dir, "statistical_analysis.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Statistical plots saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to create statistical plots: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def _create_performance_trends(self, comprehensive_metrics: Dict, save_dir: str):
        """Create performance trend analysis"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('Performance Trends Analysis', fontsize=16, fontweight='bold')
            
            algorithms = list(comprehensive_metrics.keys())
            colors = plt.cm.Set1(np.linspace(0, 1, len(algorithms)))
            
            # 1. Performance vs Speed Trade-off
            ax = axes[0, 0]
            for i, alg in enumerate(algorithms):
                metrics = comprehensive_metrics[alg]
                x = metrics['avg_computation_times']
                y = metrics['composite_performance_score']
                ax.scatter(x, y, s=100, c=[colors[i]], alpha=0.7, label=alg)
                ax.annotate(alg, (x, y), xytext=(5, 5), textcoords='offset points')
            
            ax.set_xlabel('Average Computation Time (s)')
            ax.set_ylabel('Composite Performance Score')
            ax.set_title('Performance vs Speed Trade-off')
            ax.grid(True, alpha=0.3)
            
            # 2. Quality vs Efficiency
            ax = axes[0, 1]
            for i, alg in enumerate(algorithms):
                metrics = comprehensive_metrics[alg]
                x = metrics['avg_efficiency_scores']
                y = metrics['avg_quality_scores']
                ax.scatter(x, y, s=100, c=[colors[i]], alpha=0.7, label=alg)
                ax.annotate(alg, (x, y), xytext=(5, 5), textcoords='offset points')
            
            ax.set_xlabel('Efficiency (trajectories/s)')
            ax.set_ylabel('Average Quality Score')
            ax.set_title('Quality vs Efficiency Trade-off')
            ax.grid(True, alpha=0.3)
            
            # 3. Consistency Analysis
            ax = axes[1, 0]
            consistency_metrics = []
            for alg in algorithms:
                metrics = comprehensive_metrics[alg]
                time_cv = metrics['std_computation_times'] / max(metrics['avg_computation_times'], 1e-6)
                quality_cv = metrics['std_quality_scores'] / max(metrics['avg_quality_scores'], 1e-6)
                consistency_score = 1.0 / (1.0 + time_cv + quality_cv)
                consistency_metrics.append(consistency_score)
            
            bars = ax.bar(algorithms, consistency_metrics, color=colors, alpha=0.7)
            ax.set_title('Algorithm Consistency Score')
            ax.set_ylabel('Consistency Score (higher is better)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, score in zip(bars, consistency_metrics):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{score:.3f}', ha='center', va='bottom')
            
            # 4. Overall Ranking
            ax = axes[1, 1]
            ranking_categories = ['Speed', 'Quality', 'Efficiency', 'Consistency']
            
            # Calculate rankings for each category
            speed_ranking = {alg: i+1 for i, (alg, _) in enumerate(
                sorted(comprehensive_metrics.items(), key=lambda x: x[1]['avg_computation_times']))}
            quality_ranking = {alg: i+1 for i, (alg, _) in enumerate(
                sorted(comprehensive_metrics.items(), key=lambda x: x[1]['avg_quality_scores'], reverse=True))}
            efficiency_ranking = {alg: i+1 for i, (alg, _) in enumerate(
                sorted(comprehensive_metrics.items(), key=lambda x: x[1]['avg_efficiency_scores'], reverse=True))}
            consistency_ranking = {alg: i+1 for i, (alg, score) in enumerate(
                sorted(zip(algorithms, consistency_metrics), key=lambda x: x[1], reverse=True))}
            
            # Create heatmap data
            ranking_data = []
            for alg in algorithms:
                ranking_data.append([
                    speed_ranking[alg],
                    quality_ranking[alg], 
                    efficiency_ranking[alg],
                    consistency_ranking[alg]
                ])
            
            im = ax.imshow(ranking_data, cmap='RdYlGn_r', aspect='auto')
            ax.set_xticks(range(len(ranking_categories)))
            ax.set_xticklabels(ranking_categories)
            ax.set_yticks(range(len(algorithms)))
            ax.set_yticklabels(algorithms)
            ax.set_title('Algorithm Ranking Heatmap (1=Best)')
            
            # Add text annotations
            for i in range(len(algorithms)):
                for j in range(len(ranking_categories)):
                    text = ax.text(j, i, ranking_data[i][j], ha="center", va="center", color="black")
            
            plt.colorbar(im, ax=ax)
            
            plt.tight_layout()
            
            save_path = os.path.join(save_dir, "performance_trends.png")
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Performance trends saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to create performance trends: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def export_results_to_csv(self, comprehensive_metrics: Dict, save_path: str):
        """Export detailed results to CSV for further analysis"""
        try:
            data_rows = []
            
            for algorithm_name, metrics in comprehensive_metrics.items():
                row = {
                    'Algorithm': algorithm_name,
                    'Avg_Trajectories': metrics['avg_trajectory_count'],
                    'Std_Trajectories': metrics['std_trajectory_count'],
                    'Avg_Computation_Time': metrics['avg_computation_times'],
                    'Std_Computation_Time': metrics['std_computation_times'],
                    'Avg_Quality': metrics['avg_quality_scores'],
                    'Std_Quality': metrics['std_quality_scores'],
                    'Avg_Length': metrics['avg_trajectory_lengths'],
                    'Std_Length': metrics['std_trajectory_lengths'],
                    'Avg_Efficiency': metrics['avg_efficiency_scores'],
                    'Composite_Score': metrics['composite_performance_score'],
                    'Min_Trajectories': metrics['min_trajectory_count'],
                    'Max_Trajectories': metrics['max_trajectory_count'],
                    'Min_Time': metrics['min_computation_times'],
                    'Max_Time': metrics['max_computation_times'],
                    'Min_Quality': metrics['min_quality_scores'],
                    'Max_Quality': metrics['max_quality_scores']
                }
                data_rows.append(row)
            
            df = pd.DataFrame(data_rows)
            df.to_csv(save_path, index=False)
            
            self.logger.info(f"Results exported to CSV: {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to export results to CSV: {e}")


def run_enhanced_algorithm_comparison(config, all_results, visualizer):
    """Run enhanced algorithm comparison with improved analysis"""
    comparison = EnhancedAlgorithmComparison(config)
    
    print("\n" + "="*60)
    print("Running Enhanced Algorithm Comparison Analysis...")
    print("="*60)
    
    try:
        # Calculate comprehensive metrics
        print("ðŸ“Š Calculating comprehensive performance metrics...")
        comprehensive_metrics = comparison.calculate_comprehensive_metrics(all_results)
        
        if not comprehensive_metrics:
            print("âŒ No algorithm results available for comparison")
            return None
        
        print(f"âœ“ Analyzed {len(comprehensive_metrics)} algorithms")
        
        # Generate detailed report
        print("ðŸ“ Generating detailed analysis report...")
        detailed_report = comparison.generate_detailed_report(comprehensive_metrics)
        
        # Save report
        comparison_dir = os.path.join(config.RESULTS_ROOT, "algorithm_comparison")
        os.makedirs(comparison_dir, exist_ok=True)
        
        report_path = os.path.join(comparison_dir, "enhanced_comparison_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(detailed_report)
        
        print(f"âœ“ Detailed report saved: {report_path}")
        
        # Create comprehensive visualizations
        print("ðŸ“ˆ Creating comprehensive visualizations...")
        comparison.create_comprehensive_visualizations(
            comprehensive_metrics, comparison_dir, visualizer)
        
        # Export to CSV for further analysis
        csv_path = os.path.join(comparison_dir, "algorithm_metrics.csv")
        comparison.export_results_to_csv(comprehensive_metrics, csv_path)
        
        # Display summary
        print("\n" + "="*60)
        print("ALGORITHM COMPARISON SUMMARY")
        print("="*60)
        
        # Find best performers
        best_overall = max(comprehensive_metrics.items(), 
                          key=lambda x: x[1]['composite_performance_score'])
        fastest = min(comprehensive_metrics.items(), 
                     key=lambda x: x[1]['avg_computation_times'])
        highest_quality = max(comprehensive_metrics.items(), 
                            key=lambda x: x[1]['avg_quality_scores'])
        
        print(f"ðŸ† Best Overall: {best_overall[0].upper()} (Score: {best_overall[1]['composite_performance_score']:.3f})")
        print(f"âš¡ Fastest: {fastest[0].upper()} ({fastest[1]['avg_computation_times']:.4f}s)")
        print(f"ðŸŽ¯ Highest Quality: {highest_quality[0].upper()} (Score: {highest_quality[1]['avg_quality_scores']:.3f})")
        
        print(f"\nðŸ“‚ Results Location:")
        print(f"   â€¢ Report: {report_path}")
        print(f"   â€¢ Visualizations: {comparison_dir}")
        print(f"   â€¢ CSV Data: {csv_path}")
        
        print("\n" + "="*60)
        print("Enhanced Algorithm Comparison Complete! ðŸŽ‰")
        print("="*60)
        
        return {
            'comprehensive_metrics': comprehensive_metrics,
            'report_path': report_path,
            'visualization_dir': comparison_dir,
            'csv_path': csv_path
        }
        
    except Exception as e:
        logging.getLogger(__name__).error(f"Enhanced algorithm comparison failed: {e}")
        print(f"âŒ Algorithm comparison failed: {e}")
        return None

# ========== config.py ==========
# ç›¸å¯¹è·¯å¾„: config.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./config.py

import os
import numpy as np

class Config:
    # Data path configuration
    DATA_ROOT = "../data/ds005262"
    RESULTS_ROOT = "./results"
    LOGS_ROOT = "./logs"
    
    # Ensure directories exist
    for path in [RESULTS_ROOT, LOGS_ROOT, 
                 os.path.join(RESULTS_ROOT, "topographies"),
                 os.path.join(RESULTS_ROOT, "trajectories"),
                 os.path.join(RESULTS_ROOT, "analysis"),
                 os.path.join(RESULTS_ROOT, "videos"),
                 os.path.join(RESULTS_ROOT, "algorithm_comparison")]:
        os.makedirs(path, exist_ok=True)
    
    # EEG data processing parameters
    SAMPLING_RATE = 500  
    LOW_FREQ = 1.0       
    HIGH_FREQ = 50.0     
    
    # Topography generation parameters
    TOPO_SIZE = (128, 128)
    INTERPOLATION_METHOD = 'cubic'
     
    # å¸§æ•°æŽ§åˆ¶å‚æ•° - æ–°å¢ž
    MAX_FRAMES_PER_EPOCH = 300          # æ¯ä¸ªepochæœ€å¤šå¤„ç†çš„å¸§æ•°
    MAX_ANIMATION_FRAMES = 300          # åŠ¨ç”»æœ€å¤§å¸§æ•°
    MAX_SAVE_FRAMES = 50                # ä¿å­˜å¸§åºåˆ—çš„æœ€å¤§å¸§æ•°
    
    # å®žéªŒè§„æ¨¡é…ç½® - æ”¯æŒæ‰€æœ‰è¢«è¯•
    MAX_SUBJECTS = 12               # å¤„ç†æ‰€æœ‰12ä¸ªè¢«è¯•
    MAX_EPOCHS_PER_SUBJECT = 3      # æ¯ä¸ªè¢«è¯•å¤„ç†3ä¸ªepoch
    MAX_SESSIONS_PER_SUBJECT = 5    # æ¯ä¸ªè¢«è¯•æœ€å¤šå¤„ç†5ä¸ªsession
    MEMORY_LIMIT_MB = 4096          # å¢žåŠ å†…å­˜é™åˆ¶

    
    # Algorithm comparison configuration
    ENABLE_ALGORITHM_COMPARISON = True
    COMPARISON_ALGORITHMS = [
        'greedy',           # Greedy matching (original default)
        'hungarian',        # Hungarian algorithm
        'kalman',          # Kalman prediction
        'overlap',         # Overlap matching
        'hybrid'           # Hybrid algorithm
    ]
    
    # Target tracking parameters
    THRESHOLD_PERCENTILE = 88
    MIN_REGION_SIZE = 25       
    MAX_REGIONS = 6            
    
    # Trajectory analysis parameters
    TIME_WINDOW = 2.0          
    TRAJECTORY_SMOOTH_FACTOR = 3  
    
    # Visualization parameters
    COLORMAP = 'RdYlBu_r'      
    FPS = 10                   
    DPI = 150                  
    
    # Algorithm-specific parameters
    ALGORITHM_CONFIGS = {
        'greedy': {
            'distance_threshold': 25.0,
            'enable_prediction': False,
            'enable_reconnection': True,
            'max_inactive_frames': 25,
            'description': 'Greedy matching algorithm - fast local optimum'
        },
        'hungarian': {
            'distance_threshold': 25.0,
            'enable_prediction': False,
            'enable_reconnection': True,
            'max_inactive_frames': 25,
            'description': 'Hungarian algorithm - global optimal matching'
        },
        'kalman': {
            'distance_threshold': 30.0,
            'enable_prediction': True,
            'prediction_weight': 0.4,
            'enable_reconnection': True,
            'max_inactive_frames': 30,
            'description': 'Kalman prediction algorithm - motion-based prediction'
        },
        'overlap': {
            'overlap_threshold': 0.3,
            'distance_threshold': 35.0,
            'enable_reconnection': True,
            'max_inactive_frames': 20,
            'description': 'Overlap matching - region overlap based'
        },
        'hybrid': {
            'distance_threshold': 25.0,
            'overlap_weight': 0.4,
            'intensity_weight': 0.1,
            'area_weight': 0.1,
            'enable_prediction': True,
            'enable_reconnection': True,
            'max_inactive_frames': 30,
            'description': 'Hybrid algorithm - comprehensive multi-feature'
        }
    }
    
    # Performance evaluation metrics
    EVALUATION_METRICS = [
        'trajectory_count',           
        'average_trajectory_length',  
        'max_trajectory_length',     
        'tracking_continuity',       
        'trajectory_smoothness',     
        'computation_time',          
        'memory_usage',             
        'detection_stability',      
        'trajectory_quality'        
    ]
    
    # Visualization configuration
    VISUALIZATION_CONFIG = {
        'generate_comparison_plots': True,    
        'generate_heatmaps': True,           
        'generate_trajectory_overlays': True, 
        'generate_detailed_reports': True,    
        'save_individual_results': True,     
        'create_summary_animations': False   # Disabled for performance
    }
    
    # Tracking optimization parameters
    TRACKING_OPTIMIZATION = {
        'enable_adaptive_threshold': True,     
        'enable_prediction': True,             
        'enable_reconnection': True,           
        'base_distance_threshold': 25.0,      
        'max_distance_threshold': 60.0,       
        'reconnection_distance': 40.0,        
        'max_inactive_frames': 25,            
        'prediction_weight': 0.3,             
        'quality_threshold': 0.2,             
        'min_trajectory_length': 3            
    }
    
    # Visualization optimization parameters
    VISUALIZATION_OPTIMIZATION = {
        'auto_font_detection': False,         # Disabled to avoid Chinese font issues
        'fallback_to_english': True,         
        'max_animation_frames': MAX_ANIMATION_FRAMES,
        'save_frame_sequence_fallback': True, 
        'trajectory_alpha': 0.8,             
        'show_direction_arrows': True,       
        'legend_max_items': 10               
    }
    
    # Standard electrode positions
    ELECTRODE_POSITIONS = {
        'Fp1': (-0.3, 0.85), 'Fp2': (0.3, 0.85), 'Fpz': (0, 0.9),
        'F7': (-0.7, 0.4), 'F3': (-0.4, 0.4), 'Fz': (0, 0.4), 'F4': (0.4, 0.4), 'F8': (0.7, 0.4),
        'FC5': (-0.5, 0.2), 'FC1': (-0.2, 0.2), 'FCz': (0, 0.2), 'FC2': (0.2, 0.2), 'FC6': (0.5, 0.2),
        'T7': (-0.85, 0), 'C3': (-0.4, 0), 'Cz': (0, 0), 'C4': (0.4, 0), 'T8': (0.85, 0),
        'CP5': (-0.5, -0.2), 'CP1': (-0.2, -0.2), 'CPz': (0, -0.2), 'CP2': (0.2, -0.2), 'CP6': (0.5, -0.2),
        'P7': (-0.7, -0.4), 'P3': (-0.4, -0.4), 'Pz': (0, -0.4), 'P4': (0.4, -0.4), 'P8': (0.7, -0.4),
        'PO9': (-0.8, -0.65), 'PO7': (-0.6, -0.65), 'PO3': (-0.25, -0.65), 'POz': (0, -0.65), 
        'PO4': (0.25, -0.65), 'PO8': (0.6, -0.65), 'PO10': (0.8, -0.65),
        'O1': (-0.3, -0.85), 'Oz': (0, -0.9), 'O2': (0.3, -0.85),
        
        # Additional electrodes
        'AF7': (-0.5, 0.65), 'AF3': (-0.25, 0.65), 'AFz': (0, 0.65), 'AF4': (0.25, 0.65), 'AF8': (0.5, 0.65),
        'F5': (-0.55, 0.4), 'F1': (-0.2, 0.4), 'F2': (0.2, 0.4), 'F6': (0.55, 0.4),
        'FT9': (-0.9, 0.2), 'FT7': (-0.75, 0.2), 'FT8': (0.75, 0.2), 'FT10': (0.9, 0.2),
        'C5': (-0.55, 0), 'C1': (-0.2, 0), 'C2': (0.2, 0), 'C6': (0.55, 0),
        'TP9': (-0.9, -0.2), 'TP7': (-0.75, -0.2), 'TP8': (0.75, -0.2), 'TP10': (0.9, -0.2),
        'P5': (-0.55, -0.4), 'P1': (-0.2, -0.4), 'P2': (0.2, -0.4), 'P6': (0.55, -0.4),
        
        # Case variants
        'FP1': (-0.3, 0.85), 'FP2': (0.3, 0.85),
        'fp1': (-0.3, 0.85), 'fp2': (0.3, 0.85), 'fpz': (0, 0.9),
        'f7': (-0.7, 0.4), 'f3': (-0.4, 0.4), 'fz': (0, 0.4), 'f4': (0.4, 0.4), 'f8': (0.7, 0.4),
        't7': (-0.85, 0), 'c3': (-0.4, 0), 'cz': (0, 0), 'c4': (0.4, 0), 't8': (0.85, 0),
        'p7': (-0.7, -0.4), 'p3': (-0.4, -0.4), 'pz': (0, -0.4), 'p4': (0.4, -0.4), 'p8': (0.7, -0.4),
        'o1': (-0.3, -0.85), 'oz': (0, -0.9), 'o2': (0.3, -0.85),
    }
    
    # Frame control methods
    @classmethod
    def get_max_frames(cls, frame_type: str = 'epoch') -> int:
        """Get maximum frame limit"""
        frame_limits = {
            'epoch': cls.MAX_FRAMES_PER_EPOCH,
            'animation': cls.MAX_ANIMATION_FRAMES,
            'save': cls.MAX_SAVE_FRAMES
        }
        return frame_limits.get(frame_type, cls.MAX_FRAMES_PER_EPOCH)
    
    @classmethod
    def set_max_frames(cls, max_frames: int, frame_type: str = 'epoch'):
        """Dynamically set maximum frames"""
        if frame_type == 'epoch':
            cls.MAX_FRAMES_PER_EPOCH = max_frames
        elif frame_type == 'animation':
            cls.MAX_ANIMATION_FRAMES = max_frames
        elif frame_type == 'save':
            cls.MAX_SAVE_FRAMES = max_frames
        
        # Sync related configuration
        cls.VISUALIZATION_OPTIMIZATION['max_animation_frames'] = cls.MAX_ANIMATION_FRAMES
    
    @staticmethod
    def get_default_electrode_position(ch_name: str, n_channels: int, ch_index: int):
        """Generate default position for unknown electrodes"""
        angle = 2 * np.pi * ch_index / n_channels
        radius = 0.7
        x = radius * np.cos(angle)
        y = radius * np.sin(angle)
        return (x, y)
    
    @classmethod
    def get_algorithm_config(cls, algorithm_name: str):
        """Get specific algorithm configuration"""
        return cls.ALGORITHM_CONFIGS.get(algorithm_name, cls.ALGORITHM_CONFIGS['greedy'])
    
    @classmethod
    def get_experiment_summary(cls):
        """Get experiment configuration summary"""
        return {
            'total_subjects': cls.MAX_SUBJECTS,
            'algorithms_count': len(cls.COMPARISON_ALGORITHMS),
            'algorithm_names': cls.COMPARISON_ALGORITHMS,
            'metrics_count': len(cls.EVALUATION_METRICS),
            'max_epochs_per_subject': cls.MAX_EPOCHS_PER_SUBJECT,
            'max_sessions_per_subject': cls.MAX_SESSIONS_PER_SUBJECT,
            'max_frames_per_epoch': cls.MAX_FRAMES_PER_EPOCH,
            'algorithm_comparison_enabled': cls.ENABLE_ALGORITHM_COMPARISON
        }
    
    @classmethod
    def auto_adjust_parameters(cls, data_characteristics: dict):
        """Auto-adjust parameters based on data characteristics"""
        if 'signal_strength' in data_characteristics:
            signal_strength = data_characteristics['signal_strength']
            
            if signal_strength < 0.3:  # Weak signal
                cls.THRESHOLD_PERCENTILE = 85
                cls.MIN_REGION_SIZE = 15
                print("âœ“ Detected weak signal, adjusted to high sensitivity parameters")
                
            elif signal_strength > 0.8:  # Strong signal
                cls.THRESHOLD_PERCENTILE = 92
                cls.MIN_REGION_SIZE = 40
                print("âœ“ Detected strong signal, adjusted to high precision parameters")
        
        if 'noise_level' in data_characteristics:
            noise_level = data_characteristics['noise_level']
            
            if noise_level > 0.6:  # High noise
                cls.TRAJECTORY_SMOOTH_FACTOR = 5
                print("âœ“ Detected high noise, enabled strong smoothing")
            elif noise_level < 0.2:  # Low noise
                cls.TRAJECTORY_SMOOTH_FACTOR = 2
                print("âœ“ Detected low noise, enabled fine tracking")
    
    @classmethod
    def get_config_summary(cls):
        """Get current configuration summary"""
        summary = {
            'detection_sensitivity': 'High' if cls.THRESHOLD_PERCENTILE < 90 else 'Medium' if cls.THRESHOLD_PERCENTILE < 95 else 'Low',
            'tracking_aggressiveness': 'High' if cls.TRACKING_OPTIMIZATION['base_distance_threshold'] > 25 else 'Medium',
            'quality_filter': 'Strict' if cls.TRACKING_OPTIMIZATION['quality_threshold'] > 0.25 else 'Lenient',
            'smoothing_level': 'High' if cls.TRAJECTORY_SMOOTH_FACTOR > 4 else 'Medium' if cls.TRAJECTORY_SMOOTH_FACTOR > 2 else 'Low',
            'algorithm_comparison': 'Enabled' if cls.ENABLE_ALGORITHM_COMPARISON else 'Disabled',
            'algorithms_to_compare': len(cls.COMPARISON_ALGORITHMS),
            'max_frames_per_epoch': cls.MAX_FRAMES_PER_EPOCH
        }
        return summary

# ========== main.py ==========
# ç›¸å¯¹è·¯å¾„: main.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./main.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced EEG Topography Motion Trajectory Analysis Main Program
Algorithm Comparison Edition with Improved Visualization and Analysis
Version: 3.1.0 - Enhanced Edition
Updated: 2025-08-01
"""

import os
import sys
import logging
import json
import pickle
import numpy as np
import gc
import argparse
import platform
from datetime import datetime
from tqdm import tqdm
import warnings
import time

# Suppress warnings
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# Add paths
sys.path.append('src')
sys.path.append('trackers')

# Font configuration - English only
def setup_matplotlib_font():
    """Configure matplotlib font for English only"""
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    
    try:
        fm._rebuild()
    except:
        pass
    
    # Use safe English fonts only
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
    plt.rcParams['axes.unicode_minus'] = False
    
    print("âœ“ Font configuration: English labels only")
    return True

# Set up font
USE_ENGLISH_ONLY = setup_matplotlib_font()

from config import Config
from src import EEGDataLoader, TopographyGenerator, TrajectoryAnalyzer, Visualizer
from trackers import TrackerFactory
from algorithm_comparison import run_enhanced_algorithm_comparison

def setup_logging():
    """Set up logging system"""
    log_dir = Config.LOGS_ROOT
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f"enhanced_experiment_{timestamp}.log")
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"Enhanced logging system initialized: {log_file}")
    
    return logger

def check_dependencies():
    """Check required dependencies"""
    required_packages = {
        'mne': 'MNE-Python',
        'numpy': 'NumPy',
        'scipy': 'SciPy',
        'matplotlib': 'Matplotlib',
        'sklearn': 'Scikit-learn',
        'cv2': 'OpenCV',
        'tqdm': 'tqdm',
        'pandas': 'Pandas',
        'seaborn': 'Seaborn'
    }
    
    missing_packages = []
    
    for package, name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(name)
    
    if missing_packages:
        print("âŒ Missing required dependencies:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nPlease install using: pip install -r requirements.txt")
        return False
    
    return True

def print_system_info():
    """Print system information"""
    print("=" * 80)
    print("EEG TOPOGRAPHY MOTION TRAJECTORY ANALYSIS SYSTEM")
    print("Enhanced Algorithm Comparison Edition")
    print("=" * 80)
    print(f"Python Version: {platform.python_version()}")
    print(f"Operating System: {platform.system()} {platform.release()}")
    print(f"Processor: {platform.machine()}")
    print(f"Font Support: English Only (for compatibility)")
    
    # Display experiment configuration
    summary = Config.get_experiment_summary()
    print(f"\nExperiment Configuration:")
    print(f"  â€¢ Subjects to Process: {summary['total_subjects']}")
    print(f"  â€¢ Algorithm Comparison: {len(summary['algorithm_names'])} algorithms")
    print(f"  â€¢ Algorithms: {', '.join(summary['algorithm_names'])}")
    print(f"  â€¢ Evaluation Metrics: {summary['metrics_count']}")
    print(f"  â€¢ Max Frames per Epoch: {summary['max_frames_per_epoch']}")
    print(f"  â€¢ Max Epochs per Subject: {summary['max_epochs_per_subject']}")
    print(f"  â€¢ Algorithm Comparison: {'Enabled' if summary['algorithm_comparison_enabled'] else 'Disabled'}")
    
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"  â€¢ Total Memory: {memory.total / (1024**3):.1f} GB")
        print(f"  â€¢ Available Memory: {memory.available / (1024**3):.1f} GB")
    except ImportError:
        pass
    
    print("=" * 80)

def validate_config():
    """Validate configuration parameters"""
    logger = logging.getLogger(__name__)
    
    # Check data directory
    if not os.path.exists(Config.DATA_ROOT):
        error_msg = f"Data directory not found: {Config.DATA_ROOT}"
        logger.error(error_msg)
        print(f"\nâŒ {error_msg}")
        print("Please check DATA_ROOT setting in config.py")
        return False
    
    # Validate algorithm configuration
    validation_results = TrackerFactory.validate_algorithm_config(Config)
    invalid_algorithms = [alg for alg, valid in validation_results.items() if not valid]
    
    if invalid_algorithms:
        logger.warning(f"Invalid algorithm configurations: {invalid_algorithms}")
        print(f"âš ï¸  Potentially problematic algorithm configurations: {', '.join(invalid_algorithms)}")
    
    # Check available algorithms
    available = TrackerFactory.get_available_algorithms()
    missing = [alg for alg in Config.COMPARISON_ALGORITHMS if alg not in available]
    
    if missing:
        logger.error(f"Unavailable algorithms: {missing}")
        print(f"âŒ Unavailable algorithms: {', '.join(missing)}")
        return False
    
    # Validate frame configuration
    if Config.MAX_FRAMES_PER_EPOCH <= 0:
        logger.error(f"Invalid max frames configuration: {Config.MAX_FRAMES_PER_EPOCH}")
        print(f"âŒ Invalid max frames configuration: {Config.MAX_FRAMES_PER_EPOCH}")
        return False
    
    logger.info(f"Configuration validation complete, max frames limit: {Config.MAX_FRAMES_PER_EPOCH}")
    return True

def process_subject_with_multiple_algorithms(data_loader, topo_generator, analyzer, visualizer,
                                           subject_id, sessions, logger):
    """Process single subject data with multiple algorithms"""
    subject_results = {}
    
    logger.info(f"Processing subject {subject_id} ({len(sessions)} sessions, {len(Config.COMPARISON_ALGORITHMS)} algorithms)")
    
    # Create all trackers
    trackers = TrackerFactory.create_all_trackers(Config)
    if not trackers:
        logger.error(f"Unable to create trackers")
        return None
    
    logger.info(f"Successfully created {len(trackers)} trackers: {', '.join(trackers.keys())}")
    
    session_progress = 0
    total_sessions = len(sessions)
    
    for session_id, session_data in sessions.items():
        session_progress += 1
        session_key = f"{subject_id}_{session_id}"
        logger.info(f"  Processing session {session_id} ({session_progress}/{total_sessions})")
        
        try:
            epochs = session_data['epochs']
            positions = session_data['positions']
            ch_names = epochs.ch_names
            
            # Select multiple epochs for analysis
            n_epochs_to_analyze = min(len(epochs), Config.MAX_EPOCHS_PER_SUBJECT)
            
            session_algorithm_results = {}
            
            epoch_progress = 0
            for epoch_idx in range(n_epochs_to_analyze):
                epoch_progress += 1
                
                try:
                    epoch_data = epochs.get_data()[epoch_idx]
                    
                    # Generate topography sequence
                    logger.info(f"    Generating epoch {epoch_idx+1} topography sequence...")
                    
                    # Use configuration parameters to limit time points
                    max_time_points = min(epoch_data.shape[1], Config.MAX_FRAMES_PER_EPOCH)
                    epoch_data_subset = epoch_data[:, :max_time_points]
                    
                    logger.info(f"    Using frame limit: {Config.MAX_FRAMES_PER_EPOCH}, processing: {max_time_points} frames")
                    
                    topographies = topo_generator.generate_time_series_topographies(
                        epoch_data_subset[np.newaxis, :, :], positions, ch_names
                    )[0]
                    
                    if topographies is None or topographies.size == 0:
                        logger.warning(f"    Epoch {epoch_idx+1}: topography generation failed")
                        continue
                    
                    # Normalize topographies
                    for t in range(topographies.shape[0]):
                        topographies[t] = topo_generator.normalize_topography(topographies[t])
                    
                    # Use each algorithm for trajectory tracking
                    epoch_algorithm_results = {}
                    algorithm_progress = 0
                    
                    for algorithm_name, tracker in trackers.items():
                        algorithm_progress += 1
                        
                        try:
                            logger.info(f"    Tracking epoch {epoch_idx+1} with {algorithm_name} algorithm "
                                      f"({algorithm_progress}/{len(trackers)})...")
                            
                            start_time = time.time()
                            tracking_results = tracker.track_sequence(topographies)
                            end_time = time.time()
                            
                            if not tracking_results or 'trajectories' not in tracking_results:
                                logger.warning(f"    {algorithm_name}: Epoch {epoch_idx+1} tracking returned empty results")
                                continue
                            
                            trajectories = tracking_results['trajectories']
                            if not trajectories:
                                logger.warning(f"    {algorithm_name}: Epoch {epoch_idx+1} no valid trajectories detected")
                                continue
                            
                            # Record results
                            epoch_algorithm_results[algorithm_name] = {
                                'trajectories': trajectories,
                                'metrics': tracking_results.get('metrics', {}),
                                'summary': tracking_results.get('summary', {}),
                                'computation_time': end_time - start_time,
                                'processed_frames': topographies.shape[0],
                                'tracking_results': tracking_results  # Keep full results for visualization
                            }
                            
                            logger.info(f"    {algorithm_name}: Epoch {epoch_idx+1} found {len(trajectories)} trajectories "
                                      f"(processed {topographies.shape[0]} frames, time: {end_time - start_time:.3f}s)")
                            
                        except Exception as e:
                            logger.error(f"    {algorithm_name}: Epoch {epoch_idx+1} tracking failed: {e}")
                            continue
                    
                    # If results exist, save epoch-level comparisons
                    if epoch_algorithm_results:
                        # Save representative visualizations for each algorithm
                        for algorithm_name, results in epoch_algorithm_results.items():
                            trajectories = results['trajectories']
                            
                            # Save trajectory plot
                            traj_path = os.path.join(Config.RESULTS_ROOT, "trajectories", 
                                                   f"{session_key}_epoch{epoch_idx}_{algorithm_name}_trajectories.png")
                            try:
                                title = f"Subject {subject_id} Session {session_id} Epoch {epoch_idx} - {algorithm_name.upper()} Algorithm ({results['processed_frames']} frames)"
                                visualizer.plot_trajectories(
                                    trajectories, topographies.shape[1:],
                                    title=title,
                                    save_path=traj_path
                                )
                            except Exception as e:
                                logger.warning(f"Failed to save {algorithm_name} trajectory plot: {e}")
                        
                        # Merge epoch results into session results
                        for algorithm_name, results in epoch_algorithm_results.items():
                            if algorithm_name not in session_algorithm_results:
                                session_algorithm_results[algorithm_name] = {
                                    'trajectories': {},
                                    'total_computation_time': 0,
                                    'epoch_count': 0,
                                    'metrics_sum': {},
                                    'total_frames_processed': 0
                                }
                            
                            # Merge trajectories (add epoch prefix)
                            for traj_id, traj_data in results['trajectories'].items():
                                key = f"epoch{epoch_idx}_{traj_id}"
                                session_algorithm_results[algorithm_name]['trajectories'][key] = traj_data
                            
                            # Accumulate statistics
                            session_algorithm_results[algorithm_name]['total_computation_time'] += results['computation_time']
                            session_algorithm_results[algorithm_name]['epoch_count'] += 1
                            session_algorithm_results[algorithm_name]['total_frames_processed'] += results['processed_frames']
                            
                            # Accumulate metrics
                            for metric, value in results.get('metrics', {}).items():
                                if metric not in session_algorithm_results[algorithm_name]['metrics_sum']:
                                    session_algorithm_results[algorithm_name]['metrics_sum'][metric] = []
                                session_algorithm_results[algorithm_name]['metrics_sum'][metric].append(value)
                    
                    # Memory cleanup
                    del topographies
                    gc.collect()
                    
                except Exception as e:
                    logger.error(f"    Epoch {epoch_idx+1} processing failed: {e}")
                    continue
            
            # Process session-level results
            if session_algorithm_results:
                # Calculate average metrics
                for algorithm_name in session_algorithm_results:
                    alg_result = session_algorithm_results[algorithm_name]
                    
                    # Calculate average metrics
                    avg_metrics = {}
                    for metric, values in alg_result['metrics_sum'].items():
                        if values:
                            avg_metrics[metric] = np.mean(values)
                    
                    # Update results
                    alg_result['average_metrics'] = avg_metrics
                    alg_result['total_trajectories'] = len(alg_result['trajectories'])
                    alg_result['avg_frames_per_epoch'] = alg_result['total_frames_processed'] / alg_result['epoch_count'] if alg_result['epoch_count'] > 0 else 0
                    
                    session_algorithm_results[algorithm_name] = alg_result
                
                subject_results[session_id] = session_algorithm_results
                
                logger.info(f"  Session {session_id}: algorithm comparison completed")
                
                # Display brief results for each algorithm
                for algorithm_name, alg_result in session_algorithm_results.items():
                    logger.info(f"    {algorithm_name}: {alg_result['total_trajectories']} trajectories, "
                              f"avg time {alg_result['total_computation_time']/alg_result['epoch_count']:.3f}s, "
                              f"avg frames {alg_result['avg_frames_per_epoch']:.0f}/epoch")
            else:
                logger.warning(f"  Session {session_id}: all algorithms failed to find valid trajectories")
                
        except Exception as e:
            logger.error(f"  Error processing session {session_id}: {e}")
            continue
    
    return subject_results if subject_results else None

def create_enhanced_summary_report(all_results, logger):
    """Create enhanced summary report with detailed insights"""
    logger.info("Generating enhanced summary report...")
    
    try:
        # Collect comprehensive statistics
        algorithm_stats = {}
        subject_performance = {}
        
        for subject_id, sessions in all_results.items():
            subject_performance[subject_id] = {}
            
            for session_id, session_data in sessions.items():
                for algorithm_name, alg_data in session_data.items():
                    if algorithm_name not in algorithm_stats:
                        algorithm_stats[algorithm_name] = {
                            'total_trajectories': [],
                            'computation_times': [],
                            'trajectory_lengths': [],
                            'trajectory_qualities': [],
                            'frames_processed': [],
                            'sessions_processed': 0
                        }
                    
                    # Collect statistics
                    algorithm_stats[algorithm_name]['total_trajectories'].append(alg_data['total_trajectories'])
                    algorithm_stats[algorithm_name]['computation_times'].append(alg_data['total_computation_time'])
                    algorithm_stats[algorithm_name]['frames_processed'].append(alg_data.get('total_frames_processed', 0))
                    algorithm_stats[algorithm_name]['sessions_processed'] += 1
                    
                    # Collect trajectory statistics
                    for traj_data in alg_data['trajectories'].values():
                        algorithm_stats[algorithm_name]['trajectory_lengths'].append(traj_data['length'])
                        algorithm_stats[algorithm_name]['trajectory_qualities'].append(traj_data.get('quality_score', 0))
                    
                    # Track subject performance
                    if algorithm_name not in subject_performance[subject_id]:
                        subject_performance[subject_id][algorithm_name] = {
                            'total_trajectories': 0,
                            'total_time': 0,
                            'sessions': 0
                        }
                    
                    subject_performance[subject_id][algorithm_name]['total_trajectories'] += alg_data['total_trajectories']
                    subject_performance[subject_id][algorithm_name]['total_time'] += alg_data['total_computation_time']
                    subject_performance[subject_id][algorithm_name]['sessions'] += 1
        
        # Generate enhanced report
        report = []
        report.append("=" * 100)
        report.append("ENHANCED EEG TRAJECTORY TRACKING ALGORITHM COMPARISON REPORT")
        report.append("=" * 100)
        report.append(f"Experiment Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total Subjects Processed: {len(all_results)}")
        report.append(f"Total Sessions Analyzed: {sum(len(sessions) for sessions in all_results.values())}")
        report.append(f"Algorithms Compared: {len(algorithm_stats)}")
        report.append(f"Frame Limit Configuration: {Config.MAX_FRAMES_PER_EPOCH} frames/epoch")
        report.append("")
        
        # Executive Summary
        report.append("EXECUTIVE SUMMARY")
        report.append("=" * 50)
        
        best_performers = {}
        for category in ['total_trajectories', 'computation_times', 'trajectory_qualities']:
            if category == 'computation_times':
                # Lower is better for computation time
                best_alg = min(algorithm_stats.items(), 
                             key=lambda x: np.mean(x[1][category]) if x[1][category] else float('inf'))
                best_performers[category] = (best_alg[0], np.mean(best_alg[1][category]))
            else:
                # Higher is better for other metrics
                best_alg = max(algorithm_stats.items(), 
                             key=lambda x: np.mean(x[1][category]) if x[1][category] else 0)
                best_performers[category] = (best_alg[0], np.mean(best_alg[1][category]))
        
        report.append(f"ðŸ† Most Trajectories Detected: {best_performers['total_trajectories'][0].upper()} "
                     f"({best_performers['total_trajectories'][1]:.1f} avg)")
        report.append(f"âš¡ Fastest Processing: {best_performers['computation_times'][0].upper()} "
                     f"({best_performers['computation_times'][1]:.4f}s avg)")
        report.append(f"ðŸŽ¯ Highest Quality: {best_performers['trajectory_qualities'][0].upper()} "
                     f"({best_performers['trajectory_qualities'][1]:.3f} avg quality)")
        report.append("")
        
        # Detailed Algorithm Performance
        report.append("DETAILED ALGORITHM PERFORMANCE ANALYSIS")
        report.append("=" * 60)
        
        for algorithm_name, stats in algorithm_stats.items():
            if not stats['total_trajectories']:
                continue
            
            avg_trajectories = np.mean(stats['total_trajectories'])
            std_trajectories = np.std(stats['total_trajectories'])
            avg_time = np.mean(stats['computation_times'])
            std_time = np.std(stats['computation_times'])
            avg_quality = np.mean(stats['trajectory_qualities']) if stats['trajectory_qualities'] else 0
            std_quality = np.std(stats['trajectory_qualities']) if stats['trajectory_qualities'] else 0
            avg_length = np.mean(stats['trajectory_lengths']) if stats['trajectory_lengths'] else 0
            avg_frames = np.mean(stats['frames_processed']) if stats['frames_processed'] else 0
            
            # Calculate efficiency and consistency
            efficiency = avg_trajectories / max(avg_time, 1e-6)
            time_consistency = 1.0 / (1.0 + std_time / max(avg_time, 1e-6))
            quality_consistency = 1.0 / (1.0 + std_quality / max(avg_quality, 1e-6)) if avg_quality > 0 else 0
            
            report.append(f"\n{algorithm_name.upper()} ALGORITHM ANALYSIS:")
            report.append("-" * (len(algorithm_name) + 20))
            report.append(f"  Sessions Processed: {stats['sessions_processed']}")
            report.append(f"  Average Trajectories: {avg_trajectories:.2f} Â± {std_trajectories:.2f}")
            report.append(f"  Average Processing Time: {avg_time:.4f}s Â± {std_time:.4f}s")
            report.append(f"  Average Trajectory Quality: {avg_quality:.3f} Â± {std_quality:.3f}")
            report.append(f"  Average Trajectory Length: {avg_length:.1f} frames")
            report.append(f"  Average Frames Processed: {avg_frames:.0f}/session")
            report.append(f"  Processing Efficiency: {efficiency:.1f} trajectories/second")
            report.append(f"  Time Consistency: {time_consistency:.3f}")
            report.append(f"  Quality Consistency: {quality_consistency:.3f}")
            
            # Performance rating
            if efficiency > 10 and avg_quality > 0.7 and time_consistency > 0.8:
                rating = "EXCELLENT"
            elif efficiency > 5 and avg_quality > 0.5 and time_consistency > 0.6:
                rating = "GOOD"
            elif efficiency > 2 and avg_quality > 0.3:
                rating = "FAIR"
            else:
                rating = "NEEDS_IMPROVEMENT"
            
            report.append(f"  Overall Rating: {rating}")
        
        # Subject-wise Performance Analysis
        report.append(f"\n\nSUBJECT-WISE PERFORMANCE SUMMARY")
        report.append("=" * 50)
        
        for subject_id, subject_data in subject_performance.items():
            report.append(f"\nSubject {subject_id}:")
            
            for algorithm_name, perf_data in subject_data.items():
                avg_traj_per_session = perf_data['total_trajectories'] / max(perf_data['sessions'], 1)
                avg_time_per_session = perf_data['total_time'] / max(perf_data['sessions'], 1)
                
                report.append(f"  {algorithm_name}: {avg_traj_per_session:.1f} traj/session, "
                             f"{avg_time_per_session:.3f}s/session")
        
        # Recommendations
        report.append(f"\n\nRECOMMENDATIONS & INSIGHTS")
        report.append("=" * 50)
        
        # Performance-based recommendations
        fastest_alg = best_performers['computation_times'][0]
        most_accurate_alg = best_performers['trajectory_qualities'][0]
        most_sensitive_alg = best_performers['total_trajectories'][0]
        
        report.append("Algorithm Selection Guidelines:")
        report.append(f"â€¢ For Real-time Applications: {fastest_alg.upper()} (fastest processing)")
        report.append(f"â€¢ For High-precision Analysis: {most_accurate_alg.upper()} (highest quality)")
        report.append(f"â€¢ For Maximum Detection: {most_sensitive_alg.upper()} (most trajectories)")
        
        report.append("\nPerformance Optimization Insights:")
        total_sessions = sum(stats['sessions_processed'] for stats in algorithm_stats.values())
        if total_sessions > 0:
            avg_processing_time = np.mean([np.mean(stats['computation_times']) 
                                         for stats in algorithm_stats.values() 
                                         if stats['computation_times']])
            
            report.append(f"â€¢ Average processing time across all algorithms: {avg_processing_time:.3f}s")
            report.append(f"â€¢ Frame processing efficiency varies by algorithm (see detailed analysis)")
            report.append(f"â€¢ Quality-speed trade-off is evident across different algorithms")
        
        # Configuration insights
        report.append(f"\nConfiguration Impact Analysis:")
        report.append(f"â€¢ Frame limit setting ({Config.MAX_FRAMES_PER_EPOCH} frames/epoch) affects:")
        report.append(f"  - Processing speed (lower = faster)")
        report.append(f"  - Memory usage (lower = less memory)")
        report.append(f"  - Trajectory completeness (higher = more complete)")
        
        report.append("\nGeneral Insights:")
        report.append("â€¢ Algorithm performance may vary significantly with different EEG data characteristics")
        report.append("â€¢ Consider data-specific parameter tuning for optimal results")
        report.append("â€¢ Multiple algorithm approaches provide robust analysis framework")
        
        return "\n".join(report)
        
    except Exception as e:
        logger.error(f"Enhanced summary report generation failed: {e}")
        return "Enhanced summary report generation failed. Please check logs for details."

def cleanup_memory():
    """Clean up memory"""
    gc.collect()
    
    try:
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / (1024 * 1024)
        
        if memory_mb > Config.MEMORY_LIMIT_MB:
            logging.getLogger(__name__).warning(
                f"High memory usage: {memory_mb:.1f} MB (limit: {Config.MEMORY_LIMIT_MB} MB)"
            )
            return False
    except ImportError:
        pass
    
    return True

def print_final_summary(all_results, enhanced_comparison_results):
    """Print final experiment summary"""
    print("\n" + "="*80)
    print("ENHANCED ALGORITHM COMPARISON EXPERIMENT SUMMARY")
    print("="*80)
    
    # Basic statistics
    n_subjects = len(all_results)
    total_sessions = sum(len(sessions) for sessions in all_results.values())
    
    print(f"âœ“ Successfully Processed:")
    print(f"  â€¢ Subjects: {n_subjects}")
    print(f"  â€¢ Total Sessions: {total_sessions}")
    print(f"  â€¢ Frame Limit: {Config.MAX_FRAMES_PER_EPOCH} frames/epoch")
    
    if enhanced_comparison_results:
        metrics = enhanced_comparison_results['comprehensive_metrics']
        print(f"  â€¢ Algorithms Compared: {len(metrics)}")
        print(f"  â€¢ Algorithm List: {', '.join(metrics.keys())}")
        
        # Show top performers
        if metrics:
            best_overall = max(metrics.items(), key=lambda x: x[1]['composite_performance_score'])
            fastest = min(metrics.items(), key=lambda x: x[1]['avg_computation_times'])
            highest_quality = max(metrics.items(), key=lambda x: x[1]['avg_quality_scores'])
            
            print(f"\nðŸ† Top Performers:")
            print(f"  â€¢ Best Overall: {best_overall[0].upper()} (Score: {best_overall[1]['composite_performance_score']:.3f})")
            print(f"  â€¢ Fastest: {fastest[0].upper()} ({fastest[1]['avg_computation_times']:.4f}s)")
            print(f"  â€¢ Highest Quality: {highest_quality[0].upper()} (Score: {highest_quality[1]['avg_quality_scores']:.3f})")
    
    # Output locations
    print(f"\nðŸ“‚ Results Saved To:")
    print(f"  â€¢ Trajectory Plots: {os.path.join(Config.RESULTS_ROOT, 'trajectories')}")
    print(f"  â€¢ Algorithm Comparison: {os.path.join(Config.RESULTS_ROOT, 'algorithm_comparison')}")
    
    if enhanced_comparison_results:
        print(f"  â€¢ Detailed Report: {enhanced_comparison_results['report_path']}")
        print(f"  â€¢ Visualization Suite: {enhanced_comparison_results['visualization_dir']}")
        print(f"  â€¢ CSV Data: {enhanced_comparison_results['csv_path']}")
    
    print("="*80)
    print("ðŸŽ‰ Enhanced Algorithm Comparison Experiment Complete!")
    print("="*80)

def main():
    """Main experiment workflow"""
    parser = argparse.ArgumentParser(description='Enhanced EEG Trajectory Analysis with Algorithm Comparison')
    parser.add_argument('--subjects', type=int, default=None, 
                       help='Maximum number of subjects to process')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Maximum epochs per subject')
    parser.add_argument('--frames', type=int, default=None,
                       help='Maximum frames per epoch')
    parser.add_argument('--algorithms', nargs='+', default=None,
                       help='Algorithms to compare', choices=Config.COMPARISON_ALGORITHMS)
    parser.add_argument('--disable-comparison', action='store_true',
                       help='Disable algorithm comparison (use greedy only)')
    parser.add_argument('--fast-mode', action='store_true',
                       help='Enable fast mode (reduced frames and epochs)')
    
    args = parser.parse_args()
    
    # Fast mode configuration
    if args.fast_mode:
        Config.set_max_frames(100, 'epoch')
        Config.MAX_EPOCHS_PER_SUBJECT = 1
        Config.MAX_SUBJECTS = 3
        print("ðŸš€ Fast mode enabled: reduced processing for quick testing")
    
    # Print system information
    print_system_info()
    
    # Check dependencies
    if not check_dependencies():
        return 1
    
    # Set up logging
    logger = setup_logging()
    logger.info("Starting Enhanced EEG Topography Motion Trajectory Analysis Experiment")
    
    try:
        # Validate configuration
        if not validate_config():
            return 1
        
        # Apply command line arguments
        if args.subjects:
            Config.MAX_SUBJECTS = args.subjects
        if args.epochs:
            Config.MAX_EPOCHS_PER_SUBJECT = args.epochs
        if args.frames:
            Config.set_max_frames(args.frames, 'epoch')
            logger.info(f"Frame limit set to: {args.frames}")
        if args.algorithms:
            Config.COMPARISON_ALGORITHMS = args.algorithms
        if args.disable_comparison:
            Config.ENABLE_ALGORITHM_COMPARISON = False
            Config.COMPARISON_ALGORITHMS = ['greedy']
        
        # Initialize components
        logger.info("Initializing enhanced analysis components...")
        
        data_loader = EEGDataLoader(Config.DATA_ROOT, Config)
        topo_generator = TopographyGenerator(Config)
        analyzer = TrajectoryAnalyzer(Config)
        visualizer = Visualizer(Config)
        
        # Load data
        logger.info("Loading EEG data...")
        all_data = data_loader.load_all_subjects(Config.MAX_SUBJECTS)
        
        if not all_data:
            logger.error("Failed to load any EEG data, please check data path and format")
            print("\nâŒ Failed to load any EEG data, please check data path and format")
            return 1
        
        logger.info(f"Successfully loaded data from {len(all_data)} subjects")
        
        # Store all results
        all_results = {}
        
        # Process each subject
        total_subjects = len(all_data)
        processed_subjects = 0
        
        print(f"\nðŸ”„ Processing {total_subjects} subjects with {len(Config.COMPARISON_ALGORITHMS)} algorithms...")
        
        for subject_id, sessions in tqdm(all_data.items(), desc="Processing subjects"):
            try:
                if Config.ENABLE_ALGORITHM_COMPARISON:
                    subject_results = process_subject_with_multiple_algorithms(
                        data_loader, topo_generator, analyzer, visualizer,
                        subject_id, sessions, logger
                    )
                else:
                    logger.info("Using single algorithm mode (greedy)")
                    subject_results = None
                
                if subject_results:
                    all_results[subject_id] = subject_results
                    processed_subjects += 1
                    
                    # Periodic memory cleanup
                    if processed_subjects % 2 == 0:
                        cleanup_memory()
                        logger.info(f"Processed {processed_subjects}/{total_subjects} subjects")
                else:
                    logger.warning(f"Subject {subject_id} produced no valid results")
                    
            except Exception as e:
                logger.error(f"Serious error processing subject {subject_id}: {e}")
                continue
        
        if processed_subjects == 0:
            logger.error("No subject data was successfully processed")
            print("\nâŒ No subject data was successfully processed")
            return 1
        
        logger.info(f"Data processing complete, successfully processed {processed_subjects} subjects")
        
        # Generate enhanced algorithm comparison and visualizations
        enhanced_comparison_results = None
        if Config.ENABLE_ALGORITHM_COMPARISON and all_results:
            print("\nðŸ“Š Running enhanced algorithm comparison analysis...")
            enhanced_comparison_results = run_enhanced_algorithm_comparison(all_results, visualizer, Config)
            
            if enhanced_comparison_results:
                # Create enhanced summary report
                enhanced_report = create_enhanced_summary_report(all_results, logger)
                
                # Save enhanced report
                enhanced_report_path = os.path.join(Config.RESULTS_ROOT, "enhanced_experiment_summary.txt")
                with open(enhanced_report_path, 'w', encoding='utf-8') as f:
                    f.write(enhanced_report)
                
                logger.info(f"Enhanced summary report saved: {enhanced_report_path}")
                
                # Create overall summary visualization
                summary_viz_path = os.path.join(Config.RESULTS_ROOT, "experiment_summary.png")
                visualizer.create_summary_visualization(all_results, summary_viz_path)
        else:
            logger.info("Algorithm comparison disabled or no valid results")
        
        # Save complete results
        results_path = os.path.join(Config.RESULTS_ROOT, "complete_experiment_results.pkl")
        try:
            with open(results_path, 'wb') as f:
                pickle.dump({
                    'experiment_results': all_results,
                    'enhanced_comparison': enhanced_comparison_results,
                    'config_summary': Config.get_config_summary(),
                    'experiment_summary': Config.get_experiment_summary()
                }, f, protocol=pickle.HIGHEST_PROTOCOL)
            logger.info(f"Complete results saved: {results_path}")
        except Exception as e:
            logger.error(f"Failed to save complete results: {e}")
        
        # Print final summary
        print_final_summary(all_results, enhanced_comparison_results)
        
        logger.info("Enhanced algorithm comparison experiment completed successfully!")
        return 0
        
    except KeyboardInterrupt:
        logger.info("Experiment was interrupted by user")
        print("\nðŸ›‘ Experiment was interrupted by user")
        return 130
        
    except Exception as e:
        logger.error(f"Unexpected error during experiment: {e}")
        print(f"\nâŒ Unexpected error during experiment: {e}")
        return 1
        
    finally:
        cleanup_memory()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

# ========== quick_test.py ==========
# ç›¸å¯¹è·¯å¾„: quick_test.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./quick_test.py

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced EEG Trajectory Tracking Algorithm Comparison System - Quick Test Script
Validates system installation and basic functionality with English interface
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import logging
import platform

# Set matplotlib to English only
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']

def test_dependencies():
    """Test dependency installation"""
    print("ðŸ” Testing dependency installation...")
    
    required_packages = {
        'numpy': 'NumPy',
        'scipy': 'SciPy', 
        'matplotlib': 'Matplotlib',
        'sklearn': 'Scikit-learn',
        'cv2': 'OpenCV',
        'tqdm': 'tqdm',
        'mne': 'MNE-Python',
        'pandas': 'Pandas',
        'seaborn': 'Seaborn'
    }
    
    missing_packages = []
    installed_packages = []
    
    for package, name in required_packages.items():
        try:
            __import__(package)
            installed_packages.append(name)
            print(f"  âœ“ {name}")
        except ImportError:
            missing_packages.append(name)
            print(f"  âŒ {name} - Not installed")
    
    print(f"\nInstallation status: {len(installed_packages)}/{len(required_packages)} packages installed")
    
    if missing_packages:
        print(f"\nâŒ Missing dependencies: {', '.join(missing_packages)}")
        print("Please run: pip install -r requirements.txt")
        return False
    else:
        print("âœ… All dependencies correctly installed!")
        return True

def test_tracker_factory():
    """Test tracker factory"""
    print("\nðŸ­ Testing tracker factory...")
    
    try:
        # Add paths
        sys.path.append('trackers')
        sys.path.append('src')
        from trackers import TrackerFactory
        from config import Config
        
        # Test available algorithms
        algorithms = TrackerFactory.get_available_algorithms()
        print(f"  âœ“ Available algorithms: {', '.join(algorithms)}")
        
        # Test tracker creation
        success_count = 0
        for algorithm in algorithms:
            try:
                tracker = TrackerFactory.create_tracker(algorithm, Config)
                if tracker is not None:
                    print(f"  âœ“ {algorithm} tracker created successfully")
                    success_count += 1
                else:
                    print(f"  âŒ {algorithm} tracker creation failed")
            except Exception as e:
                print(f"  âŒ {algorithm} tracker creation exception: {e}")
        
        print(f"\nTracker creation status: {success_count}/{len(algorithms)} algorithms available")
        return success_count > 0
        
    except Exception as e:
        print(f"  âŒ Tracker factory test failed: {e}")
        return False

def test_synthetic_data():
    """Test synthetic data processing with enhanced algorithms"""
    print("\nðŸ§ª Testing synthetic data processing...")
    
    try:
        sys.path.append('src')
        sys.path.append('trackers')
        
        from src.topography import TopographyGenerator
        from trackers import TrackerFactory
        from config import Config
        
        # Create synthetic topography data
        n_frames = 30  # Reduced for faster testing
        size = (64, 64)  # Smaller size for speed
        
        print(f"  ðŸ”§ Generating {n_frames} frames of {size} synthetic topographies...")
        
        # Create moving activation regions
        topographies = np.zeros((n_frames, size[0], size[1]))
        
        for i in range(n_frames):
            # Create moving Gaussian activation
            center_x = 20 + int(15 * np.sin(2 * np.pi * i / 20))
            center_y = 20 + int(10 * np.cos(2 * np.pi * i / 15))
            
            y, x = np.ogrid[:size[0], :size[1]]
            activation = np.exp(-((x - center_x)**2 + (y - center_y)**2) / (2 * 4**2))
            topographies[i] = activation
        
        print("  âœ“ Synthetic topography generation complete")
        
        # Test tracking algorithms
        test_algorithms = Config.COMPARISON_ALGORITHMS[:3]  # Test first 3 algorithms
        
        algorithm_results = {}
        
        for algorithm in test_algorithms:
            try:
                print(f"  ðŸŽ¯ Testing {algorithm} algorithm...")
                
                tracker = TrackerFactory.create_tracker(algorithm, Config)
                if tracker is None:
                    print(f"    âŒ {algorithm} tracker creation failed")
                    continue
                
                import time
                start_time = time.time()
                result = tracker.track_sequence(topographies)
                end_time = time.time()
                
                if result and 'trajectories' in result:
                    trajectories = result['trajectories']
                    metrics = result.get('metrics', {})
                    
                    algorithm_results[algorithm] = {
                        'trajectory_count': len(trajectories),
                        'computation_time': end_time - start_time,
                        'metrics': metrics
                    }
                    
                    print(f"    âœ“ {algorithm}: {len(trajectories)} trajectories detected")
                    print(f"    âœ“ Processing time: {end_time - start_time:.3f}s")
                    
                    if len(trajectories) > 0:
                        first_traj = list(trajectories.values())[0]
                        print(f"    âœ“ Trajectory length: {first_traj['length']} frames")
                else:
                    print(f"    âš ï¸  {algorithm}: No trajectories detected")
                    algorithm_results[algorithm] = {
                        'trajectory_count': 0,
                        'computation_time': end_time - start_time,
                        'metrics': {}
                    }
                
            except Exception as e:
                print(f"    âŒ {algorithm} test failed: {e}")
                algorithm_results[algorithm] = {
                    'trajectory_count': 0,
                    'computation_time': 0,
                    'error': str(e)
                }
        
        # Display comparison results
        if algorithm_results:
            print(f"\n  ðŸ“Š Algorithm Performance Comparison:")
            print(f"  {'Algorithm':<12} {'Trajectories':<12} {'Time (s)':<10} {'Status'}")
            print(f"  {'-'*50}")
            
            for alg, results in algorithm_results.items():
                status = "âœ“ Pass" if results['trajectory_count'] > 0 else "âš  No detection"
                if 'error' in results:
                    status = "âŒ Error"
                
                print(f"  {alg:<12} {results['trajectory_count']:<12} {results['computation_time']:<10.3f} {status}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Synthetic data test failed: {e}")
        return False

def test_enhanced_visualization():
    """Test enhanced visualization functionality"""
    print("\nðŸŽ¨ Testing enhanced visualization functionality...")
    
    try:
        # Test matplotlib setup
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive backend
        
        # Create comprehensive test plots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        fig.suptitle('Enhanced EEG Trajectory Analysis - Visualization Test', fontsize=14, fontweight='bold')
        
        # Test 1: Basic plotting with English labels
        ax = axes[0, 0]
        x = np.linspace(0, 10, 100)
        y = np.sin(x)
        ax.plot(x, y, 'b-', linewidth=2, label='Test Signal')
        ax.set_title('Signal Processing Test')
        ax.set_xlabel('Time (s)')
        ax.set_ylabel('Amplitude')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        # Test 2: Algorithm comparison simulation
        ax = axes[0, 1]
        algorithms = ['Greedy', 'Hungarian', 'Kalman', 'Overlap', 'Hybrid']
        performance = [4.2, 4.8, 3.9, 4.1, 4.6]
        colors = plt.cm.Set1(np.linspace(0, 1, len(algorithms)))
        
        bars = ax.bar(algorithms, performance, color=colors, alpha=0.7)
        ax.set_title('Algorithm Performance Comparison')
        ax.set_ylabel('Average Trajectories')
        ax.tick_params(axis='x', rotation=45)
        
        # Add value labels
        for bar, perf in zip(bars, performance):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                   f'{perf:.1f}', ha='center', va='bottom')
        
        # Test 3: Topography simulation
        ax = axes[1, 0]
        size = 64
        x = np.linspace(-1, 1, size)
        y = np.linspace(-1, 1, size)
        X, Y = np.meshgrid(x, y)
        Z = np.exp(-(X**2 + Y**2) / 0.3) * np.cos(3*X) * np.sin(3*Y)
        
        im = ax.imshow(Z, cmap='RdYlBu_r', origin='lower', extent=[-1, 1, -1, 1])
        ax.set_title('EEG Topography Simulation')
        ax.set_xlabel('X Position')
        ax.set_ylabel('Y Position')
        
        # Add head outline
        circle = plt.Circle((0, 0), 0.9, fill=False, color='black', linewidth=2)
        ax.add_patch(circle)
        
        # Test 4: Trajectory visualization
        ax = axes[1, 1]
        
        # Simulate trajectory data
        t = np.linspace(0, 4*np.pi, 50)
        traj1_x = 0.3 * np.cos(t) + 0.1 * np.sin(3*t)
        traj1_y = 0.3 * np.sin(t) + 0.1 * np.cos(2*t)
        traj2_x = -0.2 * np.cos(1.5*t) + 0.15 * np.sin(2*t)
        traj2_y = 0.4 * np.sin(1.5*t) - 0.1 * np.cos(4*t)
        
        ax.plot(traj1_x, traj1_y, 'r-', linewidth=2, alpha=0.8, label='Trajectory 1')
        ax.plot(traj2_x, traj2_y, 'b-', linewidth=2, alpha=0.8, label='Trajectory 2')
        
        # Mark start and end points
        ax.scatter([traj1_x[0], traj2_x[0]], [traj1_y[0], traj2_y[0]], 
                  c=['red', 'blue'], s=100, marker='o', label='Start', zorder=5)
        ax.scatter([traj1_x[-1], traj2_x[-1]], [traj1_y[-1], traj2_y[-1]], 
                  c=['red', 'blue'], s=100, marker='s', label='End', zorder=5)
        
        ax.set_title('Trajectory Tracking Simulation')
        ax.set_xlabel('X Coordinate')
        ax.set_ylabel('Y Coordinate')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_aspect('equal')
        
        plt.tight_layout()
        
        # Save test plots
        test_dir = './test_results'
        os.makedirs(test_dir, exist_ok=True)
        
        main_test_path = os.path.join(test_dir, 'enhanced_visualization_test.png')
        plt.savefig(main_test_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ“ Enhanced visualization test saved: {main_test_path}")
        
        # Test algorithm comparison visualization
        fig, ax = plt.subplots(figsize=(10, 6))
        
        # Simulate comprehensive algorithm comparison
        metrics = {
            'Trajectory Count': [4.2, 4.8, 3.9, 4.1, 4.6],
            'Quality Score': [0.72, 0.85, 0.78, 0.74, 0.82],
            'Processing Time': [0.15, 0.45, 0.25, 0.35, 0.55],
            'Efficiency': [28, 11, 16, 12, 8]
        }
        
        x = np.arange(len(algorithms))
        width = 0.2
        
        for i, (metric, values) in enumerate(metrics.items()):
            # Normalize values for comparison
            if metric == 'Processing Time':
                # Lower is better for time, so invert
                norm_values = [1.0 - (v - min(values)) / (max(values) - min(values)) for v in values]
            else:
                norm_values = [(v - min(values)) / (max(values) - min(values)) for v in values]
            
            ax.bar(x + i * width, norm_values, width, label=metric, alpha=0.8)
        
        ax.set_title('Normalized Algorithm Performance Comparison', fontweight='bold')
        ax.set_xlabel('Algorithms')
        ax.set_ylabel('Normalized Performance (0-1)')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(algorithms)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        comparison_path = os.path.join(test_dir, 'algorithm_comparison_test.png')
        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  âœ“ Algorithm comparison test saved: {comparison_path}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Enhanced visualization test failed: {e}")
        return False

def test_config():
    """Test configuration file"""
    print("\nâš™ï¸ Testing configuration file...")
    
    try:
        from config import Config
        
        # Test basic configuration
        print(f"  âœ“ Data path: {Config.DATA_ROOT}")
        print(f"  âœ“ Results path: {Config.RESULTS_ROOT}")
        print(f"  âœ“ Max subjects: {Config.MAX_SUBJECTS}")
        print(f"  âœ“ Algorithm comparison: {'Enabled' if Config.ENABLE_ALGORITHM_COMPARISON else 'Disabled'}")
        print(f"  âœ“ Comparison algorithms: {', '.join(Config.COMPARISON_ALGORITHMS)}")
        print(f"  âœ“ Max frames per epoch: {Config.MAX_FRAMES_PER_EPOCH}")
        
        # Test configuration methods
        summary = Config.get_experiment_summary()
        print(f"  âœ“ Experiment summary: {summary['algorithms_count']} algorithms, {summary['total_subjects']} subjects")
        
        # Test algorithm configuration
        for algorithm in Config.COMPARISON_ALGORITHMS:
            alg_config = Config.get_algorithm_config(algorithm)
            print(f"  âœ“ {algorithm} config: {len(alg_config)} parameters")
        
        # Test frame control
        frame_limit = Config.get_max_frames('epoch')
        print(f"  âœ“ Frame control: {frame_limit} frames/epoch limit")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Configuration test failed: {e}")
        return False

def generate_enhanced_test_report(results):
    """Generate enhanced test report"""
    print("\n" + "="*80)
    print("ðŸŽ¯ ENHANCED EEG TRAJECTORY TRACKING SYSTEM - QUICK TEST REPORT")
    print("="*80)
    print(f"Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("")
    
    test_items = [
        ('Dependency Installation', results.get('dependencies', False)),
        ('Tracker Factory', results.get('tracker_factory', False)),
        ('Synthetic Data Processing', results.get('synthetic_data', False)),
        ('Enhanced Visualization', results.get('enhanced_visualization', False)),
        ('Configuration System', results.get('config', False))
    ]
    
    passed_tests = sum(1 for _, result in test_items if result)
    total_tests = len(test_items)
    
    print("Test Results:")
    for item_name, passed in test_items:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {item_name:<25}: {status}")
    
    print(f"\nOverall Result: {passed_tests}/{total_tests} tests passed")
    
    # Provide detailed feedback based on results
    if passed_tests == total_tests:
        print("ðŸŽ‰ EXCELLENT! System is fully operational and ready for use!")
        print("\nRecommended Next Steps:")
        print("  1. Prepare your EEG data (see README.md for data format)")
        print("  2. Quick test: python main.py --fast-mode")
        print("  3. Full experiment: python main.py")
        print("  4. View results in ./results/ directory")
        
    elif passed_tests >= 4:
        print("âœ… GOOD! System is functional with minor issues.")
        print("âš ï¸  Some advanced features may have limitations.")
        print("\nRecommended Next Steps:")
        print("  1. Review failed tests and address issues if needed")
        print("  2. Try quick test: python main.py --fast-mode")
        print("  3. Check system logs for detailed error information")
        
    elif passed_tests >= 2:
        print("âš ï¸  PARTIAL! Basic functionality works but issues detected.")
        print("ðŸ”§ Some components need attention before full operation.")
        print("\nRecommended Actions:")
        print("  1. Fix failed dependency installations")
        print("  2. Check Python version (requires 3.8+)")
        print("  3. Verify system compatibility")
        
    else:
        print("âŒ CRITICAL! Major system issues detected.")
        print("ðŸš¨ System requires significant troubleshooting.")
        print("\nRequired Actions:")
        print("  1. Check Python version (requires 3.8+)")
        print("  2. Reinstall dependencies: pip install -r requirements.txt")
        print("  3. Verify system compatibility and permissions")
        print("  4. Check installation logs for specific errors")
    
    print(f"\nðŸ“ Test outputs saved in: ./test_results/")
    
    # System information
    print(f"\nSystem Information:")
    print(f"  â€¢ Python: {sys.version.split()[0]}")
    
    try:
        import platform as plt_module
        print(f"  â€¢ Platform: {plt_module.system().lower()}")
        print(f"  â€¢ Architecture: {plt_module.machine()}")
    except Exception as e:
        print(f"  â€¢ Platform: {sys.platform}")
        print(f"  â€¢ Architecture: Unknown")
    
    # Performance estimate
    if passed_tests >= 4:
        estimated_time = "2-5 minutes per subject" if passed_tests == total_tests else "3-8 minutes per subject"
        print(f"  â€¢ Estimated processing time: {estimated_time}")
        print(f"  â€¢ Recommended subjects for testing: 2-3")
        print(f"  â€¢ Memory requirement: 2-4 GB for typical datasets")
    
    print("\nðŸ“‹ For detailed help, documentation, and troubleshooting:")
    print("   â€¢ Check README.md")
    print("   â€¢ Review example configurations")
    print("   â€¢ Examine log files in ./logs/")
    print("="*80)

def run_performance_benchmark():
    """Run quick performance benchmark"""
    print("\nâš¡ Running performance benchmark...")
    
    try:
        import time
        
        # Test computation performance
        start_time = time.time()
        
        # Matrix operations test
        size = 1000
        a = np.random.rand(size, size)
        b = np.random.rand(size, size)
        c = np.dot(a, b)
        
        matrix_time = time.time() - start_time
        
        # Memory allocation test
        start_time = time.time()
        large_array = np.zeros((2000, 2000, 10))
        del large_array
        
        memory_time = time.time() - start_time
        
        print(f"  âœ“ Matrix computation: {matrix_time:.3f}s")
        print(f"  âœ“ Memory allocation: {memory_time:.3f}s")
        
        # Performance assessment
        if matrix_time < 0.5 and memory_time < 0.1:
            performance = "Excellent"
        elif matrix_time < 2.0 and memory_time < 0.5:
            performance = "Good"
        elif matrix_time < 5.0 and memory_time < 1.0:
            performance = "Fair"
        else:
            performance = "Poor"
        
        print(f"  ðŸ“Š Overall performance: {performance}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Performance benchmark failed: {e}")
        return False

def main():
    """Main test function"""
    print("ðŸš€ ENHANCED EEG TRAJECTORY TRACKING ALGORITHM COMPARISON SYSTEM")
    print("   Quick Functionality Test & System Validation")
    print("="*80)
    print("This test validates system installation and basic functionality")
    print("Estimated time: 2-3 minutes")
    print("")
    
    # Suppress some logging for cleaner output
    logging.getLogger().setLevel(logging.WARNING)
    
    # Run test suite
    results = {}
    
    results['dependencies'] = test_dependencies()
    results['config'] = test_config()
    results['tracker_factory'] = test_tracker_factory()
    results['synthetic_data'] = test_synthetic_data()
    results['enhanced_visualization'] = test_enhanced_visualization()
    
    # Optional performance benchmark
    print("\nðŸŽ­ Additional Tests:")
    results['performance'] = run_performance_benchmark()
    
    # Generate comprehensive test report
    generate_enhanced_test_report(results)
    
    return 0 if all(results.values()) else 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

# ========== src/__init__.py ==========
# ç›¸å¯¹è·¯å¾„: src/__init__.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./src/__init__.py

# EEGè„‘ç”µåœ°å½¢å›¾è¿åŠ¨è½¨è¿¹åˆ†æžåŒ…
__version__ = "1.0.0"
__author__ = "EEG Research Team"

from .data_loader import EEGDataLoader
from .topography import TopographyGenerator
from .trajectory_analysis import TrajectoryAnalyzer
from .visualization import Visualizer

__all__ = [
    'EEGDataLoader',
    'TopographyGenerator', 
    'TrajectoryAnalyzer',
    'Visualizer'
]

# ========== src/data_loader.py ==========
# ç›¸å¯¹è·¯å¾„: src/data_loader.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./src/data_loader.py

import os
import mne
import numpy as np
import pandas as pd
from tqdm import tqdm
import logging
import gc
from typing import List, Tuple, Dict, Optional

class EEGDataLoader:
    def __init__(self, data_root: str, config):
        self.data_root = data_root
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # è®¾ç½®MNEæ—¥å¿—çº§åˆ«
        mne.set_log_level('WARNING')
        
    def get_subject_sessions(self, subject_id: str) -> List[str]:
        """èŽ·å–æŒ‡å®šè¢«è¯•çš„æ‰€æœ‰session"""
        subject_dir = os.path.join(self.data_root, f"sub-{subject_id}")
        if not os.path.exists(subject_dir):
            self.logger.warning(f"Subject directory not found: {subject_dir}")
            return []
        
        sessions = []
        try:
            for item in os.listdir(subject_dir):
                if item.startswith("ses-") and os.path.isdir(os.path.join(subject_dir, item)):
                    session_num = item.split("-")[1]
                    sessions.append(session_num)
        except Exception as e:
            self.logger.error(f"Error reading subject directory {subject_dir}: {e}")
            return []
        
        return sorted(sessions, key=lambda x: int(x) if x.isdigit() else 0)
    
    def find_eeg_files(self, subject_id: str, session_id: str) -> Optional[str]:
        """æŸ¥æ‰¾EEGæ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        eeg_dir = os.path.join(self.data_root, f"sub-{subject_id}", f"ses-{session_id}", "eeg")
        
        if not os.path.exists(eeg_dir):
            self.logger.warning(f"EEG directory not found: {eeg_dir}")
            return None
        
        # æŒ‰ä¼˜å…ˆçº§æœç´¢ä¸åŒæ ¼å¼çš„æ–‡ä»¶
        file_patterns = [
            f"sub-{subject_id}_ses-{session_id}_task-innerspeech_eeg.vhdr",
            f"sub-{subject_id}_ses-{session_id}_task-innerspeech_eeg.edf",
            f"sub-{subject_id}_ses-{session_id}_task-innerspeech_eeg.fif",
            f"sub-{subject_id}_ses-{session_id}_eeg.vhdr",
            f"sub-{subject_id}_ses-{session_id}_eeg.edf",
            f"sub-{subject_id}_ses-{session_id}_eeg.fif"
        ]
        
        for pattern in file_patterns:
            file_path = os.path.join(eeg_dir, pattern)
            if os.path.exists(file_path):
                return file_path
        
        # å¦‚æžœæ‰¾ä¸åˆ°ç‰¹å®šæ¨¡å¼ï¼Œåˆ—å‡ºæ‰€æœ‰EEGæ–‡ä»¶
        try:
            eeg_files = [f for f in os.listdir(eeg_dir) 
                        if f.endswith(('.vhdr', '.edf', '.fif', '.set', '.cnt'))]
            if eeg_files:
                return os.path.join(eeg_dir, eeg_files[0])
        except Exception as e:
            self.logger.error(f"Error listing EEG directory {eeg_dir}: {e}")
        
        return None
    
    def load_raw_eeg(self, subject_id: str, session_id: str) -> Optional[mne.io.Raw]:
        """åŠ è½½åŽŸå§‹EEGæ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼"""
        eeg_file = self.find_eeg_files(subject_id, session_id)
        
        if not eeg_file:
            self.logger.warning(f"No EEG file found for subject {subject_id}, session {session_id}")
            return None
        
        try:
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©åŠ è½½æ–¹æ³•
            file_ext = os.path.splitext(eeg_file)[1].lower()
            
            if file_ext == '.vhdr':
                raw = mne.io.read_raw_brainvision(eeg_file, preload=True, verbose=False)
            elif file_ext == '.edf':
                raw = mne.io.read_raw_edf(eeg_file, preload=True, verbose=False)
            elif file_ext == '.fif':
                raw = mne.io.read_raw_fif(eeg_file, preload=True, verbose=False)
            elif file_ext == '.set':
                raw = mne.io.read_raw_eeglab(eeg_file, preload=True, verbose=False)
            elif file_ext == '.cnt':
                raw = mne.io.read_raw_cnt(eeg_file, preload=True, verbose=False)
            else:
                self.logger.error(f"Unsupported file format: {file_ext}")
                return None
            
            self.logger.info(f"Successfully loaded {eeg_file}")
            return raw
            
        except Exception as e:
            self.logger.error(f"Error loading EEG data from {eeg_file}: {e}")
            return None
    
    def preprocess_eeg(self, raw: mne.io.Raw) -> Optional[mne.io.Raw]:
        """é¢„å¤„ç†EEGæ•°æ®"""
        try:
            # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŽŸå§‹æ•°æ®
            raw_copy = raw.copy()
            
            # æ£€æŸ¥å¹¶è®¾ç½®ç”µæžç±»åž‹
            if len(raw_copy.info['chs']) > 0:
                # è‡ªåŠ¨æ£€æµ‹EEGé€šé“
                raw_copy.set_channel_types({ch: 'eeg' for ch in raw_copy.ch_names 
                                          if not ch.startswith(('EOG', 'ECG', 'EMG', 'TRIG', 'STIM'))})
            
            # é€‰æ‹©EEGé€šé“
            raw_copy.pick_types(eeg=True, exclude='bads')
            
            if len(raw_copy.ch_names) == 0:
                self.logger.error("No EEG channels found after preprocessing")
                return None
            
            # è®¾ç½®å‚è€ƒç”µæž
            try:
                raw_copy.set_eeg_reference('average', projection=True, verbose=False)
                raw_copy.apply_proj(verbose=False)
            except Exception as e:
                self.logger.warning(f"Failed to set average reference: {e}")
            
            # æ»¤æ³¢
            try:
                raw_copy.filter(l_freq=self.config.LOW_FREQ, h_freq=self.config.HIGH_FREQ, 
                              fir_design='firwin', verbose=False)
            except Exception as e:
                self.logger.warning(f"Filtering failed: {e}")
            
            # é‡é‡‡æ ·ï¼ˆå¦‚æžœéœ€è¦ï¼‰
            if raw_copy.info['sfreq'] != self.config.SAMPLING_RATE:
                try:
                    raw_copy.resample(self.config.SAMPLING_RATE, verbose=False)
                except Exception as e:
                    self.logger.warning(f"Resampling failed: {e}")
            
            return raw_copy
            
        except Exception as e:
            self.logger.error(f"Preprocessing failed: {e}")
            return None
    
    def extract_epochs(self, raw: mne.io.Raw, epoch_length: float = None, 
                      overlap: float = 0.5, max_epochs: int = None) -> Optional[mne.Epochs]:
        """æå–å›ºå®šé•¿åº¦çš„epoch"""
        if epoch_length is None:
            epoch_length = self.config.TIME_WINDOW
        
        if max_epochs is None:
            max_epochs = self.config.MAX_EPOCHS_PER_SUBJECT
        
        try:
            # åˆ›å»ºè™šæ‹Ÿäº‹ä»¶
            duration = epoch_length
            interval = duration * (1 - overlap)
            
            n_samples = int(duration * raw.info['sfreq'])
            step_samples = int(interval * raw.info['sfreq'])
            
            if n_samples >= len(raw.times):
                self.logger.warning("Epoch length longer than recording, using full recording")
                n_samples = len(raw.times) - 1
            
            events = []
            event_id = 1
            
            # é™åˆ¶epochæ•°é‡ä»¥èŠ‚çœå†…å­˜
            max_start_sample = len(raw.times) - n_samples
            epoch_count = 0
            
            for start_sample in range(0, max_start_sample, step_samples):
                if epoch_count >= max_epochs:
                    break
                events.append([start_sample, 0, event_id])
                epoch_count += 1
            
            if not events:
                self.logger.error("No epochs could be created")
                return None
            
            events = np.array(events)
            
            epochs = mne.Epochs(raw, events, event_id={'epoch': event_id}, 
                               tmin=0, tmax=duration-1/raw.info['sfreq'], 
                               baseline=None, preload=True, verbose=False)
            
            # æ£€æŸ¥epochè´¨é‡
            if len(epochs) == 0:
                self.logger.error("No valid epochs after creation")
                return None
            
            self.logger.info(f"Created {len(epochs)} epochs of {duration}s each")
            return epochs
            
        except Exception as e:
            self.logger.error(f"Epoch extraction failed: {e}")
            return None
    
    def get_electrode_positions(self, ch_names: List[str]) -> Dict[str, Tuple[float, float]]:
        """èŽ·å–ç”µæžä½ç½®ï¼Œæ”¹è¿›çš„åŒ¹é…ç®—æ³•"""
        positions = {}
        unmatched_channels = []
        
        for ch_name in ch_names:
            # æ¸…ç†é€šé“åç§°
            clean_name = ch_name.strip()
            position_found = False
            
            # å°è¯•å¤šç§åŒ¹é…æ–¹å¼
            search_variants = [
                clean_name,
                clean_name.upper(),
                clean_name.lower(),
                clean_name.capitalize(),
                clean_name.replace(' ', ''),
                clean_name.replace('-', ''),
                clean_name.replace('_', '')
            ]
            
            for variant in search_variants:
                if variant in self.config.ELECTRODE_POSITIONS:
                    positions[ch_name] = self.config.ELECTRODE_POSITIONS[variant]
                    position_found = True
                    break
            
            if not position_found:
                unmatched_channels.append(ch_name)
        
        # ä¸ºæœªåŒ¹é…çš„ç”µæžåˆ†é…é»˜è®¤ä½ç½®
        if unmatched_channels:
            self.logger.warning(f"Using default positions for electrodes: {unmatched_channels}")
            for i, ch_name in enumerate(unmatched_channels):
                default_pos = self.config.get_default_electrode_position(
                    ch_name, len(ch_names), len(positions) + i
                )
                positions[ch_name] = default_pos
        
        return positions
    
    def check_memory_usage(self) -> bool:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        try:
            import psutil
            memory_usage = psutil.virtual_memory().percent
            if memory_usage > 85:  # è¶…è¿‡85%å†…å­˜ä½¿ç”¨çŽ‡
                self.logger.warning(f"High memory usage: {memory_usage:.1f}%")
                gc.collect()  # å¼ºåˆ¶åžƒåœ¾å›žæ”¶
                return False
            return True
        except ImportError:
            return True  # å¦‚æžœæ²¡æœ‰psutilï¼Œå‡è®¾å†…å­˜å……è¶³
    
    def load_all_subjects(self, max_subjects: Optional[int] = None) -> Dict:
        """åŠ è½½æ‰€æœ‰è¢«è¯•æ•°æ®ï¼Œæ”¹è¿›å†…å­˜ç®¡ç†"""
        all_data = {}
        
        if max_subjects is None:
            max_subjects = self.config.MAX_SUBJECTS
        
        # èŽ·å–æ‰€æœ‰è¢«è¯•ID
        subject_ids = []
        try:
            for item in os.listdir(self.data_root):
                if item.startswith("sub-") and os.path.isdir(os.path.join(self.data_root, item)):
                    subject_id = item.split("-")[1]
                    subject_ids.append(subject_id)
        except Exception as e:
            self.logger.error(f"Error reading data directory {self.data_root}: {e}")
            return {}
        
        subject_ids = sorted(subject_ids, key=lambda x: int(x) if x.isdigit() else 0)
        if max_subjects:
            subject_ids = subject_ids[:max_subjects]
        
        self.logger.info(f"Found {len(subject_ids)} subjects to process")
        
        successful_loads = 0
        for subject_id in tqdm(subject_ids, desc="Loading subjects"):
            if not self.check_memory_usage():
                self.logger.warning("Memory usage too high, stopping data loading")
                break
            
            sessions = self.get_subject_sessions(subject_id)
            if not sessions:
                continue
            
            all_data[subject_id] = {}
            
            for session_id in sessions:
                try:
                    raw = self.load_raw_eeg(subject_id, session_id)
                    if raw is not None:
                        raw = self.preprocess_eeg(raw)
                        if raw is not None:
                            epochs = self.extract_epochs(raw)
                            if epochs is not None:
                                all_data[subject_id][session_id] = {
                                    'raw': raw,
                                    'epochs': epochs,
                                    'positions': self.get_electrode_positions(raw.ch_names)
                                }
                                successful_loads += 1
                                self.logger.info(f"Successfully loaded subject {subject_id}, session {session_id}")
                            else:
                                self.logger.warning(f"Failed to extract epochs for subject {subject_id}, session {session_id}")
                        else:
                            self.logger.warning(f"Failed to preprocess data for subject {subject_id}, session {session_id}")
                    else:
                        self.logger.warning(f"Failed to load raw data for subject {subject_id}, session {session_id}")
                        
                except Exception as e:
                    self.logger.error(f"Error processing subject {subject_id}, session {session_id}: {e}")
                    continue
            
            # å¦‚æžœè¿™ä¸ªè¢«è¯•æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•sessionï¼Œç§»é™¤å®ƒ
            if not all_data[subject_id]:
                del all_data[subject_id]
                
            # å®šæœŸæ¸…ç†å†…å­˜
            if successful_loads % 5 == 0:
                gc.collect()
        
        self.logger.info(f"Successfully loaded data from {len(all_data)} subjects")
        return all_data

# ========== src/topography.py ==========
# ç›¸å¯¹è·¯å¾„: src/topography.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./src/topography.py

import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
from scipy.ndimage import gaussian_filter
import cv2
from typing import Dict, Tuple, List, Optional
import logging
import warnings

# æŠ‘åˆ¶æ’å€¼è­¦å‘Š
warnings.filterwarnings('ignore', category=RuntimeWarning)

class TopographyGenerator:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # é¢„è®¡ç®—å¤´éƒ¨æŽ©ç ä»¥æé«˜æ•ˆçŽ‡
        self._head_mask = self.create_head_mask(config.TOPO_SIZE)
        
    def create_head_mask(self, size: Tuple[int, int]) -> np.ndarray:
        """åˆ›å»ºå¤´éƒ¨è½®å»“æŽ©ç """
        h, w = size
        center = (w // 2, h // 2)
        radius = min(w, h) // 2 - 5
        
        # åˆ›å»ºåœ†å½¢æŽ©ç 
        y, x = np.ogrid[:h, :w]
        mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2
        
        return mask.astype(bool)
    
    def electrode_to_pixel(self, pos: Tuple[float, float], size: Tuple[int, int]) -> Tuple[int, int]:
        """å°†ç”µæžä½ç½®è½¬æ¢ä¸ºåƒç´ åæ ‡"""
        x, y = pos
        h, w = size
        
        # æ ‡å‡†åŒ–åæ ‡åˆ°åƒç´ åæ ‡
        # ç”µæžåæ ‡èŒƒå›´ [-1, 1] æ˜ å°„åˆ°åƒç´ åæ ‡
        pixel_x = int((x + 1) * w / 2)
        pixel_y = int((1 - y) * h / 2)  # Yè½´ç¿»è½¬
        
        # ç¡®ä¿åœ¨è¾¹ç•Œå†…
        pixel_x = max(0, min(w - 1, pixel_x))
        pixel_y = max(0, min(h - 1, pixel_y))
        
        return pixel_x, pixel_y
    
    def validate_electrode_data(self, eeg_data: np.ndarray, 
                               electrode_positions: Dict[str, Tuple[float, float]],
                               ch_names: List[str]) -> Tuple[np.ndarray, List[Tuple[float, float]], List[str]]:
        """éªŒè¯å’Œæ¸…ç†ç”µæžæ•°æ®"""
        valid_positions = []
        valid_values = []
        valid_names = []
        
        for i, ch_name in enumerate(ch_names):
            if ch_name in electrode_positions:
                pos = electrode_positions[ch_name]
                
                # æ£€æŸ¥ä½ç½®æ˜¯å¦åˆç†
                if abs(pos[0]) <= 1.2 and abs(pos[1]) <= 1.2:  # å…è®¸ç•¥å¾®è¶…å‡ºæ ‡å‡†èŒƒå›´
                    # æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                    if i < len(eeg_data) and not np.isnan(eeg_data[i]) and not np.isinf(eeg_data[i]):
                        valid_positions.append(pos)
                        valid_values.append(eeg_data[i])
                        valid_names.append(ch_name)
                    else:
                        self.logger.warning(f"Invalid data for electrode {ch_name}: {eeg_data[i] if i < len(eeg_data) else 'missing'}")
                else:
                    self.logger.warning(f"Invalid position for electrode {ch_name}: {pos}")
            else:
                self.logger.warning(f"No position found for electrode {ch_name}")
        
        return np.array(valid_values), valid_positions, valid_names
    
    def generate_topography(self, eeg_data: np.ndarray, 
                          electrode_positions: Dict[str, Tuple[float, float]],
                          ch_names: List[str]) -> np.ndarray:
        """ç”Ÿæˆè„‘ç”µåœ°å½¢å›¾"""
        size = self.config.TOPO_SIZE
        
        # éªŒè¯å’Œæ¸…ç†æ•°æ®
        values, positions, valid_names = self.validate_electrode_data(
            eeg_data, electrode_positions, ch_names
        )
        
        if len(positions) < 3:
            self.logger.warning(f"Not enough valid electrode positions for interpolation: {len(positions)}")
            return np.zeros(size)
        
        positions = np.array(positions)
        
        # åˆ›å»ºæ’å€¼ç½‘æ ¼
        xi = np.linspace(-1.2, 1.2, size[1])  # ç¨å¾®æ‰©å¤§èŒƒå›´ä»¥èŽ·å¾—æ›´å¥½çš„è¾¹ç•Œæ•ˆæžœ
        yi = np.linspace(-1.2, 1.2, size[0])
        xi_grid, yi_grid = np.meshgrid(xi, yi)
        
        # æ‰§è¡Œæ’å€¼
        try:
            # é¦–å…ˆå°è¯•cubicæ’å€¼
            topography = griddata(positions, values, (xi_grid, yi_grid), 
                                method=self.config.INTERPOLATION_METHOD, 
                                fill_value=0)
            
            # æ£€æŸ¥æ’å€¼ç»“æžœ
            if np.all(np.isnan(topography)):
                raise ValueError("Cubic interpolation failed")
                
        except (ValueError, Exception) as e:
            self.logger.warning(f"Cubic interpolation failed: {e}, trying linear interpolation")
            try:
                topography = griddata(positions, values, (xi_grid, yi_grid), 
                                    method='linear', fill_value=0)
            except Exception as e2:
                self.logger.warning(f"Linear interpolation failed: {e2}, trying nearest neighbor")
                topography = griddata(positions, values, (xi_grid, yi_grid), 
                                    method='nearest', fill_value=0)
        
        # å¤„ç†NaNå€¼
        topography = np.nan_to_num(topography, nan=0.0, posinf=0.0, neginf=0.0)
        
        # åº”ç”¨å¤´éƒ¨æŽ©ç 
        topography[~self._head_mask] = 0
        
        # å¹³æ»‘å¤„ç†
        try:
            sigma = max(1.0, min(size) / 64.0)  # è‡ªé€‚åº”å¹³æ»‘å‚æ•°
            topography = gaussian_filter(topography, sigma=sigma)
        except Exception as e:
            self.logger.warning(f"Gaussian filtering failed: {e}")
        
        return topography
    
    def generate_time_series_topographies(self, epochs_data: np.ndarray,
                                        electrode_positions: Dict[str, Tuple[float, float]],
                                        ch_names: List[str]) -> np.ndarray:
        """ä¸ºæ—¶é—´åºåˆ—æ•°æ®ç”Ÿæˆåœ°å½¢å›¾åºåˆ—"""
        n_epochs, n_channels, n_times = epochs_data.shape
        size = self.config.TOPO_SIZE
        
        self.logger.info(f"ç”Ÿæˆåœ°å½¢å›¾åºåˆ—: {n_epochs} epochs, {n_channels} channels, {n_times} time points")
        
        # åˆå§‹åŒ–è¾“å‡ºæ•°ç»„
        topographies = np.zeros((n_epochs, n_times, size[0], size[1]))
        
        # é¢„å¤„ç†ç”µæžä½ç½®ä¿¡æ¯
        valid_electrode_indices = []
        valid_positions = []
        
        for i, ch_name in enumerate(ch_names):
            if ch_name in electrode_positions:
                pos = electrode_positions[ch_name]
                if abs(pos[0]) <= 1.2 and abs(pos[1]) <= 1.2:
                    valid_electrode_indices.append(i)
                    valid_positions.append(pos)
        
        if len(valid_positions) < 3:
            self.logger.error("Not enough valid electrodes for topography generation")
            return topographies
        
        self.logger.info(f"Using {len(valid_positions)} valid electrodes for interpolation")
        
        # ç”Ÿæˆåœ°å½¢å›¾
        for epoch in range(n_epochs):
            for time_point in range(n_times):
                # æå–æœ‰æ•ˆç”µæžçš„æ•°æ®
                valid_data = epochs_data[epoch, valid_electrode_indices, time_point]
                
                # æ£€æŸ¥æ•°æ®è´¨é‡
                if np.any(np.isfinite(valid_data)) and np.std(valid_data) > 1e-10:
                    try:
                        topo = self._generate_single_topography(valid_data, valid_positions)
                        topographies[epoch, time_point] = topo
                    except Exception as e:
                        self.logger.warning(f"Failed to generate topography for epoch {epoch}, time {time_point}: {e}")
                        topographies[epoch, time_point] = np.zeros(size)
                else:
                    # æ•°æ®è´¨é‡ä¸å¥½æˆ–å…¨ä¸ºé›¶ï¼Œä½¿ç”¨é›¶åœ°å½¢å›¾
                    topographies[epoch, time_point] = np.zeros(size)
        
        return topographies
    
    def _generate_single_topography(self, eeg_data: np.ndarray, 
                                   positions: List[Tuple[float, float]]) -> np.ndarray:
        """ç”Ÿæˆå•ä¸ªåœ°å½¢å›¾ï¼ˆå†…éƒ¨æ–¹æ³•ï¼Œå·²çŸ¥æ•°æ®æœ‰æ•ˆï¼‰"""
        size = self.config.TOPO_SIZE
        positions = np.array(positions)
        
        # åˆ›å»ºæ’å€¼ç½‘æ ¼
        xi = np.linspace(-1.2, 1.2, size[1])
        yi = np.linspace(-1.2, 1.2, size[0])
        xi_grid, yi_grid = np.meshgrid(xi, yi)
        
        # æ‰§è¡Œæ’å€¼
        try:
            topography = griddata(positions, eeg_data, (xi_grid, yi_grid), 
                                method=self.config.INTERPOLATION_METHOD, 
                                fill_value=0)
            
            if np.all(np.isnan(topography)):
                topography = griddata(positions, eeg_data, (xi_grid, yi_grid), 
                                    method='linear', fill_value=0)
        except:
            topography = griddata(positions, eeg_data, (xi_grid, yi_grid), 
                                method='nearest', fill_value=0)
        
        # å¤„ç†å¼‚å¸¸å€¼
        topography = np.nan_to_num(topography, nan=0.0, posinf=0.0, neginf=0.0)
        
        # åº”ç”¨å¤´éƒ¨æŽ©ç 
        topography[~self._head_mask] = 0
        
        # å¹³æ»‘å¤„ç†
        sigma = max(1.0, min(size) / 64.0)
        topography = gaussian_filter(topography, sigma=sigma)
        
        return topography
    
    def normalize_topography(self, topography: np.ndarray, 
                           method: str = 'minmax') -> np.ndarray:
        """æ ‡å‡†åŒ–åœ°å½¢å›¾"""
        # åªè€ƒè™‘å¤´éƒ¨åŒºåŸŸå†…çš„å€¼
        masked_topo = topography[self._head_mask]
        
        if len(masked_topo) == 0 or np.all(masked_topo == 0):
            return topography
        
        if method == 'minmax':
            topo_min = np.min(masked_topo)
            topo_max = np.max(masked_topo)
            
            if topo_max > topo_min:
                # åªæ ‡å‡†åŒ–å¤´éƒ¨åŒºåŸŸ
                normalized = topography.copy()
                normalized[self._head_mask] = (masked_topo - topo_min) / (topo_max - topo_min)
                return normalized
            else:
                return topography
                
        elif method == 'zscore':
            mean_val = np.mean(masked_topo)
            std_val = np.std(masked_topo)
            
            if std_val > 1e-10:
                normalized = topography.copy()
                normalized[self._head_mask] = (masked_topo - mean_val) / std_val
                return normalized
            else:
                return topography
                
        elif method == 'robust':
            # ä½¿ç”¨ä¸­ä½æ•°å’Œå››åˆ†ä½è·è¿›è¡Œrobustæ ‡å‡†åŒ–
            median_val = np.median(masked_topo)
            q75, q25 = np.percentile(masked_topo, [75, 25])
            iqr = q75 - q25
            
            if iqr > 1e-10:
                normalized = topography.copy()
                normalized[self._head_mask] = (masked_topo - median_val) / iqr
                return normalized
            else:
                return topography
        else:
            return topography
    
    def enhance_topography(self, topography: np.ndarray, 
                          enhancement_factor: float = 1.5) -> np.ndarray:
        """å¢žå¼ºåœ°å½¢å›¾å¯¹æ¯”åº¦"""
        enhanced = topography.copy()
        
        # åªå¤„ç†å¤´éƒ¨åŒºåŸŸ
        masked_data = enhanced[self._head_mask]
        
        if len(masked_data) > 0:
            # ä½¿ç”¨sigmoidå‡½æ•°å¢žå¼ºå¯¹æ¯”åº¦
            mean_val = np.mean(masked_data)
            enhanced_data = mean_val + (masked_data - mean_val) * enhancement_factor
            
            # ä½¿ç”¨tanhå‡½æ•°å¹³æ»‘æˆªæ–­
            enhanced_data = np.tanh(enhanced_data)
            
            enhanced[self._head_mask] = enhanced_data
        
        return enhanced
    
    def save_topography(self, topography: np.ndarray, filepath: str,
                       colormap: str = None, title: str = "") -> None:
        """ä¿å­˜åœ°å½¢å›¾"""
        if colormap is None:
            colormap = self.config.COLORMAP
        
        plt.figure(figsize=(8, 8))
        
        # åˆ›å»ºåœ°å½¢å›¾
        im = plt.imshow(topography, cmap=colormap, interpolation='bilinear', origin='upper')
        
        # æ·»åŠ é¢œè‰²æ¡
        cbar = plt.colorbar(im, shrink=0.8)
        cbar.set_label('æ¿€æ´»å¼ºåº¦ (Î¼V)', fontsize=12)
        
        # æ·»åŠ å¤´éƒ¨è½®å»“
        center = (topography.shape[1]//2, topography.shape[0]//2)
        radius = min(topography.shape)//2 - 5
        circle = plt.Circle(center, radius, fill=False, color='black', linewidth=2)
        plt.gca().add_patch(circle)
        
        plt.title(title if title else 'EEG Topography', fontsize=14, fontweight='bold')
        plt.axis('off')
        plt.tight_layout()
        
        try:
            plt.savefig(filepath, dpi=150, bbox_inches='tight')
            self.logger.info(f"Topography saved to {filepath}")
        except Exception as e:
            self.logger.error(f"Failed to save topography to {filepath}: {e}")
        finally:
            plt.close()
    
    def get_electrode_contributions(self, topography: np.ndarray,
                                  electrode_positions: Dict[str, Tuple[float, float]],
                                  ch_names: List[str]) -> Dict[str, float]:
        """è®¡ç®—å„ç”µæžå¯¹åœ°å½¢å›¾çš„è´¡çŒ®åº¦"""
        contributions = {}
        size = self.config.TOPO_SIZE
        
        for ch_name in ch_names:
            if ch_name in electrode_positions:
                pos = electrode_positions[ch_name]
                pixel_x, pixel_y = self.electrode_to_pixel(pos, size)
                
                # æå–ç”µæžå‘¨å›´åŒºåŸŸçš„å¹³å‡å€¼ä½œä¸ºè´¡çŒ®åº¦
                radius = 5
                y_min = max(0, pixel_y - radius)
                y_max = min(size[0], pixel_y + radius + 1)
                x_min = max(0, pixel_x - radius)
                x_max = min(size[1], pixel_x + radius + 1)
                
                region = topography[y_min:y_max, x_min:x_max]
                contributions[ch_name] = float(np.mean(region)) if region.size > 0 else 0.0
        
        return contributions
    
    def create_difference_topography(self, topo1: np.ndarray, topo2: np.ndarray) -> np.ndarray:
        """åˆ›å»ºå·®å¼‚åœ°å½¢å›¾"""
        if topo1.shape != topo2.shape:
            self.logger.error("Topographies must have the same shape for difference calculation")
            return np.zeros_like(topo1)
        
        difference = topo1 - topo2
        
        # åªä¿ç•™å¤´éƒ¨åŒºåŸŸçš„å·®å¼‚
        difference[~self._head_mask] = 0
        
        return difference

# ========== src/trajectory_analysis.py ==========
# ç›¸å¯¹è·¯å¾„: src/trajectory_analysis.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./src/trajectory_analysis.py

import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform, euclidean
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import logging

# ä½¿ç”¨fastdtwæ›¿ä»£dtw
try:
    from fastdtw import fastdtw
    DTW_AVAILABLE = True
except ImportError:
    DTW_AVAILABLE = False

class TrajectoryAnalyzer:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        if not DTW_AVAILABLE:
            self.logger.warning("FastDTW not available, using Euclidean distance for trajectory comparison")
    
    def compute_trajectory_features(self, trajectory: np.ndarray) -> Dict:
        """è®¡ç®—è½¨è¿¹ç‰¹å¾"""
        if len(trajectory) < 2:
            return {}
        
        try:
            # åŸºæœ¬å‡ ä½•ç‰¹å¾
            length = len(trajectory)
            
            # è®¡ç®—é€æ­¥è·ç¦»
            step_distances = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
            total_distance = np.sum(step_distances)
            displacement = np.linalg.norm(trajectory[-1] - trajectory[0])
            
            # é€Ÿåº¦ç‰¹å¾
            mean_velocity = np.mean(step_distances) if len(step_distances) > 0 else 0
            max_velocity = np.max(step_distances) if len(step_distances) > 0 else 0
            velocity_std = np.std(step_distances) if len(step_distances) > 0 else 0
            
            # æ–¹å‘ç‰¹å¾
            if length > 2:
                directions = np.diff(trajectory, axis=0)
                angles = np.arctan2(directions[:, 1], directions[:, 0])
                angle_changes = np.diff(angles)
                # å¤„ç†è§’åº¦è·³è·ƒ
                angle_changes = np.mod(angle_changes + np.pi, 2*np.pi) - np.pi
                mean_angle_change = np.mean(np.abs(angle_changes))
                tortuosity = total_distance / (displacement + 1e-8)
            else:
                mean_angle_change = 0
                tortuosity = 1
            
            # è¦†ç›–åŒºåŸŸ
            if length > 2:
                min_coords = np.min(trajectory, axis=0)
                max_coords = np.max(trajectory, axis=0)
                ranges = max_coords - min_coords
                bounding_area = np.prod(ranges) if np.all(ranges > 0) else 0
            else:
                bounding_area = 0
            
            # è´¨å¿ƒå’Œæ•£å¸ƒ
            centroid = np.mean(trajectory, axis=0)
            distances_to_centroid = np.linalg.norm(trajectory - centroid, axis=1)
            mean_spread = np.mean(distances_to_centroid)
            max_spread = np.max(distances_to_centroid)
            
            # è½¨è¿¹å¤æ‚åº¦
            complexity = np.sum(np.abs(np.diff(step_distances))) if len(step_distances) > 1 else 0
            
            return {
                'length': length,
                'total_distance': total_distance,
                'displacement': displacement,
                'mean_velocity': mean_velocity,
                'max_velocity': max_velocity,
                'velocity_std': velocity_std,
                'mean_angle_change': mean_angle_change,
                'tortuosity': tortuosity,
                'bounding_area': bounding_area,
                'straightness': displacement / (total_distance + 1e-8),
                'mean_spread': mean_spread,
                'max_spread': max_spread,
                'complexity': complexity
            }
            
        except Exception as e:
            self.logger.error(f"Error computing trajectory features: {e}")
            return {}
    
    def compute_dtw_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """è®¡ç®—ä¸¤æ¡è½¨è¿¹ä¹‹é—´çš„DTWè·ç¦»"""
        try:
            if DTW_AVAILABLE:
                distance, _ = fastdtw(traj1, traj2, dist=euclidean)
                return distance
            else:
                # ä½¿ç”¨å½¢çŠ¶åŒ¹é…çš„æ¬§å‡ é‡Œå¾—è·ç¦»ä½œä¸ºåŽå¤‡
                return self.compute_shape_distance(traj1, traj2)
        except Exception as e:
            self.logger.warning(f"DTW computation failed: {e}, using shape distance")
            return self.compute_shape_distance(traj1, traj2)
    
    def compute_shape_distance(self, traj1: np.ndarray, traj2: np.ndarray) -> float:
        """è®¡ç®—è½¨è¿¹å½¢çŠ¶è·ç¦»ï¼ˆå½“DTWä¸å¯ç”¨æ—¶çš„åŽå¤‡æ–¹æ¡ˆï¼‰"""
        try:
            # æ ‡å‡†åŒ–è½¨è¿¹é•¿åº¦
            from scipy.interpolate import interp1d
            
            # é€‰æ‹©è¾ƒçŸ­è½¨è¿¹çš„é•¿åº¦ä½œä¸ºæ ‡å‡†é•¿åº¦
            target_length = min(len(traj1), len(traj2), 50)  # é™åˆ¶æœ€å¤§é•¿åº¦ä»¥æé«˜æ•ˆçŽ‡
            
            if len(traj1) < 2 or len(traj2) < 2:
                return np.linalg.norm(traj1.flatten() - traj2.flatten())
            
            # åˆ›å»ºæ’å€¼å‡½æ•°
            t1 = np.linspace(0, 1, len(traj1))
            t2 = np.linspace(0, 1, len(traj2))
            t_new = np.linspace(0, 1, target_length)
            
            # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œæ’å€¼
            traj1_interp = np.zeros((target_length, traj1.shape[1]))
            traj2_interp = np.zeros((target_length, traj2.shape[1]))
            
            for dim in range(traj1.shape[1]):
                f1 = interp1d(t1, traj1[:, dim], kind='linear', bounds_error=False, fill_value='extrapolate')
                f2 = interp1d(t2, traj2[:, dim], kind='linear', bounds_error=False, fill_value='extrapolate')
                traj1_interp[:, dim] = f1(t_new)
                traj2_interp[:, dim] = f2(t_new)
            
            # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
            return np.linalg.norm(traj1_interp - traj2_interp)
            
        except Exception as e:
            self.logger.warning(f"Shape distance computation failed: {e}")
            # æœ€åŽçš„åŽå¤‡æ–¹æ¡ˆï¼šç®€å•çš„ç«¯ç‚¹è·ç¦»
            return euclidean(traj1[-1], traj2[-1]) + euclidean(traj1[0], traj2[0])
    
    def compute_trajectory_similarity_matrix(self, trajectories: Dict) -> np.ndarray:
        """è®¡ç®—è½¨è¿¹ç›¸ä¼¼æ€§çŸ©é˜µ"""
        trajectory_list = list(trajectories.values())
        n_trajectories = len(trajectory_list)
        
        if n_trajectories < 2:
            return np.array([[1.0]])
        
        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distance_matrix = np.zeros((n_trajectories, n_trajectories))
        
        self.logger.info(f"Computing similarity matrix for {n_trajectories} trajectories")
        
        for i in range(n_trajectories):
            for j in range(i+1, n_trajectories):
                traj1 = trajectory_list[i]['trajectory']
                traj2 = trajectory_list[j]['trajectory']
                
                try:
                    distance = self.compute_dtw_distance(traj1, traj2)
                    distance_matrix[i, j] = distance
                    distance_matrix[j, i] = distance
                except Exception as e:
                    self.logger.warning(f"Distance computation failed for trajectories {i}, {j}: {e}")
                    distance_matrix[i, j] = float('inf')
                    distance_matrix[j, i] = float('inf')
        
        # è½¬æ¢ä¸ºç›¸ä¼¼æ€§çŸ©é˜µ
        max_distance = np.max(distance_matrix[distance_matrix != float('inf')])
        if max_distance > 0:
            # å¤„ç†æ— ç©·å¤§å€¼
            distance_matrix[distance_matrix == float('inf')] = max_distance * 2
            similarity_matrix = 1 - distance_matrix / (max_distance * 2)
        else:
            similarity_matrix = np.ones_like(distance_matrix)
        
        # ç¡®ä¿å¯¹è§’çº¿ä¸º1
        np.fill_diagonal(similarity_matrix, 1.0)
        
        return similarity_matrix
    
    def cluster_trajectories(self, trajectories: Dict, method: str = 'hierarchical',
                           n_clusters: Optional[int] = None) -> Dict:
        """èšç±»è½¨è¿¹"""
        if len(trajectories) < 2:
            return {'labels': [0], 'n_clusters': 1, 'trajectory_ids': list(trajectories.keys())}
        
        # è®¡ç®—ç‰¹å¾çŸ©é˜µ
        features = []
        trajectory_ids = []
        
        for traj_id, traj_data in trajectories.items():
            traj_features = self.compute_trajectory_features(traj_data['trajectory'])
            if traj_features:  # ç¡®ä¿ç‰¹å¾ä¸ä¸ºç©º
                feature_vector = [
                    traj_features.get('total_distance', 0),
                    traj_features.get('displacement', 0),
                    traj_features.get('mean_velocity', 0),
                    traj_features.get('tortuosity', 1),
                    traj_features.get('straightness', 0),
                    traj_features.get('bounding_area', 0),
                    traj_features.get('mean_spread', 0),
                    traj_features.get('complexity', 0)
                ]
                features.append(feature_vector)
                trajectory_ids.append(traj_id)
        
        if len(features) < 2:
            return {'labels': [0], 'n_clusters': 1, 'trajectory_ids': trajectory_ids}
        
        features = np.array(features)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        try:
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(features)
        except Exception as e:
            self.logger.warning(f"Feature scaling failed: {e}, using original features")
            features_scaled = features
        
        # å¤„ç†NaNå’Œæ— ç©·å¤§å€¼
        features_scaled = np.nan_to_num(features_scaled, nan=0.0, posinf=1.0, neginf=-1.0)
        
        try:
            if method == 'hierarchical':
                labels, n_clusters = self._hierarchical_clustering(features_scaled, n_clusters)
            elif method == 'kmeans':
                labels, n_clusters = self._kmeans_clustering(features_scaled, n_clusters)
            elif method == 'dbscan':
                labels, n_clusters = self._dbscan_clustering(features_scaled)
            else:
                self.logger.error(f"Unknown clustering method: {method}")
                return {'labels': [0] * len(features), 'n_clusters': 1, 'trajectory_ids': trajectory_ids}
            
        except Exception as e:
            self.logger.error(f"Clustering failed: {e}")
            return {'labels': [0] * len(features), 'n_clusters': 1, 'trajectory_ids': trajectory_ids}
        
        return {
            'labels': labels,
            'n_clusters': n_clusters,
            'trajectory_ids': trajectory_ids,
            'features': features_scaled
        }
    
    def _hierarchical_clustering(self, features: np.ndarray, n_clusters: Optional[int]) -> Tuple[np.ndarray, int]:
        """å±‚æ¬¡èšç±»"""
        linkage_matrix = linkage(features, method='ward')
        
        if n_clusters is None:
            # è‡ªåŠ¨ç¡®å®šèšç±»æ•°
            max_clusters = min(len(features) // 2, 5)
            best_score = -1
            best_n_clusters = 2
            
            for n in range(2, max_clusters + 1):
                try:
                    labels = fcluster(linkage_matrix, n, criterion='maxclust')
                    if len(np.unique(labels)) > 1:
                        score = silhouette_score(features, labels)
                        if score > best_score:
                            best_score = score
                            best_n_clusters = n
                except Exception as e:
                    self.logger.warning(f"Silhouette score computation failed for n={n}: {e}")
                    continue
            
            n_clusters = best_n_clusters
        
        labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')
        return labels, n_clusters
    
    def _kmeans_clustering(self, features: np.ndarray, n_clusters: Optional[int]) -> Tuple[np.ndarray, int]:
        """K-meansèšç±»"""
        if n_clusters is None:
            n_clusters = min(len(features) // 2, 3)
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = kmeans.fit_predict(features)
        return labels, n_clusters
    
    def _dbscan_clustering(self, features: np.ndarray) -> Tuple[np.ndarray, int]:
        """DBSCANèšç±»"""
        dbscan = DBSCAN(eps=0.5, min_samples=2)
        labels = dbscan.fit_predict(features)
        n_clusters = len(np.unique(labels[labels >= 0]))
        return labels, n_clusters
    
    def analyze_subject_consistency(self, subject_trajectories: Dict) -> Dict:
        """åˆ†æžå•ä¸ªè¢«è¯•çš„è½¨è¿¹ä¸€è‡´æ€§"""
        results = {}
        
        for subject_id, sessions in subject_trajectories.items():
            self.logger.info(f"Analyzing consistency for subject {subject_id}")
            subject_results = {}
            
            # æ”¶é›†è¯¥è¢«è¯•çš„æ‰€æœ‰è½¨è¿¹
            all_trajectories = {}
            for session_id, trajectories in sessions.items():
                for traj_id, traj_data in trajectories.items():
                    key = f"{session_id}_{traj_id}"
                    all_trajectories[key] = traj_data
            
            if len(all_trajectories) > 1:
                try:
                    # è®¡ç®—ç›¸ä¼¼æ€§çŸ©é˜µ
                    similarity_matrix = self.compute_trajectory_similarity_matrix(all_trajectories)
                    
                    # èšç±»åˆ†æž
                    clustering_results = self.cluster_trajectories(all_trajectories)
                    
                    # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
                    upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
                    mean_similarity = np.mean(upper_triangle)
                    consistency_score = mean_similarity
                    
                    subject_results = {
                        'n_trajectories': len(all_trajectories),
                        'mean_similarity': float(mean_similarity),
                        'consistency_score': float(consistency_score),
                        'n_clusters': clustering_results['n_clusters'],
                        'clustering': clustering_results,
                        'similarity_matrix': similarity_matrix
                    }
                    
                except Exception as e:
                    self.logger.error(f"Consistency analysis failed for subject {subject_id}: {e}")
                    subject_results = {
                        'n_trajectories': len(all_trajectories),
                        'error': str(e)
                    }
            else:
                subject_results = {
                    'n_trajectories': len(all_trajectories),
                    'note': 'Insufficient trajectories for analysis'
                }
            
            results[subject_id] = subject_results
        
        return results
    
    def compare_subjects(self, subject_trajectories: Dict) -> Dict:
        """æ¯”è¾ƒä¸åŒè¢«è¯•ä¹‹é—´çš„å·®å¼‚"""
        # è®¡ç®—æ¯ä¸ªè¢«è¯•çš„æ€»ä½“ç‰¹å¾
        subject_features = {}
        
        for subject_id, sessions in subject_trajectories.items():
            all_trajectories = {}
            for session_id, trajectories in sessions.items():
                for traj_id, traj_data in trajectories.items():
                    key = f"{session_id}_{traj_id}"
                    all_trajectories[key] = traj_data
            
            if all_trajectories:
                # è®¡ç®—å¹³å‡ç‰¹å¾
                features_list = []
                for traj_data in all_trajectories.values():
                    traj_features = self.compute_trajectory_features(traj_data['trajectory'])
                    if traj_features:
                        features_list.append(traj_features)
                
                if features_list:
                    # è®¡ç®—å¹³å‡ç‰¹å¾
                    mean_features = {}
                    for key in features_list[0].keys():
                        values = [f[key] for f in features_list if key in f and not np.isnan(f[key])]
                        mean_features[key] = np.mean(values) if values else 0
                    
                    subject_features[subject_id] = mean_features
        
        # è®¡ç®—è¢«è¯•é—´è·ç¦»çŸ©é˜µ
        inter_subject_similarity = {}
        if len(subject_features) > 1:
            subject_ids = list(subject_features.keys())
            n_subjects = len(subject_ids)
            distance_matrix = np.zeros((n_subjects, n_subjects))
            
            # æå–ç‰¹å¾å‘é‡
            feature_keys = list(subject_features[subject_ids[0]].keys())
            subject_vectors = []
            
            for subject_id in subject_ids:
                vector = [subject_features[subject_id][key] for key in feature_keys]
                subject_vectors.append(vector)
            
            subject_vectors = np.array(subject_vectors)
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            for i in range(n_subjects):
                for j in range(i+1, n_subjects):
                    dist = euclidean(subject_vectors[i], subject_vectors[j])
                    distance_matrix[i, j] = dist
                    distance_matrix[j, i] = dist
            
            # è½¬æ¢ä¸ºç›¸ä¼¼æ€§
            max_dist = np.max(distance_matrix)
            if max_dist > 0:
                similarity_matrix = 1 - distance_matrix / max_dist
            else:
                similarity_matrix = np.ones_like(distance_matrix)
            
            np.fill_diagonal(similarity_matrix, 1.0)
            inter_subject_similarity = {
                'similarity_matrix': similarity_matrix,
                'subject_ids': subject_ids
            }
        
        return {
            'subject_features': subject_features,
            'n_subjects': len(subject_features),
            'inter_subject_similarity': inter_subject_similarity
        }
    
    def generate_summary_report(self, analysis_results: Dict) -> str:
        """ç”Ÿæˆåˆ†æžæŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("EEGè½¨è¿¹åˆ†æžæŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        if 'subject_consistency' in analysis_results:
            consistency_results = analysis_results['subject_consistency']
            n_subjects = len(consistency_results)
            report.append(f"åˆ†æžè¢«è¯•æ•°é‡: {n_subjects}")
            
            # ä¸€è‡´æ€§ç»Ÿè®¡
            consistency_scores = []
            trajectory_counts = []
            
            for subject_id, results in consistency_results.items():
                if 'consistency_score' in results:
                    consistency_scores.append(results['consistency_score'])
                    trajectory_counts.append(results['n_trajectories'])
            
            if consistency_scores:
                mean_consistency = np.mean(consistency_scores)
                std_consistency = np.std(consistency_scores)
                min_consistency = np.min(consistency_scores)
                max_consistency = np.max(consistency_scores)
                
                report.append(f"å¹³å‡ä¸€è‡´æ€§å¾—åˆ†: {mean_consistency:.3f} Â± {std_consistency:.3f}")
                report.append(f"ä¸€è‡´æ€§å¾—åˆ†èŒƒå›´: {min_consistency:.3f} - {max_consistency:.3f}")
                
                total_trajectories = sum(trajectory_counts)
                mean_trajectories = np.mean(trajectory_counts)
                report.append(f"æ€»è½¨è¿¹æ•°é‡: {total_trajectories}")
                report.append(f"å¹³å‡æ¯è¢«è¯•è½¨è¿¹æ•°: {mean_trajectories:.1f}")
            
            report.append("")
        
        # å„è¢«è¯•è¯¦ç»†ç»“æžœ
        if 'subject_consistency' in analysis_results:
            report.append("å„è¢«è¯•ä¸€è‡´æ€§åˆ†æžè¯¦æƒ…:")
            report.append("-" * 40)
            
            for subject_id, results in consistency_results.items():
                report.append(f"è¢«è¯• {subject_id}:")
                
                if 'error' in results:
                    report.append(f"  åˆ†æžå‡ºé”™: {results['error']}")
                elif 'note' in results:
                    report.append(f"  {results['note']}")
                    report.append(f"  è½¨è¿¹æ•°é‡: {results['n_trajectories']}")
                else:
                    report.append(f"  è½¨è¿¹æ•°é‡: {results['n_trajectories']}")
                    if 'consistency_score' in results:
                        report.append(f"  ä¸€è‡´æ€§å¾—åˆ†: {results['consistency_score']:.3f}")
                        report.append(f"  èšç±»æ•°é‡: {results['n_clusters']}")
                        
                        # å¦‚æžœæœ‰èšç±»ä¿¡æ¯ï¼Œæ˜¾ç¤ºèšç±»åˆ†å¸ƒ
                        if 'clustering' in results and 'labels' in results['clustering']:
                            labels = results['clustering']['labels']
                            unique_labels, counts = np.unique(labels, return_counts=True)
                            cluster_info = [f"ç°‡{label}: {count}æ¡è½¨è¿¹" for label, count in zip(unique_labels, counts)]
                            report.append(f"  èšç±»åˆ†å¸ƒ: {', '.join(cluster_info)}")
                
                report.append("")
        
        # è¢«è¯•é—´æ¯”è¾ƒ
        if 'subject_comparison' in analysis_results:
            comparison_results = analysis_results['subject_comparison']
            report.append("è¢«è¯•é—´æ¯”è¾ƒ:")
            report.append("-" * 40)
            
            if 'inter_subject_similarity' in comparison_results:
                similarity_info = comparison_results['inter_subject_similarity']
                if 'similarity_matrix' in similarity_info:
                    similarity_matrix = similarity_info['similarity_matrix']
                    # è®¡ç®—å¹³å‡è¢«è¯•é—´ç›¸ä¼¼æ€§ï¼ˆæŽ’é™¤å¯¹è§’çº¿ï¼‰
                    upper_triangle = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]
                    mean_inter_similarity = np.mean(upper_triangle)
                    report.append(f"å¹³å‡è¢«è¯•é—´ç›¸ä¼¼æ€§: {mean_inter_similarity:.3f}")
                    
                    # æ‰¾å‡ºæœ€ç›¸ä¼¼å’Œæœ€ä¸ç›¸ä¼¼çš„è¢«è¯•å¯¹
                    subject_ids = similarity_info['subject_ids']
                    max_idx = np.unravel_index(np.argmax(upper_triangle), 
                                             (len(subject_ids), len(subject_ids)))
                    min_idx = np.unravel_index(np.argmin(upper_triangle), 
                                             (len(subject_ids), len(subject_ids)))
                    
                    if len(subject_ids) > 1:
                        # é‡æ–°è®¡ç®—ä¸Šä¸‰è§’çŸ©é˜µçš„ç´¢å¼•
                        triu_indices = np.triu_indices_from(similarity_matrix, k=1)
                        max_pos = np.argmax(upper_triangle)
                        min_pos = np.argmin(upper_triangle)
                        
                        max_i, max_j = triu_indices[0][max_pos], triu_indices[1][max_pos]
                        min_i, min_j = triu_indices[0][min_pos], triu_indices[1][min_pos]
                        
                        report.append(f"æœ€ç›¸ä¼¼è¢«è¯•å¯¹: {subject_ids[max_i]} - {subject_ids[max_j]} "
                                    f"(ç›¸ä¼¼æ€§: {similarity_matrix[max_i, max_j]:.3f})")
                        report.append(f"æœ€ä¸ç›¸ä¼¼è¢«è¯•å¯¹: {subject_ids[min_i]} - {subject_ids[min_j]} "
                                    f"(ç›¸ä¼¼æ€§: {similarity_matrix[min_i, min_j]:.3f})")
            
            report.append("")
        
        # æ·»åŠ æ–¹æ³•è¯´æ˜Ž
        report.append("åˆ†æžæ–¹æ³•è¯´æ˜Ž:")
        report.append("-" * 40)
        report.append("â€¢ è½¨è¿¹ç‰¹å¾: åŒ…æ‹¬æ€»è·ç¦»ã€ä½ç§»ã€é€Ÿåº¦ã€å¼¯æ›²åº¦ã€ç›´çº¿åº¦ç­‰")
        if DTW_AVAILABLE:
            report.append("â€¢ ç›¸ä¼¼æ€§è®¡ç®—: ä½¿ç”¨åŠ¨æ€æ—¶é—´è§„æ•´(DTW)ç®—æ³•")
        else:
            report.append("â€¢ ç›¸ä¼¼æ€§è®¡ç®—: ä½¿ç”¨å½¢çŠ¶åŒ¹é…çš„æ¬§å‡ é‡Œå¾—è·ç¦»")
        report.append("â€¢ èšç±»æ–¹æ³•: å±‚æ¬¡èšç±»ï¼Œè‡ªåŠ¨ç¡®å®šæœ€ä½³èšç±»æ•°")
        report.append("â€¢ ä¸€è‡´æ€§è¯„åˆ†: åŸºäºŽè½¨è¿¹é—´å¹³å‡ç›¸ä¼¼æ€§è®¡ç®—")
        
        return "\n".join(report)

# ========== src/visualization.py ==========
# ç›¸å¯¹è·¯å¾„: src/visualization.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./src/visualization.py

import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import seaborn as sns
import cv2
from typing import Dict, List, Tuple, Optional
import os
import logging
import warnings

# Suppress warnings
warnings.filterwarnings('ignore', category=UserWarning)

# Set up matplotlib for English only
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']

class Visualizer:
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Set color theme
        self.colors = plt.cm.Set1(np.linspace(0, 1, 10))
        self.background_color = '#f8f9fa'
        self.grid_color = '#e9ecef'
        
        self.logger.info("Visualizer initialized with English labels")
    
    def setup_figure_style(self, fig, title: str = ""):
        """Set unified figure style"""
        fig.patch.set_facecolor(self.background_color)
        if title:
            fig.suptitle(title, fontsize=16, fontweight='bold', y=0.95)
    
    def add_head_outline(self, ax, center: Tuple[float, float], radius: float, 
                        color: str = 'black', linewidth: float = 2):
        """Add head outline"""
        circle = plt.Circle(center, radius, fill=False, color=color, 
                           linewidth=linewidth, alpha=0.8)
        ax.add_patch(circle)
        
        # Add nose marker
        nose_x, nose_y = center[0], center[1] + radius * 0.1
        ax.plot([nose_x], [nose_y], 'k^', markersize=8, alpha=0.8)
        
        # Add ear markers
        ear_y = center[1]
        left_ear_x = center[0] - radius * 1.1
        right_ear_x = center[0] + radius * 1.1
        ax.plot([left_ear_x, right_ear_x], [ear_y, ear_y], 'k-', 
                linewidth=3, alpha=0.6)
    
    def plot_topography(self, topography: np.ndarray, title: str = "", 
                       save_path: Optional[str] = None, show_colorbar: bool = True,
                       electrode_positions: Optional[Dict] = None) -> None:
        """Plot single topography"""
        try:
            fig, ax = plt.subplots(figsize=(10, 8))
            self.setup_figure_style(fig, title)
            
            # Create topography
            im = ax.imshow(topography, cmap=self.config.COLORMAP, 
                          interpolation='bilinear', origin='upper',
                          extent=[0, topography.shape[1], topography.shape[0], 0])
            
            # Add colorbar
            if show_colorbar:
                cbar = plt.colorbar(im, ax=ax, shrink=0.8, pad=0.02)
                cbar.set_label('Activation Intensity (Î¼V)', fontsize=12, fontweight='bold')
                cbar.ax.tick_params(labelsize=10)
            
            # Add head outline
            center = (topography.shape[1]//2, topography.shape[0]//2)
            radius = min(topography.shape)//2 - 5
            self.add_head_outline(ax, center, radius)
            
            # Mark electrodes if provided
            if electrode_positions:
                self.plot_electrode_positions(ax, electrode_positions, topography.shape)
            
            # Set axes
            ax.set_xlim(0, topography.shape[1])
            ax.set_ylim(topography.shape[0], 0)
            ax.set_aspect('equal')
            ax.axis('off')
            
            # Add scale bar
            self.add_scale_bar(ax, topography.shape)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight', 
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Topography saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot topography: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def plot_electrode_positions(self, ax, electrode_positions: Dict, 
                               topography_shape: Tuple[int, int]):
        """Mark electrode positions on topography"""
        for ch_name, (x, y) in electrode_positions.items():
            # Convert coordinates
            pixel_x = (x + 1) * topography_shape[1] / 2
            pixel_y = (1 - y) * topography_shape[0] / 2
            
            # Draw electrode point
            ax.plot(pixel_x, pixel_y, 'wo', markersize=6, 
                   markeredgecolor='black', markeredgewidth=1)
            
            # Add electrode label (only for key electrodes to avoid crowding)
            if ch_name in ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2']:
                ax.text(pixel_x, pixel_y-8, ch_name, ha='center', va='top',
                       fontsize=8, fontweight='bold', color='white',
                       bbox=dict(boxstyle='round,pad=0.2', facecolor='black', alpha=0.7))
    
    def add_scale_bar(self, ax, shape: Tuple[int, int]):
        """Add scale bar"""
        scale_length = shape[1] // 10
        scale_x = shape[1] * 0.05
        scale_y = shape[0] * 0.95
        
        ax.plot([scale_x, scale_x + scale_length], [scale_y, scale_y], 
               'k-', linewidth=3)
        ax.text(scale_x + scale_length/2, scale_y - 5, f'{scale_length}px',
               ha='center', va='top', fontsize=10, fontweight='bold')
    
    def plot_trajectories(self, trajectories: Dict, topography_shape: Tuple[int, int],
                         title: str = "", save_path: Optional[str] = None,
                         show_legend: bool = True, alpha: float = 0.8) -> None:
        """Plot trajectories"""
        try:
            fig, ax = plt.subplots(figsize=(12, 10))
            self.setup_figure_style(fig, title)
            
            # Create background
            background = np.zeros(topography_shape)
            ax.imshow(background, cmap='gray', alpha=0.2, origin='upper',
                     extent=[0, topography_shape[1], topography_shape[0], 0])
            
            # Add grid
            ax.grid(True, alpha=0.3, color=self.grid_color, linewidth=0.5)
            
            # Plot trajectories
            legend_elements = []
            
            for i, (traj_id, traj_data) in enumerate(trajectories.items()):
                trajectory = traj_data['trajectory']
                color = self.colors[i % len(self.colors)]
                
                # Plot trajectory line
                line = ax.plot(trajectory[:, 1], trajectory[:, 0], 
                              color=color, linewidth=3, alpha=alpha, 
                              label=f'Trajectory {traj_id}')[0]
                
                # Mark start point
                start_point = ax.scatter(trajectory[0, 1], trajectory[0, 0], 
                                       color=color, s=150, marker='o', 
                                       edgecolors='white', linewidth=2, 
                                       zorder=5, alpha=0.9)
                
                # Mark end point
                end_point = ax.scatter(trajectory[-1, 1], trajectory[-1, 0], 
                                     color=color, s=150, marker='s', 
                                     edgecolors='white', linewidth=2, 
                                     zorder=5, alpha=0.9)
                
                # Add direction arrows
                self.add_direction_arrows(ax, trajectory, color, alpha)
                
                # Add trajectory info
                trajectory_info = f"Trajectory {traj_id}\nLength: {len(trajectory)} points"
                
                # Safely get intensity information
                intensity_value = None
                for key in ['mean_intensity', 'final_intensity', 'intensity']:
                    if key in traj_data:
                        intensity_value = traj_data[key]
                        break
                
                if intensity_value is not None:
                    trajectory_info += f"\nIntensity: {intensity_value:.2f}"
                
                legend_elements.append((line, trajectory_info))
            
            # Add head outline
            center = (topography_shape[1]//2, topography_shape[0]//2)
            radius = min(topography_shape)//2 - 5
            self.add_head_outline(ax, center, radius)
            
            # Set axes
            ax.set_xlim(0, topography_shape[1])
            ax.set_ylim(topography_shape[0], 0)
            ax.set_aspect('equal')
            ax.set_xlabel('X Coordinate (pixels)', fontsize=12, fontweight='bold')
            ax.set_ylabel('Y Coordinate (pixels)', fontsize=12, fontweight='bold')
            
            # Add legend
            if show_legend and legend_elements:
                legend_lines = [elem[0] for elem in legend_elements]
                legend_labels = [elem[1] for elem in legend_elements]
                ax.legend(legend_lines, legend_labels, bbox_to_anchor=(1.05, 1), 
                         loc='upper left', fontsize=10)
            
            # Add statistics
            self.add_trajectory_stats(ax, trajectories, topography_shape)
            
            plt.tight_layout()
            
            if save_path:
                plt.savefig(save_path, dpi=self.config.DPI, bbox_inches='tight',
                           facecolor=self.background_color)
                plt.close()
                self.logger.info(f"Trajectories plot saved to {save_path}")
            else:
                plt.show()
                
        except Exception as e:
            self.logger.error(f"Failed to plot trajectories: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def add_direction_arrows(self, ax, trajectory: np.ndarray, color, alpha: float):
        """Add trajectory direction arrows"""
        if len(trajectory) < 3:
            return
        
        # Add arrows at key points in trajectory
        arrow_positions = [len(trajectory)//4, len(trajectory)//2, 3*len(trajectory)//4]
        
        for pos in arrow_positions:
            if pos < len(trajectory) - 1:
                start = trajectory[pos]
                end = trajectory[pos + 1]
                
                dx = end[1] - start[1]
                dy = end[0] - start[0]
                
                # Only show arrow if movement is significant
                if abs(dx) > 0.5 or abs(dy) > 0.5:
                    try:
                        ax.arrow(start[1], start[0], dx, dy, 
                                head_width=3, head_length=4, fc=color, ec=color,
                                alpha=alpha*0.7, length_includes_head=True)
                    except:
                        pass  # Ignore arrow drawing errors
    
    def add_trajectory_stats(self, ax, trajectories: Dict, shape: Tuple[int, int]):
        """Add trajectory statistics"""
        stats_text = f"Total Trajectories: {len(trajectories)}\n"
        
        if trajectories:
            lengths = [len(traj_data['trajectory']) for traj_data in trajectories.values()]
            stats_text += f"Average Length: {np.mean(lengths):.1f} points\n"
            stats_text += f"Length Range: {min(lengths)}-{max(lengths)} points"
        
        ax.text(0.02, 0.98, stats_text, transform=ax.transAxes, 
               fontsize=10, verticalalignment='top',
               bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))
    
    def create_algorithm_comparison_plot(self, algorithm_results: Dict, save_path: str):
        """Create comprehensive algorithm comparison visualization"""
        try:
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('EEG Trajectory Tracking Algorithm Comparison', fontsize=16, fontweight='bold')
            
            algorithms = list(algorithm_results.keys())
            colors = plt.cm.Set1(np.linspace(0, 1, len(algorithms)))
            
            # 1. Trajectory Count Comparison
            ax = axes[0, 0]
            trajectory_counts = []
            for alg in algorithms:
                count = algorithm_results[alg].get('avg_trajectories', 0)
                trajectory_counts.append(count)
            
            bars = ax.bar(algorithms, trajectory_counts, color=colors, alpha=0.7)
            ax.set_title('Average Trajectory Count', fontweight='bold')
            ax.set_ylabel('Number of Trajectories')
            ax.tick_params(axis='x', rotation=45)
            
            # Add value labels
            for bar, count in zip(bars, trajectory_counts):
                if count > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                           f'{count:.1f}', ha='center', va='bottom')
            
            # 2. Computation Time Comparison
            ax = axes[0, 1]
            comp_times = []
            for alg in algorithms:
                time = algorithm_results[alg].get('avg_time', 0)
                comp_times.append(time)
            
            bars = ax.bar(algorithms, comp_times, color=colors, alpha=0.7)
            ax.set_title('Average Computation Time', fontweight='bold')
            ax.set_ylabel('Time (seconds)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, time in zip(bars, comp_times):
                if time > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{time:.3f}s', ha='center', va='bottom', fontsize=9)
            
            # 3. Trajectory Quality Comparison
            ax = axes[0, 2]
            qualities = []
            for alg in algorithms:
                quality = algorithm_results[alg].get('avg_quality', 0)
                qualities.append(quality)
            
            bars = ax.bar(algorithms, qualities, color=colors, alpha=0.7)
            ax.set_title('Average Trajectory Quality', fontweight='bold')
            ax.set_ylabel('Quality Score')
            ax.tick_params(axis='x', rotation=45)
            ax.set_ylim(0, 1)
            
            for bar, quality in zip(bars, qualities):
                if quality > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                           f'{quality:.3f}', ha='center', va='bottom')
            
            # 4. Trajectory Length Comparison
            ax = axes[1, 0]
            lengths = []
            for alg in algorithms:
                length = algorithm_results[alg].get('avg_length', 0)
                lengths.append(length)
            
            bars = ax.bar(algorithms, lengths, color=colors, alpha=0.7)
            ax.set_title('Average Trajectory Length', fontweight='bold')
            ax.set_ylabel('Length (frames)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, length in zip(bars, lengths):
                if length > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                           f'{length:.1f}', ha='center', va='bottom')
            
            # 5. Performance Efficiency (Trajectories per Second)
            ax = axes[1, 1]
            efficiencies = []
            for alg in algorithms:
                traj_count = algorithm_results[alg].get('avg_trajectories', 0)
                time = algorithm_results[alg].get('avg_time', 1e-6)  # Avoid division by zero
                efficiency = traj_count / time if time > 0 else 0
                efficiencies.append(efficiency)
            
            bars = ax.bar(algorithms, efficiencies, color=colors, alpha=0.7)
            ax.set_title('Processing Efficiency', fontweight='bold')
            ax.set_ylabel('Trajectories per Second')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, eff in zip(bars, efficiencies):
                if eff > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                           f'{eff:.1f}', ha='center', va='bottom')
            
            # 6. Overall Performance Score
            ax = axes[1, 2]
            performance_scores = []
            for alg in algorithms:
                # Calculate composite performance score
                traj_score = min(1.0, algorithm_results[alg].get('avg_trajectories', 0) / 5.0)
                quality_score = algorithm_results[alg].get('avg_quality', 0)
                time_penalty = max(0, 1.0 - algorithm_results[alg].get('avg_time', 0) / 0.5)
                
                overall_score = (traj_score * 0.4 + quality_score * 0.4 + time_penalty * 0.2)
                performance_scores.append(overall_score)
            
            bars = ax.bar(algorithms, performance_scores, color=colors, alpha=0.7)
            ax.set_title('Overall Performance Score', fontweight='bold')
            ax.set_ylabel('Composite Score')
            ax.tick_params(axis='x', rotation=45)
            ax.set_ylim(0, 1)
            
            for bar, score in zip(bars, performance_scores):
                if score > 0:
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                           f'{score:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Algorithm comparison plot saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to create algorithm comparison plot: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def create_performance_radar_chart(self, algorithm_results: Dict, save_path: str):
        """Create radar chart for algorithm performance comparison"""
        try:
            fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
            
            # Performance metrics
            metrics = ['Trajectory Count', 'Quality Score', 'Speed', 'Efficiency', 'Stability']
            algorithms = list(algorithm_results.keys())
            colors = plt.cm.Set1(np.linspace(0, 1, len(algorithms)))
            
            # Normalize metrics to 0-1 scale
            max_values = {}
            for metric in metrics:
                if metric == 'Trajectory Count':
                    max_values[metric] = max([alg.get('avg_trajectories', 0) for alg in algorithm_results.values()])
                elif metric == 'Quality Score':
                    max_values[metric] = 1.0
                elif metric == 'Speed':
                    min_time = min([alg.get('avg_time', 1) for alg in algorithm_results.values() if alg.get('avg_time', 1) > 0])
                    max_values[metric] = min_time  # Lower time is better
                elif metric == 'Efficiency':
                    max_values[metric] = max([alg.get('avg_trajectories', 0) / max(alg.get('avg_time', 1), 1e-6) 
                                            for alg in algorithm_results.values()])
                elif metric == 'Stability':
                    max_values[metric] = 1.0
            
            # Plot each algorithm
            angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
            angles += angles[:1]  # Complete the circle
            
            for i, (alg_name, alg_data) in enumerate(algorithm_results.items()):
                values = []
                
                # Calculate normalized values
                traj_count = alg_data.get('avg_trajectories', 0)
                values.append(traj_count / max(max_values['Trajectory Count'], 1))
                
                quality = alg_data.get('avg_quality', 0)
                values.append(quality)
                
                time = alg_data.get('avg_time', 1)
                speed_score = max_values['Speed'] / max(time, 1e-6) if time > 0 else 0
                values.append(min(1.0, speed_score))
                
                efficiency = traj_count / max(time, 1e-6)
                values.append(efficiency / max(max_values['Efficiency'], 1))
                
                stability = 1.0 - min(1.0, alg_data.get('computation_time_std', 0) / max(time, 1e-6))
                values.append(max(0, stability))
                
                values += values[:1]  # Complete the circle
                
                ax.plot(angles, values, 'o-', linewidth=2, label=alg_name, 
                       color=colors[i % len(colors)])
                ax.fill(angles, values, alpha=0.15, color=colors[i % len(colors)])
            
            # Customize the chart
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            ax.set_ylim(0, 1)
            ax.set_title('Algorithm Performance Radar Chart', size=16, fontweight='bold', pad=20)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            ax.grid(True)
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Performance radar chart saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to create radar chart: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def create_detailed_comparison_table(self, algorithm_results: Dict, save_path: str):
        """Create detailed comparison table visualization"""
        try:
            # Prepare data for table
            data = []
            headers = ['Algorithm', 'Avg Trajectories', 'Avg Quality', 'Avg Time (s)', 
                      'Efficiency', 'Best For']
            
            best_for = {
                'greedy': 'Real-time processing',
                'hungarian': 'High precision tasks',
                'kalman': 'Predictable motion',
                'overlap': 'Shape-stable objects',
                'hybrid': 'Complex scenarios'
            }
            
            for alg_name, alg_data in algorithm_results.items():
                row = [
                    alg_name.capitalize(),
                    f"{alg_data.get('avg_trajectories', 0):.2f}",
                    f"{alg_data.get('avg_quality', 0):.3f}",
                    f"{alg_data.get('avg_time', 0):.4f}",
                    f"{alg_data.get('avg_trajectories', 0) / max(alg_data.get('avg_time', 1), 1e-6):.1f}",
                    best_for.get(alg_name, 'General purpose')
                ]
                data.append(row)
            
            # Create table plot
            fig, ax = plt.subplots(figsize=(14, 8))
            ax.axis('tight')
            ax.axis('off')
            
            table = ax.table(cellText=data, colLabels=headers, cellLoc='center', loc='center')
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1.2, 1.5)
            
            # Style the table
            for i in range(len(headers)):
                table[(0, i)].set_facecolor('#40466e')
                table[(0, i)].set_text_props(weight='bold', color='white')
            
            # Color rows alternately
            for i in range(1, len(data) + 1):
                for j in range(len(headers)):
                    if i % 2 == 0:
                        table[(i, j)].set_facecolor('#f1f1f2')
                    else:
                        table[(i, j)].set_facecolor('white')
            
            plt.title('Algorithm Performance Comparison Table', fontsize=16, fontweight='bold', pad=20)
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Comparison table saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to create comparison table: {e}")
            if 'fig' in locals():
                plt.close(fig)
    
    def create_trajectory_animation(self, topographies: np.ndarray, 
                                  tracking_results: Dict,
                                  save_path: str, fps: int = None) -> None:
        """Create trajectory animation (simplified version)"""
        if fps is None:
            fps = self.config.FPS
        
        try:
            # Save frame sequence instead of animation for better compatibility
            frame_dir = f"{os.path.splitext(save_path)[0]}_frames"
            os.makedirs(frame_dir, exist_ok=True)
            
            n_frames = min(len(topographies), self.config.MAX_SAVE_FRAMES)
            
            for frame in range(n_frames):
                try:
                    fig, ax = plt.subplots(figsize=(10, 8))
                    
                    # Display topography
                    ax.imshow(topographies[frame], cmap=self.config.COLORMAP, 
                             interpolation='bilinear', origin='upper')
                    
                    # Draw trajectories up to current frame
                    if frame < len(tracking_results.get('frame_results', [])):
                        frame_result = tracking_results['frame_results'][frame]
                        
                        for i, region in enumerate(frame_result.get('tracked_regions', [])):
                            trajectory = np.array(region.trajectory)
                            color = self.colors[i % len(self.colors)]
                            
                            if len(trajectory) > 1:
                                ax.plot(trajectory[:, 1], trajectory[:, 0], 
                                       color=color, linewidth=2, alpha=0.8)
                            
                            if len(trajectory) > 0:
                                current_pos = trajectory[-1]
                                ax.scatter(current_pos[1], current_pos[0], 
                                          s=100, c=color, marker='o', 
                                          edgecolors='white', linewidth=2)
                    
                    # Add head outline
                    center = (topographies.shape[2]//2, topographies.shape[1]//2)
                    radius = min(topographies.shape[1:])//2 - 5
                    self.add_head_outline(ax, center, radius)
                    
                    ax.set_title(f'Frame {frame+1}/{n_frames}', fontweight='bold')
                    ax.axis('off')
                    
                    frame_path = os.path.join(frame_dir, f"frame_{frame:04d}.png")
                    plt.savefig(frame_path, dpi=150, bbox_inches='tight')
                    plt.close()
                    
                except Exception as e:
                    self.logger.warning(f"Failed to save frame {frame}: {e}")
                    if 'fig' in locals():
                        plt.close(fig)
                    continue
            
            self.logger.info(f"Animation frames saved to {frame_dir}")
            
        except Exception as e:
            self.logger.error(f"Failed to create animation: {e}")
    
    def create_summary_visualization(self, all_results: Dict, save_path: str):
        """Create comprehensive summary visualization"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle('EEG Trajectory Analysis Summary', fontsize=16, fontweight='bold')
            
            # Extract summary statistics
            subject_counts = {}
            algorithm_performance = {}
            
            for subject_id, sessions in all_results.items():
                for session_id, session_data in sessions.items():
                    for algorithm_name, alg_data in session_data.items():
                        if algorithm_name not in algorithm_performance:
                            algorithm_performance[algorithm_name] = {
                                'trajectory_counts': [],
                                'computation_times': [],
                                'qualities': []
                            }
                        
                        algorithm_performance[algorithm_name]['trajectory_counts'].append(
                            alg_data.get('total_trajectories', 0))
                        algorithm_performance[algorithm_name]['computation_times'].append(
                            alg_data.get('total_computation_time', 0))
                        
                        # Calculate average quality
                        qualities = []
                        for traj_data in alg_data.get('trajectories', {}).values():
                            qualities.append(traj_data.get('quality_score', 0))
                        
                        if qualities:
                            algorithm_performance[algorithm_name]['qualities'].append(np.mean(qualities))
            
            # 1. Algorithm Performance Summary
            ax = axes[0, 0]
            algorithms = list(algorithm_performance.keys())
            avg_trajectories = [np.mean(algorithm_performance[alg]['trajectory_counts']) 
                              for alg in algorithms]
            
            bars = ax.bar(algorithms, avg_trajectories, alpha=0.7)
            ax.set_title('Average Trajectories per Algorithm')
            ax.set_ylabel('Number of Trajectories')
            ax.tick_params(axis='x', rotation=45)
            
            # 2. Processing Time Comparison
            ax = axes[0, 1]
            avg_times = [np.mean(algorithm_performance[alg]['computation_times']) 
                        for alg in algorithms]
            
            bars = ax.bar(algorithms, avg_times, alpha=0.7, color='orange')
            ax.set_title('Average Processing Time')
            ax.set_ylabel('Time (seconds)')
            ax.tick_params(axis='x', rotation=45)
            
            # 3. Quality Distribution
            ax = axes[1, 0]
            all_qualities = []
            labels = []
            
            for alg in algorithms:
                qualities = algorithm_performance[alg]['qualities']
                if qualities:
                    all_qualities.extend(qualities)
                    labels.extend([alg] * len(qualities))
            
            if all_qualities:
                unique_algorithms = list(set(labels))
                quality_data = [algorithm_performance[alg]['qualities'] for alg in unique_algorithms]
                
                ax.boxplot(quality_data, labels=unique_algorithms)
                ax.set_title('Quality Score Distribution')
                ax.set_ylabel('Quality Score')
                ax.tick_params(axis='x', rotation=45)
            
            # 4. Summary Statistics
            ax = axes[1, 1]
            ax.axis('off')
            
            # Create summary text
            total_subjects = len(all_results)
            total_sessions = sum(len(sessions) for sessions in all_results.values())
            total_algorithms = len(algorithms)
            
            summary_text = f"""
Experiment Summary:
â€¢ Total Subjects: {total_subjects}
â€¢ Total Sessions: {total_sessions}  
â€¢ Algorithms Compared: {total_algorithms}
â€¢ Algorithms: {', '.join(algorithms)}

Best Performing Algorithm:
â€¢ Most Trajectories: {algorithms[np.argmax(avg_trajectories)]}
â€¢ Fastest Processing: {algorithms[np.argmin(avg_times)]}
"""
            
            ax.text(0.1, 0.9, summary_text, transform=ax.transAxes, 
                   fontsize=12, verticalalignment='top',
                   bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Summary visualization saved to {save_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to create summary visualization: {e}")
            if 'fig' in locals():
                plt.close(fig)

# ========== trackers/__init__.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/__init__.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/__init__.py

# è·Ÿè¸ªç®—æ³•æ¨¡å—
"""
EEGè½¨è¿¹è·Ÿè¸ªç®—æ³•é›†åˆ
åŒ…å«å¤šç§ä¸åŒçš„è½¨è¿¹è·Ÿè¸ªç®—æ³•å®žçŽ°
"""

from .base_tracker import BaseTracker
from .greedy_tracker import GreedyTracker
from .hungarian_tracker import HungarianTracker
from .kalman_tracker import KalmanTracker
from .overlap_tracker import OverlapTracker
from .hybrid_tracker import HybridTracker
from .tracker_factory import TrackerFactory

__all__ = [
    'BaseTracker',
    'GreedyTracker', 
    'HungarianTracker',
    'KalmanTracker',
    'OverlapTracker',
    'HybridTracker',
    'TrackerFactory'
]

__version__ = "2.0.0"

# ========== trackers/base_tracker.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/base_tracker.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/base_tracker.py

import numpy as np
import cv2
from scipy.ndimage import label, center_of_mass
import logging
import time
from typing import List, Tuple, Dict, Optional
from abc import ABC, abstractmethod

class Region:
    """è½¨è¿¹åŒºåŸŸç±»"""
    def __init__(self, center: Tuple[float, float], area: float, intensity: float, id: int):
        self.center = center
        self.area = area
        self.intensity = intensity
        self.id = id
        self.trajectory = [center]
        self.active = True
        self.inactive_frames = 0
        self.max_inactive_frames = 25
        self.velocity_history = []
        self.predicted_position = None
        self.last_mask = None
        self.quality_score = 0.0

class BaseTracker(ABC):
    """åŸºç¡€è·Ÿè¸ªå™¨æŠ½è±¡ç±»"""
    
    def __init__(self, config):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.regions = []
        self.next_region_id = 0
        
        # ç®—æ³•åç§°ï¼ˆç”±å­ç±»è®¾ç½®ï¼‰
        self.algorithm_name = "base"
        
        # æ€§èƒ½ç»Ÿè®¡
        self.performance_stats = {
            'total_frames': 0,
            'total_detections': 0,
            'total_matches': 0,
            'computation_times': [],
            'memory_usage': []
        }
    
    def detect_high_activation_regions(self, topography: np.ndarray, frame_idx: int = 0) -> List[Dict]:
        """æ£€æµ‹é«˜æ¿€æ´»åŒºåŸŸ - é€šç”¨å®žçŽ°"""
        try:
            # åªè€ƒè™‘éžé›¶åŒºåŸŸï¼ˆå¤´éƒ¨å†…éƒ¨ï¼‰
            valid_mask = topography != 0
            if not np.any(valid_mask):
                return []
            
            valid_values = topography[valid_mask]
            
            # è‡ªé€‚åº”é˜ˆå€¼è®¡ç®—
            threshold = np.percentile(valid_values, self.config.THRESHOLD_PERCENTILE)
            
            # äºŒå€¼åŒ–
            binary = (topography > threshold) & valid_mask
            
            if not np.any(binary):
                # é™ä½Žé˜ˆå€¼é‡è¯•
                threshold = np.percentile(valid_values, max(70, self.config.THRESHOLD_PERCENTILE - 15))
                binary = (topography > threshold) & valid_mask
                
                if not np.any(binary):
                    return []
            
            # å½¢æ€å­¦æ“ä½œæ¸…ç†å™ªå£°
            try:
                kernel = np.ones((3, 3), np.uint8)
                binary = cv2.morphologyEx(binary.astype(np.uint8), cv2.MORPH_OPEN, kernel)
                binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            except:
                pass
            
            # è¿žé€šåŸŸåˆ†æž
            labeled_array, num_features = label(binary)
            
            regions = []
            for i in range(1, num_features + 1):
                region_mask = labeled_array == i
                area = np.sum(region_mask)
                
                if area < self.config.MIN_REGION_SIZE:
                    continue
                
                # è®¡ç®—è´¨å¿ƒ
                center = center_of_mass(region_mask)
                
                # è®¡ç®—å¼ºåº¦ç»Ÿè®¡
                region_values = topography[region_mask]
                intensity = np.mean(region_values)
                max_intensity = np.max(region_values)
                
                regions.append({
                    'center': center,
                    'area': area,
                    'intensity': intensity,
                    'max_intensity': max_intensity,
                    'mask': region_mask,
                    'threshold_used': threshold
                })
            
            # æŒ‰å¼ºåº¦å’Œé¢ç§¯çš„ç»„åˆæŽ’åº
            regions.sort(key=lambda x: x['intensity'] * np.sqrt(x['area']), reverse=True)
            selected_regions = regions[:self.config.MAX_REGIONS]
            
            return selected_regions
            
        except Exception as e:
            self.logger.error(f"åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")
            return []
    
    @abstractmethod
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = 20.0, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """åŒ¹é…åŒºåŸŸ - ç”±å­ç±»å®žçŽ°å…·ä½“ç®—æ³•"""
        pass
    
    def update_tracker(self, topography: np.ndarray, frame_idx: int = 0) -> Dict:
        """æ›´æ–°è·Ÿè¸ªå™¨ - é€šç”¨æ¡†æž¶"""
        start_time = time.time()
        
        try:
            # æ£€æµ‹å½“å‰å¸§çš„åŒºåŸŸ
            current_regions = self.detect_high_activation_regions(topography, frame_idx)
            
            # ä½¿ç”¨å…·ä½“ç®—æ³•è¿›è¡ŒåŒ¹é…
            matches = self.match_regions(current_regions, frame_idx=frame_idx)
            
            # æ›´æ–°åŒ¹é…çš„åŒºåŸŸ
            active_regions = [r for r in self.regions if r.active]
            matched_tracked = set()
            matched_current = set()
            
            for tracked_idx, current_idx in matches:
                if tracked_idx < len(active_regions) and current_idx < len(current_regions):
                    region = active_regions[tracked_idx]
                    current_region = current_regions[current_idx]
                    
                    # æ›´æ–°è½¨è¿¹
                    region.trajectory.append(current_region['center'])
                    region.area = current_region['area']
                    region.intensity = current_region['intensity']
                    region.inactive_frames = 0
                    region.last_mask = current_region.get('mask')
                    
                    matched_tracked.add(tracked_idx)
                    matched_current.add(current_idx)
            
            # å¤„ç†æœªåŒ¹é…çš„è·Ÿè¸ªåŒºåŸŸ
            for i, region in enumerate(active_regions):
                if i not in matched_tracked:
                    region.inactive_frames += 1
                    
                    if region.inactive_frames >= region.max_inactive_frames:
                        region.active = False
            
            # ä¸ºæœªåŒ¹é…çš„å½“å‰åŒºåŸŸåˆ›å»ºæ–°çš„è·Ÿè¸ªåŒºåŸŸ
            for i, current_region in enumerate(current_regions):
                if i not in matched_current:
                    new_region = Region(
                        center=current_region['center'],
                        area=current_region['area'],
                        intensity=current_region['intensity'],
                        id=self.next_region_id
                    )
                    new_region.last_mask = current_region.get('mask')
                    self.regions.append(new_region)
                    self.next_region_id += 1
            
            # æ›´æ–°æ€§èƒ½ç»Ÿè®¡
            computation_time = time.time() - start_time
            self.performance_stats['computation_times'].append(computation_time)
            self.performance_stats['total_frames'] += 1
            self.performance_stats['total_detections'] += len(current_regions)
            self.performance_stats['total_matches'] += len(matches)
            
            return {
                'current_regions': current_regions,
                'tracked_regions': [r for r in self.regions if r.active],
                'all_regions': self.regions,
                'frame_idx': frame_idx,
                'matches': matches,
                'algorithm': self.algorithm_name
            }
            
        except Exception as e:
            self.logger.error(f"è·Ÿè¸ªæ›´æ–°å¤±è´¥: {e}")
            return {
                'current_regions': [],
                'tracked_regions': [],
                'all_regions': self.regions,
                'frame_idx': frame_idx,
                'error': str(e),
                'algorithm': self.algorithm_name
            }
    
    def track_sequence(self, topographies: np.ndarray) -> Dict:
        """è·Ÿè¸ªæ•´ä¸ªåºåˆ—"""
        n_frames = topographies.shape[0]
        tracking_results = []
        
        # é‡ç½®è·Ÿè¸ªå™¨
        self.reset_tracker()
        
        self.logger.info(f"å¼€å§‹ä½¿ç”¨{self.algorithm_name}ç®—æ³•è·Ÿè¸ª{n_frames}å¸§")
        
        start_time = time.time()
        
        try:
            for frame_idx in range(n_frames):
                topography = topographies[frame_idx]
                result = self.update_tracker(topography, frame_idx)
                result['frame'] = frame_idx
                tracking_results.append(result)
            
            total_time = time.time() - start_time
            
            # æå–è½¨è¿¹
            trajectories = self.extract_trajectories()
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            metrics = self.calculate_performance_metrics(trajectories, total_time)
            
            self.logger.info(f"{self.algorithm_name}ç®—æ³•å®Œæˆ: {len(trajectories)}æ¡è½¨è¿¹, "
                           f"è€—æ—¶{total_time:.2f}ç§’")
            
            return {
                'algorithm': self.algorithm_name,
                'frame_results': tracking_results,
                'trajectories': trajectories,
                'metrics': metrics,
                'summary': {
                    'total_regions': len(self.regions),
                    'tracked_regions': len(trajectories),
                    'total_frames': n_frames,
                    'total_time': total_time
                }
            }
            
        except Exception as e:
            self.logger.error(f"{self.algorithm_name}ç®—æ³•è·Ÿè¸ªå¤±è´¥: {e}")
            return {
                'algorithm': self.algorithm_name,
                'frame_results': tracking_results,
                'trajectories': {},
                'metrics': {},
                'summary': {
                    'total_regions': 0,
                    'tracked_regions': 0,
                    'total_frames': n_frames,
                    'error': str(e)
                }
            }
    
    def reset_tracker(self):
        """é‡ç½®è·Ÿè¸ªå™¨çŠ¶æ€"""
        self.regions = []
        self.next_region_id = 0
        self.performance_stats = {
            'total_frames': 0,
            'total_detections': 0,
            'total_matches': 0,
            'computation_times': [],
            'memory_usage': []
        }
    
    def extract_trajectories(self) -> Dict:
        """æå–æœ‰æ•ˆè½¨è¿¹"""
        trajectories = {}
        
        for region in self.regions:
            if len(region.trajectory) > 2:  # è‡³å°‘è·Ÿè¸ªäº†3å¸§
                try:
                    trajectory_array = np.array(region.trajectory)
                    
                    # è®¡ç®—åŸºæœ¬ç»Ÿè®¡
                    distances = np.linalg.norm(np.diff(trajectory_array, axis=0), axis=1)
                    total_distance = np.sum(distances)
                    avg_velocity = np.mean(distances) if len(distances) > 0 else 0
                    
                    # è®¡ç®—è½¨è¿¹è´¨é‡åˆ†æ•°
                    quality_score = self.compute_trajectory_quality(region)
                    
                    trajectories[region.id] = {
                        'trajectory': trajectory_array,
                        'length': len(region.trajectory),
                        'mean_intensity': float(getattr(region, 'intensity', 0.0)),
                        'area': float(getattr(region, 'area', 0.0)),
                        'total_distance': float(total_distance),
                        'avg_velocity': float(avg_velocity),
                        'inactive_frames': getattr(region, 'inactive_frames', 0),
                        'quality_score': float(quality_score)
                    }
                except Exception as e:
                    self.logger.warning(f"æå–è½¨è¿¹{region.id}å¤±è´¥: {e}")
                    continue
        
        return trajectories
    
    def compute_trajectory_quality(self, region: Region) -> float:
        """è®¡ç®—è½¨è¿¹è´¨é‡åˆ†æ•°"""
        try:
            trajectory = np.array(region.trajectory)
            
            if len(trajectory) < 2:
                return 0.0
            
            # é•¿åº¦åˆ†æ•°
            length_score = min(1.0, len(trajectory) / 50.0)
            
            # è¿žç»­æ€§åˆ†æ•°
            continuity_score = max(0.0, 1.0 - region.inactive_frames / region.max_inactive_frames)
            
            # è¿åŠ¨å¹³æ»‘æ€§åˆ†æ•°
            if len(trajectory) >= 3:
                velocities = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
                if len(velocities) > 1:
                    velocity_var = np.var(velocities)
                    smoothness_score = max(0.0, 1.0 - velocity_var / 50.0)
                else:
                    smoothness_score = 0.8
            else:
                smoothness_score = 0.5
            
            # å¼ºåº¦ä¸€è‡´æ€§åˆ†æ•°
            intensity_score = min(1.0, getattr(region, 'intensity', 0) / 0.5)
            
            # ç»¼åˆè´¨é‡åˆ†æ•°
            quality_score = (length_score * 0.3 + 
                           continuity_score * 0.3 + 
                           smoothness_score * 0.25 + 
                           intensity_score * 0.15)
            
            return quality_score
            
        except Exception as e:
            self.logger.warning(f"è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def calculate_performance_metrics(self, trajectories: Dict, total_time: float) -> Dict:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        try:
            # åŸºæœ¬æŒ‡æ ‡
            metrics['trajectory_count'] = len(trajectories)
            metrics['computation_time'] = total_time
            
            if trajectories:
                lengths = [traj['length'] for traj in trajectories.values()]
                qualities = [traj['quality_score'] for traj in trajectories.values()]
                
                metrics['average_trajectory_length'] = np.mean(lengths)
                metrics['max_trajectory_length'] = np.max(lengths)
                metrics['trajectory_quality'] = np.mean(qualities)
                
                # è¿žç»­æ€§æŒ‡æ ‡
                metrics['tracking_continuity'] = self._calculate_continuity(trajectories)
                metrics['trajectory_smoothness'] = self._calculate_smoothness(trajectories)
            else:
                metrics['average_trajectory_length'] = 0
                metrics['max_trajectory_length'] = 0
                metrics['trajectory_quality'] = 0
                metrics['tracking_continuity'] = 0
                metrics['trajectory_smoothness'] = 0
            
            # æ£€æµ‹ç¨³å®šæ€§
            if self.performance_stats['computation_times']:
                avg_time = np.mean(self.performance_stats['computation_times'])
                time_std = np.std(self.performance_stats['computation_times'])
                metrics['detection_stability'] = 1.0 / (1.0 + time_std / avg_time) if avg_time > 0 else 0
            else:
                metrics['detection_stability'] = 0
            
            # å†…å­˜ä½¿ç”¨ï¼ˆç®€åŒ–ç‰ˆï¼‰
            try:
                import psutil
                metrics['memory_usage'] = psutil.Process().memory_info().rss / 1024 / 1024
            except ImportError:
                metrics['memory_usage'] = 0
            
        except Exception as e:
            self.logger.error(f"æ€§èƒ½æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            for metric in self.config.EVALUATION_METRICS:
                metrics[metric] = 0.0
        
        return metrics
    
    def _calculate_continuity(self, trajectories: Dict) -> float:
        """è®¡ç®—è·Ÿè¸ªè¿žç»­æ€§"""
        if not trajectories:
            return 0.0
        
        continuity_scores = []
        for traj_data in trajectories.values():
            trajectory = traj_data['trajectory']
            if len(trajectory) > 2:
                velocities = np.linalg.norm(np.diff(trajectory, axis=0), axis=1)
                if len(velocities) > 1:
                    velocity_stability = 1.0 / (1.0 + np.std(velocities))
                    continuity_scores.append(velocity_stability)
        
        return np.mean(continuity_scores) if continuity_scores else 0.0
    
    def _calculate_smoothness(self, trajectories: Dict) -> float:
        """è®¡ç®—è½¨è¿¹å¹³æ»‘åº¦"""
        if not trajectories:
            return 0.0
        
        smoothness_scores = []
        for traj_data in trajectories.values():
            trajectory = traj_data['trajectory']
            if len(trajectory) > 3:
                velocities = np.diff(trajectory, axis=0)
                accelerations = np.diff(velocities, axis=0)
                if len(accelerations) > 0:
                    acceleration_magnitude = np.linalg.norm(accelerations, axis=1)
                    if len(acceleration_magnitude) > 1:
                        smoothness = 1.0 / (1.0 + np.std(acceleration_magnitude))
                        smoothness_scores.append(smoothness)
        
        return np.mean(smoothness_scores) if smoothness_scores else 0.0
    
    def get_algorithm_info(self) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        return {
            'name': self.algorithm_name,
            'description': getattr(self, 'description', 'åŸºç¡€è·Ÿè¸ªç®—æ³•'),
            'parameters': getattr(self, 'algorithm_params', {}),
            'performance_stats': self.performance_stats
        }

# ========== trackers/greedy_tracker.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/greedy_tracker.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/greedy_tracker.py

import numpy as np
from scipy.spatial.distance import cdist
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class GreedyTracker(BaseTracker):
    """è´ªå©ªåŒ¹é…è·Ÿè¸ªç®—æ³•"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "greedy"
        self.description = "è´ªå©ªåŒ¹é…ç®—æ³• - å¿«é€Ÿå±€éƒ¨æœ€ä¼˜è§£"
        
        # èŽ·å–ç®—æ³•ç‰¹å®šé…ç½®
        alg_config = config.get_algorithm_config('greedy')
        self.distance_threshold = alg_config.get('distance_threshold', 25.0)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 25)
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """è´ªå©ªåŒ¹é…ç®—æ³•å®žçŽ°"""
        if not current_regions:
            return []
        
        # èŽ·å–æ´»è·ƒåŒºåŸŸ
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # ä½¿ç”¨é…ç½®çš„è·ç¦»é˜ˆå€¼
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            # èŽ·å–å½“å‰åŒºåŸŸä¸­å¿ƒç‚¹å’Œå·²è·Ÿè¸ªåŒºåŸŸçš„æœ€æ–°ä½ç½®
            current_centers = np.array([r['center'] for r in current_regions])
            tracked_centers = np.array([r.trajectory[-1] for r in active_regions])
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            distances = cdist(tracked_centers, current_centers)
            
            # è´ªå©ªåŒ¹é…ï¼šæŒ‰è·ç¦»ä»Žå°åˆ°å¤§è¿›è¡ŒåŒ¹é…
            matches = []
            used_current = set()
            used_tracked = set()
            
            # èŽ·å–æ‰€æœ‰è·ç¦»çš„æŽ’åºç´¢å¼•
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for tracked_idx, current_idx in zip(dist_indices[0], dist_indices[1]):
                # è·³è¿‡å·²ä½¿ç”¨çš„åŒºåŸŸ
                if tracked_idx in used_tracked or current_idx in used_current:
                    continue
                
                # æ£€æŸ¥è·ç¦»æ˜¯å¦åœ¨é˜ˆå€¼å†…
                if distances[tracked_idx, current_idx] < distance_threshold:
                    matches.append((tracked_idx, current_idx))
                    used_tracked.add(tracked_idx)
                    used_current.add(current_idx)
                else:
                    # ç”±äºŽæŒ‰è·ç¦»æŽ’åºï¼ŒåŽç»­è·ç¦»åªä¼šæ›´å¤§ï¼Œå¯ä»¥æå‰ç»“æŸ
                    break
            
            # å¦‚æžœå¯ç”¨é‡è¿žï¼Œå°è¯•ä¸ºæœªåŒ¹é…çš„åŒºåŸŸè¿›è¡Œé‡è¿ž
            if self.enable_reconnection and len(matches) < len(active_regions):
                reconnection_matches = self._attempt_reconnection(
                    current_regions, active_regions, used_current, used_tracked, frame_idx
                )
                matches.extend(reconnection_matches)
            
            self.logger.debug(f"è´ªå©ªç®—æ³•ç¬¬{frame_idx}å¸§: åŒ¹é…{len(matches)}å¯¹åŒºåŸŸ")
            
            return matches
            
        except Exception as e:
            self.logger.error(f"è´ªå©ªåŒ¹é…ç®—æ³•å¤±è´¥: {e}")
            return []
    
    def _attempt_reconnection(self, current_regions: List[Dict], 
                            active_regions: List, 
                            used_current: set, used_tracked: set, 
                            frame_idx: int) -> List[Tuple[int, int]]:
        """å°è¯•é‡æ–°è¿žæŽ¥æ–­å¼€çš„è½¨è¿¹"""
        reconnection_matches = []
        
        try:
            # æ‰©å¤§æœç´¢è·ç¦»è¿›è¡Œé‡è¿ž
            reconnection_threshold = self.distance_threshold * 1.5
            
            # èŽ·å–æœªåŒ¹é…çš„åŒºåŸŸ
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in used_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in used_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # è®¡ç®—æœªåŒ¹é…åŒºåŸŸé—´çš„è·ç¦»
            unmatched_tracked_centers = np.array([active_regions[i].trajectory[-1] for i in unmatched_tracked])
            unmatched_current_centers = np.array([current_regions[i]['center'] for i in unmatched_current])
            
            distances = cdist(unmatched_tracked_centers, unmatched_current_centers)
            
            # è´ªå©ªé‡è¿žåŒ¹é…
            reconnection_used_current = set()
            reconnection_used_tracked = set()
            
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for rel_tracked_idx, rel_current_idx in zip(dist_indices[0], dist_indices[1]):
                if rel_tracked_idx in reconnection_used_tracked or rel_current_idx in reconnection_used_current:
                    continue
                
                if distances[rel_tracked_idx, rel_current_idx] < reconnection_threshold:
                    # è½¬æ¢å›žåŽŸå§‹ç´¢å¼•
                    original_tracked_idx = unmatched_tracked[rel_tracked_idx]
                    original_current_idx = unmatched_current[rel_current_idx]
                    
                    # æ£€æŸ¥éžæ´»è·ƒå¸§æ•°ï¼Œå¦‚æžœå¤ªå¤šåˆ™ä¸é‡è¿ž
                    region = active_regions[original_tracked_idx]
                    if region.inactive_frames < self.max_inactive_frames:
                        reconnection_matches.append((original_tracked_idx, original_current_idx))
                        reconnection_used_tracked.add(rel_tracked_idx)
                        reconnection_used_current.add(rel_current_idx)
                        
                        self.logger.debug(f"é‡è¿žè½¨è¿¹{region.id}: è·ç¦»{distances[rel_tracked_idx, rel_current_idx]:.2f}")
            
        except Exception as e:
            self.logger.warning(f"é‡è¿žå°è¯•å¤±è´¥: {e}")
        
        return reconnection_matches
    
    def compute_match_quality(self, tracked_region, current_region, distance: float) -> float:
        """è®¡ç®—åŒ¹é…è´¨é‡åˆ†æ•°"""
        try:
            # è·ç¦»åˆ†æ•°ï¼ˆè·ç¦»è¶Šå°åˆ†æ•°è¶Šé«˜ï¼‰
            distance_score = max(0, 1.0 - distance / self.distance_threshold)
            
            # å¼ºåº¦ç›¸ä¼¼æ€§åˆ†æ•°
            intensity_diff = abs(tracked_region.intensity - current_region.get('intensity', 0))
            max_intensity = max(tracked_region.intensity, current_region.get('intensity', 0), 0.1)
            intensity_score = 1.0 - min(1.0, intensity_diff / max_intensity)
            
            # é¢ç§¯ç›¸ä¼¼æ€§åˆ†æ•°
            area_diff = abs(tracked_region.area - current_region.get('area', 0))
            max_area = max(tracked_region.area, current_region.get('area', 0), 1.0)
            area_score = 1.0 - min(1.0, area_diff / max_area)
            
            # ç»¼åˆåˆ†æ•°
            quality_score = (distance_score * 0.6 + 
                           intensity_score * 0.25 + 
                           area_score * 0.15)
            
            return quality_score
            
        except Exception as e:
            self.logger.warning(f"åŒ¹é…è´¨é‡è®¡ç®—å¤±è´¥: {e}")
            return distance_score if 'distance_score' in locals() else 0.0
    
    def get_algorithm_info(self) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "å¿«é€Ÿè®¡ç®—",
                "å±€éƒ¨æœ€ä¼˜è§£",
                "è´ªå©ªç­–ç•¥",
                "æ”¯æŒè½¨è¿¹é‡è¿ž"
            ],
            'advantages': [
                "è®¡ç®—æ•ˆçŽ‡é«˜",
                "å®žçŽ°ç®€å•",
                "å†…å­˜æ¶ˆè€—ä½Ž",
                "é€‚åˆå®žæ—¶å¤„ç†"
            ],
            'disadvantages': [
                "å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜",
                "å¯¹å™ªå£°æ•æ„Ÿ",
                "åŒ¹é…è´¨é‡ä¸å¦‚å…¨å±€ç®—æ³•"
            ],
            'best_for': [
                "å®žæ—¶å¤„ç†åœºæ™¯",
                "è®¡ç®—èµ„æºæœ‰é™åœºæ™¯",
                "è½¨è¿¹æ•°é‡è¾ƒå°‘åœºæ™¯"
            ]
        })
        return info

# ========== trackers/hungarian_tracker.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/hungarian_tracker.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/hungarian_tracker.py

import numpy as np
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class HungarianTracker(BaseTracker):
    """åŒˆç‰™åˆ©ç®—æ³•è·Ÿè¸ªå™¨ - å…¨å±€æœ€ä¼˜åŒ¹é…"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "hungarian"
        self.description = "åŒˆç‰™åˆ©ç®—æ³• - å…¨å±€æœ€ä¼˜åŒ¹é…è§£"
        
        # èŽ·å–ç®—æ³•ç‰¹å®šé…ç½®
        alg_config = config.get_algorithm_config('hungarian')
        self.distance_threshold = alg_config.get('distance_threshold', 25.0)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 25)
        
        # åŒˆç‰™åˆ©ç®—æ³•ç‰¹å®šå‚æ•°
        self.cost_threshold = self.distance_threshold * 2  # æˆæœ¬é˜ˆå€¼
        self.use_weighted_cost = True  # æ˜¯å¦ä½¿ç”¨åŠ æƒæˆæœ¬
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'cost_threshold': self.cost_threshold,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames,
            'use_weighted_cost': self.use_weighted_cost
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å®žçŽ°"""
        if not current_regions:
            return []
        
        # èŽ·å–æ´»è·ƒåŒºåŸŸ
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # ä½¿ç”¨é…ç½®çš„è·ç¦»é˜ˆå€¼
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            # æž„å»ºæˆæœ¬çŸ©é˜µ
            cost_matrix = self._build_cost_matrix(current_regions, active_regions)
            
            if cost_matrix.size == 0:
                return []
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ±‚è§£æœ€ä¼˜åˆ†é…
            try:
                row_indices, col_indices = linear_sum_assignment(cost_matrix)
            except Exception as e:
                self.logger.warning(f"åŒˆç‰™åˆ©ç®—æ³•æ±‚è§£å¤±è´¥: {e}, ä½¿ç”¨å¤‡ç”¨è´ªå©ªç®—æ³•")
                return self._fallback_greedy_matching(current_regions, active_regions, distance_threshold)
            
            # ç­›é€‰æœ‰æ•ˆåŒ¹é…
            matches = []
            for row_idx, col_idx in zip(row_indices, col_indices):
                cost = cost_matrix[row_idx, col_idx]
                
                # åªæŽ¥å—æˆæœ¬ä½ŽäºŽé˜ˆå€¼çš„åŒ¹é…
                if cost < self.cost_threshold:
                    matches.append((row_idx, col_idx))
                    self.logger.debug(f"åŒ¹é…è½¨è¿¹{active_regions[row_idx].id}ä¸ŽåŒºåŸŸ{col_idx}, æˆæœ¬: {cost:.2f}")
            
            # å¦‚æžœå¯ç”¨é‡è¿žï¼Œå°è¯•é‡è¿žæœªåŒ¹é…çš„åŒºåŸŸ
            if self.enable_reconnection and len(matches) < len(active_regions):
                reconnection_matches = self._attempt_hungarian_reconnection(
                    current_regions, active_regions, matches, frame_idx
                )
                matches.extend(reconnection_matches)
            
            self.logger.debug(f"åŒˆç‰™åˆ©ç®—æ³•ç¬¬{frame_idx}å¸§: åŒ¹é…{len(matches)}å¯¹åŒºåŸŸ")
            
            return matches
            
        except Exception as e:
            self.logger.error(f"åŒˆç‰™åˆ©ç®—æ³•åŒ¹é…å¤±è´¥: {e}")
            return self._fallback_greedy_matching(current_regions, active_regions, distance_threshold)
    
    def _build_cost_matrix(self, current_regions: List[Dict], active_regions: List) -> np.ndarray:
        """æž„å»ºæˆæœ¬çŸ©é˜µ"""
        try:
            n_tracked = len(active_regions)
            n_current = len(current_regions)
            
            # åˆå§‹åŒ–æˆæœ¬çŸ©é˜µ
            cost_matrix = np.full((n_tracked, n_current), np.inf)
            
            # è®¡ç®—å„ç§æˆæœ¬ç»„ä»¶
            for i, tracked_region in enumerate(active_regions):
                tracked_center = np.array(tracked_region.trajectory[-1])
                
                for j, current_region in enumerate(current_regions):
                    current_center = np.array(current_region['center'])
                    
                    # è®¡ç®—ç»¼åˆæˆæœ¬
                    cost = self._calculate_assignment_cost(tracked_region, current_region, 
                                                         tracked_center, current_center)
                    cost_matrix[i, j] = cost
            
            return cost_matrix
            
        except Exception as e:
            self.logger.error(f"æˆæœ¬çŸ©é˜µæž„å»ºå¤±è´¥: {e}")
            return np.array([])
    
    def _calculate_assignment_cost(self, tracked_region, current_region, 
                                 tracked_center: np.ndarray, current_center: np.ndarray) -> float:
        """è®¡ç®—åˆ†é…æˆæœ¬"""
        try:
            # 1. åŸºç¡€è·ç¦»æˆæœ¬
            distance = np.linalg.norm(tracked_center - current_center)
            distance_cost = distance
            
            # å¦‚æžœè·ç¦»è¶…è¿‡é˜ˆå€¼ï¼Œè¿”å›žæžé«˜æˆæœ¬
            if distance > self.distance_threshold:
                return np.inf
            
            if not self.use_weighted_cost:
                return distance_cost
            
            # 2. å¼ºåº¦å·®å¼‚æˆæœ¬
            intensity_diff = abs(tracked_region.intensity - current_region.get('intensity', 0))
            max_intensity = max(tracked_region.intensity, current_region.get('intensity', 0), 0.1)
            intensity_cost = intensity_diff / max_intensity * 10  # æƒé‡ä¸º10
            
            # 3. é¢ç§¯å·®å¼‚æˆæœ¬
            area_diff = abs(tracked_region.area - current_region.get('area', 0))
            max_area = max(tracked_region.area, current_region.get('area', 0), 1.0)
            area_cost = area_diff / max_area * 5  # æƒé‡ä¸º5
            
            # 4. é€Ÿåº¦ä¸€è‡´æ€§æˆæœ¬
            velocity_cost = 0
            if len(tracked_region.trajectory) >= 2:
                prev_velocity = np.array(tracked_region.trajectory[-1]) - np.array(tracked_region.trajectory[-2])
                current_velocity = current_center - tracked_center
                velocity_diff = np.linalg.norm(current_velocity - prev_velocity)
                velocity_cost = velocity_diff * 2  # æƒé‡ä¸º2
            
            # 5. éžæ´»è·ƒå¸§æ•°æƒ©ç½š
            inactive_penalty = tracked_region.inactive_frames * 3  # æƒé‡ä¸º3
            
            # ç»¼åˆæˆæœ¬
            total_cost = (distance_cost + intensity_cost + area_cost + 
                         velocity_cost + inactive_penalty)
            
            return total_cost
            
        except Exception as e:
            self.logger.warning(f"æˆæœ¬è®¡ç®—å¤±è´¥: {e}")
            return distance if 'distance' in locals() else np.inf
    
    def _attempt_hungarian_reconnection(self, current_regions: List[Dict], 
                                      active_regions: List,
                                      existing_matches: List[Tuple[int, int]], 
                                      frame_idx: int) -> List[Tuple[int, int]]:
        """ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•å°è¯•é‡è¿ž"""
        reconnection_matches = []
        
        try:
            # èŽ·å–æœªåŒ¹é…çš„åŒºåŸŸç´¢å¼•
            matched_tracked = set([m[0] for m in existing_matches])
            matched_current = set([m[1] for m in existing_matches])
            
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in matched_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in matched_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # æž„å»ºé‡è¿žæˆæœ¬çŸ©é˜µï¼ˆä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼ï¼‰
            reconnection_threshold = self.distance_threshold * 2.0
            cost_matrix = np.full((len(unmatched_tracked), len(unmatched_current)), np.inf)
            
            for i, tracked_idx in enumerate(unmatched_tracked):
                tracked_region = active_regions[tracked_idx]
                
                # åªä¸ºéžæ´»è·ƒå¸§æ•°ä¸å¤ªå¤šçš„åŒºåŸŸå°è¯•é‡è¿ž
                if tracked_region.inactive_frames >= self.max_inactive_frames:
                    continue
                
                tracked_center = np.array(tracked_region.trajectory[-1])
                
                for j, current_idx in enumerate(unmatched_current):
                    current_region = current_regions[current_idx]
                    current_center = np.array(current_region['center'])
                    
                    distance = np.linalg.norm(tracked_center - current_center)
                    
                    if distance < reconnection_threshold:
                        # é‡è¿žæ—¶çš„æˆæœ¬åŒ…å«éžæ´»è·ƒæƒ©ç½š
                        reconnection_cost = distance + tracked_region.inactive_frames * 5
                        cost_matrix[i, j] = reconnection_cost
            
            # å¦‚æžœæœ‰å¯é‡è¿žçš„ç»„åˆï¼Œä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•
            if np.any(cost_matrix < np.inf):
                try:
                    row_indices, col_indices = linear_sum_assignment(cost_matrix)
                    
                    for row_idx, col_idx in zip(row_indices, col_indices):
                        if cost_matrix[row_idx, col_idx] < np.inf:
                            original_tracked_idx = unmatched_tracked[row_idx]
                            original_current_idx = unmatched_current[col_idx]
                            
                            reconnection_matches.append((original_tracked_idx, original_current_idx))
                            
                            region = active_regions[original_tracked_idx]
                            self.logger.debug(f"é‡è¿žè½¨è¿¹{region.id}: æˆæœ¬{cost_matrix[row_idx, col_idx]:.2f}")
                            
                except Exception as e:
                    self.logger.warning(f"é‡è¿žåŒˆç‰™åˆ©ç®—æ³•å¤±è´¥: {e}")
            
        except Exception as e:
            self.logger.warning(f"é‡è¿žå°è¯•å¤±è´¥: {e}")
        
        return reconnection_matches
    
    def _fallback_greedy_matching(self, current_regions: List[Dict], 
                                active_regions: List, 
                                distance_threshold: float) -> List[Tuple[int, int]]:
        """å¤‡ç”¨è´ªå©ªåŒ¹é…ç®—æ³•"""
        try:
            current_centers = np.array([r['center'] for r in current_regions])
            tracked_centers = np.array([r.trajectory[-1] for r in active_regions])
            
            distances = cdist(tracked_centers, current_centers)
            
            matches = []
            used_current = set()
            used_tracked = set()
            
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for tracked_idx, current_idx in zip(dist_indices[0], dist_indices[1]):
                if tracked_idx in used_tracked or current_idx in used_current:
                    continue
                
                if distances[tracked_idx, current_idx] < distance_threshold:
                    matches.append((tracked_idx, current_idx))
                    used_tracked.add(tracked_idx)
                    used_current.add(current_idx)
            
            return matches
            
        except Exception as e:
            self.logger.error(f"å¤‡ç”¨è´ªå©ªç®—æ³•å¤±è´¥: {e}")
            return []
    
    def get_algorithm_info(self) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "å…¨å±€æœ€ä¼˜è§£",
                "æˆæœ¬çŸ©é˜µä¼˜åŒ–",
                "å¤šå› ç´ æƒè¡¡",
                "é«˜ç²¾åº¦åŒ¹é…"
            ],
            'advantages': [
                "å…¨å±€æœ€ä¼˜åŒ¹é…",
                "è€ƒè™‘å¤šç§ç‰¹å¾",
                "åŒ¹é…è´¨é‡é«˜",
                "æ•°å­¦åŸºç¡€æ‰Žå®ž"
            ],
            'disadvantages': [
                "è®¡ç®—å¤æ‚åº¦è¾ƒé«˜",
                "å†…å­˜æ¶ˆè€—è¾ƒå¤§",
                "å‚æ•°è°ƒä¼˜å¤æ‚"
            ],
            'best_for': [
                "é«˜ç²¾åº¦è¦æ±‚åœºæ™¯",
                "è½¨è¿¹è´¨é‡ä¼˜å…ˆåœºæ™¯",
                "å¤æ‚åŒ¹é…é—®é¢˜",
                "ç¦»çº¿åˆ†æžåœºæ™¯"
            ]
        })
        return info

# ========== trackers/hybrid_tracker.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/hybrid_tracker.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/hybrid_tracker.py

import numpy as np
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class HybridTracker(BaseTracker):
    """æ··åˆè·Ÿè¸ªå™¨ - ç»¼åˆå¤šç§ç®—æ³•ä¼˜åŠ¿"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "hybrid"
        self.description = "æ··åˆç®—æ³• - ç»¼åˆå¤šç§ç‰¹å¾"
        
        # èŽ·å–ç®—æ³•ç‰¹å®šé…ç½®
        alg_config = config.get_algorithm_config('hybrid')
        self.distance_threshold = alg_config.get('distance_threshold', 25.0)
        self.overlap_weight = alg_config.get('overlap_weight', 0.4)
        self.intensity_weight = alg_config.get('intensity_weight', 0.1)
        self.area_weight = alg_config.get('area_weight', 0.1)
        self.enable_prediction = alg_config.get('enable_prediction', True)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 30)
        
        # é¢„æµ‹ç›¸å…³å‚æ•°
        self.prediction_weight = 0.3
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'overlap_weight': self.overlap_weight,
            'intensity_weight': self.intensity_weight,
            'area_weight': self.area_weight,
            'enable_prediction': self.enable_prediction,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """æ··åˆåŒ¹é…ç®—æ³•"""
        if not current_regions:
            return []
        
        # èŽ·å–æ´»è·ƒåŒºåŸŸ
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # ä½¿ç”¨é…ç½®çš„è·ç¦»é˜ˆå€¼
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            # æž„å»ºç»¼åˆå¾—åˆ†çŸ©é˜µ
            score_matrix = self._build_hybrid_score_matrix(current_regions, active_regions)
            
            if score_matrix.size == 0:
                return []
            
            # è½¬æ¢ä¸ºæˆæœ¬çŸ©é˜µç”¨äºŽåŒˆç‰™åˆ©ç®—æ³•
            cost_matrix = 1.0 - score_matrix
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ±‚è§£æœ€ä¼˜åˆ†é…
            try:
                row_indices, col_indices = linear_sum_assignment(cost_matrix)
                matches = []
                
                for row_idx, col_idx in zip(row_indices, col_indices):
                    score = score_matrix[row_idx, col_idx]
                    
                    # åªæŽ¥å—å¾—åˆ†é«˜äºŽé˜ˆå€¼çš„åŒ¹é…
                    if score > 0.3:  # å¯è°ƒé˜ˆå€¼
                        matches.append((row_idx, col_idx))
                        self.logger.debug(f"æ··åˆåŒ¹é…è½¨è¿¹{active_regions[row_idx].id}: "
                                        f"ç»¼åˆå¾—åˆ†{score:.3f}")
                
                # å¦‚æžœå¯ç”¨é‡è¿žï¼Œå°è¯•é‡è¿žæœªåŒ¹é…çš„åŒºåŸŸ
                if self.enable_reconnection and len(matches) < len(active_regions):
                    reconnection_matches = self._attempt_hybrid_reconnection(
                        current_regions, active_regions, matches, frame_idx
                    )
                    matches.extend(reconnection_matches)
                
                self.logger.debug(f"æ··åˆç®—æ³•ç¬¬{frame_idx}å¸§: åŒ¹é…{len(matches)}å¯¹åŒºåŸŸ")
                return matches
                
            except Exception as e:
                self.logger.warning(f"æ··åˆåŒˆç‰™åˆ©åˆ†é…å¤±è´¥: {e}, ä½¿ç”¨å¤‡ç”¨ç®—æ³•")
                return self._fallback_hybrid_matching(current_regions, active_regions, distance_threshold)
            
        except Exception as e:
            self.logger.error(f"æ··åˆåŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _build_hybrid_score_matrix(self, current_regions: List[Dict], active_regions: List) -> np.ndarray:
        """æž„å»ºæ··åˆå¾—åˆ†çŸ©é˜µ"""
        try:
            n_tracked = len(active_regions)
            n_current = len(current_regions)
            
            score_matrix = np.zeros((n_tracked, n_current))
            
            for i, tracked_region in enumerate(active_regions):
                for j, current_region in enumerate(current_regions):
                    score = self._calculate_hybrid_score(tracked_region, current_region)
                    score_matrix[i, j] = score
            
            return score_matrix
            
        except Exception as e:
            self.logger.error(f"æ··åˆå¾—åˆ†çŸ©é˜µæž„å»ºå¤±è´¥: {e}")
            return np.array([])
    
    def _calculate_hybrid_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—æ··åˆå¾—åˆ†"""
        try:
            # 1. è·ç¦»å¾—åˆ†
            distance_score = self._calculate_distance_score(tracked_region, current_region)
            
            # 2. é¢„æµ‹å¾—åˆ†ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            prediction_score = 0.0
            if self.enable_prediction:
                prediction_score = self._calculate_prediction_score(tracked_region, current_region)
            
            # 3. å¼ºåº¦ç›¸ä¼¼æ€§å¾—åˆ†
            intensity_score = self._calculate_intensity_score(tracked_region, current_region)
            
            # 4. é¢ç§¯ç›¸ä¼¼æ€§å¾—åˆ†
            area_score = self._calculate_area_score(tracked_region, current_region)
            
            # 5. é‡å åº¦å¾—åˆ†
            overlap_score = self._calculate_overlap_score(tracked_region, current_region)
            
            # 6. è½¨è¿¹è´¨é‡å¾—åˆ†
            quality_score = self._calculate_quality_score(tracked_region)
            
            # 7. é€Ÿåº¦ä¸€è‡´æ€§å¾—åˆ†
            velocity_score = self._calculate_velocity_score(tracked_region, current_region)
            
            # ç»¼åˆå¾—åˆ†ï¼ˆæƒé‡å¯è°ƒï¼‰
            total_score = (distance_score * 0.25 + 
                         prediction_score * 0.15 + 
                         intensity_score * self.intensity_weight + 
                         area_score * self.area_weight + 
                         overlap_score * self.overlap_weight + 
                         quality_score * 0.1 + 
                         velocity_score * 0.1)
            
            return min(1.0, total_score)
            
        except Exception as e:
            self.logger.warning(f"æ··åˆå¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_distance_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—è·ç¦»å¾—åˆ†"""
        try:
            tracked_center = np.array(tracked_region.trajectory[-1])
            current_center = np.array(current_region['center'])
            
            distance = np.linalg.norm(tracked_center - current_center)
            
            if distance > self.distance_threshold:
                return 0.0
            
            return 1.0 - (distance / self.distance_threshold)
            
        except Exception as e:
            self.logger.warning(f"è·ç¦»å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_prediction_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—é¢„æµ‹å¾—åˆ†"""
        try:
            if len(tracked_region.trajectory) < 2:
                return 0.0
            
            # ç®€å•çº¿æ€§é¢„æµ‹
            if len(tracked_region.trajectory) == 2:
                velocity = np.array(tracked_region.trajectory[-1]) - np.array(tracked_region.trajectory[-2])
                predicted = np.array(tracked_region.trajectory[-1]) + velocity * self.prediction_weight
            else:
                # è€ƒè™‘åŠ é€Ÿåº¦çš„é¢„æµ‹
                current_pos = np.array(tracked_region.trajectory[-1])
                prev_pos = np.array(tracked_region.trajectory[-2])
                prev_prev_pos = np.array(tracked_region.trajectory[-3])
                
                velocity = current_pos - prev_pos
                acceleration = velocity - (prev_pos - prev_prev_pos)
                
                predicted = current_pos + velocity * self.prediction_weight + acceleration * 0.5 * (self.prediction_weight ** 2)
            
            current_center = np.array(current_region['center'])
            prediction_error = np.linalg.norm(predicted - current_center)
            
            # é¢„æµ‹è¯¯å·®è¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜
            max_error = self.distance_threshold
            if prediction_error > max_error:
                return 0.0
            
            return 1.0 - (prediction_error / max_error)
            
        except Exception as e:
            self.logger.warning(f"é¢„æµ‹å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_intensity_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—å¼ºåº¦å¾—åˆ†"""
        try:
            tracked_intensity = getattr(tracked_region, 'intensity', 0)
            current_intensity = current_region.get('intensity', 0)
            
            if tracked_intensity == 0 and current_intensity == 0:
                return 1.0
            
            max_intensity = max(tracked_intensity, current_intensity, 0.1)
            intensity_diff = abs(tracked_intensity - current_intensity)
            
            return 1.0 - min(1.0, intensity_diff / max_intensity)
            
        except Exception as e:
            self.logger.warning(f"å¼ºåº¦å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_area_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—é¢ç§¯å¾—åˆ†"""
        try:
            tracked_area = getattr(tracked_region, 'area', 0)
            current_area = current_region.get('area', 0)
            
            if tracked_area == 0 and current_area == 0:
                return 1.0
            
            max_area = max(tracked_area, current_area, 1.0)
            area_diff = abs(tracked_area - current_area)
            
            return 1.0 - min(1.0, area_diff / max_area)
            
        except Exception as e:
            self.logger.warning(f"é¢ç§¯å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_overlap_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—é‡å åº¦å¾—åˆ†"""
        try:
            tracked_center = np.array(tracked_region.trajectory[-1])
            current_center = np.array(current_region['center'])
            distance = np.linalg.norm(tracked_center - current_center)
            
            tracked_area = getattr(tracked_region, 'area', 0)
            current_area = current_region.get('area', 0)
            
            if tracked_area == 0 or current_area == 0:
                return 0.0
            
            # ä¼°ç®—é‡å ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            estimated_radius_tracked = np.sqrt(tracked_area / np.pi)
            estimated_radius_current = np.sqrt(current_area / np.pi)
            
            radius_sum = estimated_radius_tracked + estimated_radius_current
            
            if distance >= radius_sum:
                return 0.0
            elif distance <= abs(estimated_radius_tracked - estimated_radius_current):
                return 1.0
            else:
                return 1.0 - (distance / radius_sum)
                
        except Exception as e:
            self.logger.warning(f"é‡å å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_quality_score(self, tracked_region) -> float:
        """è®¡ç®—è½¨è¿¹è´¨é‡å¾—åˆ†"""
        try:
            # åŸºäºŽè½¨è¿¹é•¿åº¦å’Œç¨³å®šæ€§
            length_score = min(1.0, len(tracked_region.trajectory) / 20.0)
            stability_score = max(0.0, 1.0 - tracked_region.inactive_frames / self.max_inactive_frames)
            
            return (length_score * 0.6 + stability_score * 0.4)
            
        except Exception as e:
            self.logger.warning(f"è´¨é‡å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _calculate_velocity_score(self, tracked_region, current_region) -> float:
        """è®¡ç®—é€Ÿåº¦ä¸€è‡´æ€§å¾—åˆ†"""
        try:
            if len(tracked_region.trajectory) < 2:
                return 0.5
            
            # è®¡ç®—åŽ†å²é€Ÿåº¦
            prev_velocity = np.array(tracked_region.trajectory[-1]) - np.array(tracked_region.trajectory[-2])
            
            # è®¡ç®—å½“å‰é€Ÿåº¦
            tracked_center = np.array(tracked_region.trajectory[-1])
            current_center = np.array(current_region['center'])
            current_velocity = current_center - tracked_center
            
            # é€Ÿåº¦å·®å¼‚
            velocity_diff = np.linalg.norm(current_velocity - prev_velocity)
            
            # é€Ÿåº¦å·®å¼‚è¶Šå°ï¼Œå¾—åˆ†è¶Šé«˜
            max_velocity_diff = 20.0  # å¯è°ƒå‚æ•°
            return max(0.0, 1.0 - velocity_diff / max_velocity_diff)
            
        except Exception as e:
            self.logger.warning(f"é€Ÿåº¦å¾—åˆ†è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _attempt_hybrid_reconnection(self, current_regions: List[Dict], 
                                   active_regions: List,
                                   existing_matches: List[Tuple[int, int]], 
                                   frame_idx: int) -> List[Tuple[int, int]]:
        """æ··åˆé‡è¿žå°è¯•"""
        reconnection_matches = []
        
        try:
            # èŽ·å–æœªåŒ¹é…çš„åŒºåŸŸç´¢å¼•
            matched_tracked = set([m[0] for m in existing_matches])
            matched_current = set([m[1] for m in existing_matches])
            
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in matched_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in matched_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼è¿›è¡Œé‡è¿ž
            for tracked_idx in unmatched_tracked:
                tracked_region = active_regions[tracked_idx]
                
                # åªä¸ºéžæ´»è·ƒå¸§æ•°ä¸å¤ªå¤šçš„åŒºåŸŸå°è¯•é‡è¿ž
                if tracked_region.inactive_frames >= self.max_inactive_frames:
                    continue
                
                best_match = -1
                best_score = 0
                
                for current_idx in unmatched_current:
                    current_region = current_regions[current_idx]
                    
                    # è®¡ç®—é‡è¿žåˆ†æ•°ï¼ˆæ›´å®½æ¾çš„æ¡ä»¶ï¼‰
                    score = self._calculate_hybrid_score(tracked_region, current_region)
                    
                    if score > 0.2 and score > best_score:  # æ›´ä½Žçš„é˜ˆå€¼
                        best_score = score
                        best_match = current_idx
                
                if best_match >= 0:
                    reconnection_matches.append((tracked_idx, best_match))
                    unmatched_current.remove(best_match)
                    
                    self.logger.debug(f"æ··åˆé‡è¿žè½¨è¿¹{tracked_region.id}: å¾—åˆ†{best_score:.3f}")
            
        except Exception as e:
            self.logger.warning(f"æ··åˆé‡è¿žå¤±è´¥: {e}")
        
        return reconnection_matches
    
    def _fallback_hybrid_matching(self, current_regions: List[Dict], 
                                 active_regions: List, 
                                 distance_threshold: float) -> List[Tuple[int, int]]:
        """å¤‡ç”¨æ··åˆåŒ¹é…"""
        try:
            matches = []
            used_current = set()
            used_tracked = set()
            
            # è®¡ç®—æ‰€æœ‰å¾—åˆ†å¯¹
            score_pairs = []
            for i, tracked_region in enumerate(active_regions):
                for j, current_region in enumerate(current_regions):
                    score = self._calculate_hybrid_score(tracked_region, current_region)
                    if score > 0.3:
                        score_pairs.append((score, i, j))
            
            # æŒ‰å¾—åˆ†æŽ’åº
            score_pairs.sort(reverse=True)
            
            # è´ªå©ªé€‰æ‹©
            for score, tracked_idx, current_idx in score_pairs:
                if tracked_idx not in used_tracked and current_idx not in used_current:
                    matches.append((tracked_idx, current_idx))
                    used_tracked.add(tracked_idx)
                    used_current.add(current_idx)
            
            return matches
            
        except Exception as e:
            self.logger.error(f"å¤‡ç”¨æ··åˆç®—æ³•å¤±è´¥: {e}")
            return []
    
    def get_algorithm_info(self) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "å¤šç®—æ³•èžåˆ",
                "ç»¼åˆç‰¹å¾è¯„ä¼°",
                "è‡ªé€‚åº”æƒé‡",
                "å…¨å±€ä¼˜åŒ–"
            ],
            'advantages': [
                "ç»¼åˆå¤šç§ç®—æ³•ä¼˜åŠ¿",
                "é€‚åº”æ€§å¼º",
                "åŒ¹é…ç²¾åº¦é«˜",
                "é²æ£’æ€§å¥½"
            ],
            'disadvantages': [
                "è®¡ç®—å¤æ‚åº¦æœ€é«˜",
                "å‚æ•°ä¼—å¤š",
                "è°ƒä¼˜å›°éš¾",
                "èµ„æºæ¶ˆè€—å¤§"
            ],
            'best_for': [
                "å¤æ‚è·Ÿè¸ªåœºæ™¯",
                "é«˜ç²¾åº¦è¦æ±‚",
                "å¤šå˜çŽ¯å¢ƒ",
                "ç¦»çº¿åˆ†æžåœºæ™¯"
            ]
        })
        return info

# ========== trackers/kalman_tracker.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/kalman_tracker.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/kalman_tracker.py

import numpy as np
from scipy.spatial.distance import cdist
from scipy.optimize import linear_sum_assignment
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class KalmanTracker(BaseTracker):
    """å¡å°”æ›¼é¢„æµ‹è·Ÿè¸ªå™¨"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "kalman"
        self.description = "å¡å°”æ›¼é¢„æµ‹ç®—æ³• - åŸºäºŽè¿åŠ¨é¢„æµ‹"
        
        # èŽ·å–ç®—æ³•ç‰¹å®šé…ç½®
        alg_config = config.get_algorithm_config('kalman')
        self.distance_threshold = alg_config.get('distance_threshold', 30.0)
        self.prediction_weight = alg_config.get('prediction_weight', 0.4)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 30)
        
        self.algorithm_params = {
            'distance_threshold': self.distance_threshold,
            'prediction_weight': self.prediction_weight,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """å¡å°”æ›¼é¢„æµ‹åŒ¹é…ç®—æ³•"""
        if not current_regions:
            return []
        
        # èŽ·å–æ´»è·ƒåŒºåŸŸ
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # ä½¿ç”¨é…ç½®çš„è·ç¦»é˜ˆå€¼
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            # é¢„æµ‹ä¸‹ä¸€å¸§ä½ç½®
            predicted_centers = []
            for region in active_regions:
                predicted_pos = self._predict_next_position(region)
                predicted_centers.append(predicted_pos)
            
            if not predicted_centers:
                return []
            
            predicted_centers = np.array(predicted_centers)
            current_centers = np.array([r['center'] for r in current_regions])
            
            # è®¡ç®—è·ç¦»çŸ©é˜µ
            distances = cdist(predicted_centers, current_centers)
            
            # ä½¿ç”¨åŒˆç‰™åˆ©ç®—æ³•æ±‚è§£æœ€ä¼˜åˆ†é…
            try:
                row_indices, col_indices = linear_sum_assignment(distances)
                matches = []
                
                for row_idx, col_idx in zip(row_indices, col_indices):
                    if distances[row_idx, col_idx] < distance_threshold:
                        matches.append((row_idx, col_idx))
                        self.logger.debug(f"å¡å°”æ›¼åŒ¹é…è½¨è¿¹{active_regions[row_idx].id}: "
                                        f"é¢„æµ‹è·ç¦»{distances[row_idx, col_idx]:.2f}")
                
                # å¦‚æžœå¯ç”¨é‡è¿žï¼Œå°è¯•é‡è¿žæœªåŒ¹é…çš„åŒºåŸŸ
                if self.enable_reconnection and len(matches) < len(active_regions):
                    reconnection_matches = self._attempt_kalman_reconnection(
                        current_regions, active_regions, matches, frame_idx
                    )
                    matches.extend(reconnection_matches)
                
                self.logger.debug(f"å¡å°”æ›¼ç®—æ³•ç¬¬{frame_idx}å¸§: åŒ¹é…{len(matches)}å¯¹åŒºåŸŸ")
                return matches
                
            except Exception as e:
                self.logger.warning(f"å¡å°”æ›¼åŒˆç‰™åˆ©åˆ†é…å¤±è´¥: {e}")
                return self._fallback_greedy_matching(current_regions, active_regions, distance_threshold)
            
        except Exception as e:
            self.logger.error(f"å¡å°”æ›¼åŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _predict_next_position(self, region) -> np.ndarray:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªä½ç½®"""
        try:
            trajectory = region.trajectory
            
            if len(trajectory) < 2:
                # å¦‚æžœè½¨è¿¹ç‚¹ä¸è¶³ï¼Œè¿”å›žå½“å‰ä½ç½®
                return np.array(trajectory[-1])
            
            elif len(trajectory) == 2:
                # ç®€å•çº¿æ€§é¢„æµ‹
                velocity = np.array(trajectory[-1]) - np.array(trajectory[-2])
                predicted = np.array(trajectory[-1]) + velocity * self.prediction_weight
                
            else:
                # ä½¿ç”¨å¤šç‚¹é¢„æµ‹ï¼Œè€ƒè™‘åŠ é€Ÿåº¦
                current_pos = np.array(trajectory[-1])
                prev_pos = np.array(trajectory[-2])
                prev_prev_pos = np.array(trajectory[-3])
                
                # è®¡ç®—é€Ÿåº¦å’ŒåŠ é€Ÿåº¦
                velocity = current_pos - prev_pos
                prev_velocity = prev_pos - prev_prev_pos
                acceleration = velocity - prev_velocity
                
                # é¢„æµ‹ä¸‹ä¸€ä¸ªä½ç½®ï¼ˆè€ƒè™‘é€Ÿåº¦å’ŒåŠ é€Ÿåº¦ï¼‰
                predicted = (current_pos + 
                           velocity * self.prediction_weight + 
                           acceleration * (self.prediction_weight ** 2) * 0.5)
            
            return predicted
            
        except Exception as e:
            self.logger.warning(f"ä½ç½®é¢„æµ‹å¤±è´¥: {e}")
            return np.array(region.trajectory[-1])
    
    def _attempt_kalman_reconnection(self, current_regions: List[Dict], 
                                   active_regions: List,
                                   existing_matches: List[Tuple[int, int]], 
                                   frame_idx: int) -> List[Tuple[int, int]]:
        """å¡å°”æ›¼é‡è¿žå°è¯•"""
        reconnection_matches = []
        
        try:
            # èŽ·å–æœªåŒ¹é…çš„åŒºåŸŸç´¢å¼•
            matched_tracked = set([m[0] for m in existing_matches])
            matched_current = set([m[1] for m in existing_matches])
            
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in matched_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in matched_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼è¿›è¡Œé‡è¿ž
            reconnection_threshold = self.distance_threshold * 1.8
            
            # é¢„æµ‹æœªåŒ¹é…åŒºåŸŸçš„ä½ç½®
            for tracked_idx in unmatched_tracked:
                tracked_region = active_regions[tracked_idx]
                
                # åªä¸ºéžæ´»è·ƒå¸§æ•°ä¸å¤ªå¤šçš„åŒºåŸŸå°è¯•é‡è¿ž
                if tracked_region.inactive_frames >= self.max_inactive_frames:
                    continue
                
                predicted_pos = self._predict_next_position(tracked_region)
                
                best_match = -1
                best_distance = float('inf')
                
                for current_idx in unmatched_current:
                    current_region = current_regions[current_idx]
                    current_pos = np.array(current_region['center'])
                    
                    distance = np.linalg.norm(predicted_pos - current_pos)
                    
                    if distance < reconnection_threshold and distance < best_distance:
                        best_distance = distance
                        best_match = current_idx
                
                if best_match >= 0:
                    reconnection_matches.append((tracked_idx, best_match))
                    unmatched_current.remove(best_match)
                    
                    self.logger.debug(f"å¡å°”æ›¼é‡è¿žè½¨è¿¹{tracked_region.id}: "
                                    f"é¢„æµ‹è·ç¦»{best_distance:.2f}")
            
        except Exception as e:
            self.logger.warning(f"å¡å°”æ›¼é‡è¿žå¤±è´¥: {e}")
        
        return reconnection_matches
    
    def _fallback_greedy_matching(self, current_regions: List[Dict], 
                                active_regions: List, 
                                distance_threshold: float) -> List[Tuple[int, int]]:
        """å¤‡ç”¨è´ªå©ªåŒ¹é…"""
        try:
            current_centers = np.array([r['center'] for r in current_regions])
            tracked_centers = np.array([r.trajectory[-1] for r in active_regions])
            
            distances = cdist(tracked_centers, current_centers)
            
            matches = []
            used_current = set()
            used_tracked = set()
            
            dist_indices = np.unravel_index(np.argsort(distances.ravel()), distances.shape)
            
            for tracked_idx, current_idx in zip(dist_indices[0], dist_indices[1]):
                if tracked_idx in used_tracked or current_idx in used_current:
                    continue
                
                if distances[tracked_idx, current_idx] < distance_threshold:
                    matches.append((tracked_idx, current_idx))
                    used_tracked.add(tracked_idx)
                    used_current.add(current_idx)
            
            return matches
            
        except Exception as e:
            self.logger.error(f"å¤‡ç”¨è´ªå©ªç®—æ³•å¤±è´¥: {e}")
            return []
    
    def update_region_velocity_history(self, region, current_position: np.ndarray):
        """æ›´æ–°åŒºåŸŸçš„é€Ÿåº¦åŽ†å²"""
        try:
            if len(region.trajectory) >= 2:
                velocity = current_position - np.array(region.trajectory[-1])
                
                # ç»´æŠ¤é€Ÿåº¦åŽ†å²ï¼ˆæœ€å¤šä¿å­˜10ä¸ªåŽ†å²é€Ÿåº¦ï¼‰
                if not hasattr(region, 'velocity_history'):
                    region.velocity_history = []
                
                region.velocity_history.append(velocity)
                
                if len(region.velocity_history) > 10:
                    region.velocity_history.pop(0)
                    
        except Exception as e:
            self.logger.warning(f"é€Ÿåº¦åŽ†å²æ›´æ–°å¤±è´¥: {e}")
    
    def get_algorithm_info(self) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "è¿åŠ¨é¢„æµ‹",
                "è€ƒè™‘é€Ÿåº¦å’ŒåŠ é€Ÿåº¦",
                "è‡ªé€‚åº”é˜ˆå€¼",
                "è½¨è¿¹è¿žç»­æ€§ä¼˜åŒ–"
            ],
            'advantages': [
                "å¯¹è¿åŠ¨ç›®æ ‡è·Ÿè¸ªæ•ˆæžœå¥½",
                "èƒ½å¤Ÿå¤„ç†çŸ­æš‚é®æŒ¡",
                "é¢„æµ‹èƒ½åŠ›å¼º",
                "è½¨è¿¹å¹³æ»‘æ€§å¥½"
            ],
            'disadvantages': [
                "å¯¹éžçº¿æ€§è¿åŠ¨æ•æ„Ÿ",
                "éœ€è¦è¶³å¤Ÿçš„åŽ†å²æ•°æ®",
                "è®¡ç®—å¤æ‚åº¦ä¸­ç­‰",
                "å‚æ•°è°ƒä¼˜è¾ƒå¤æ‚"
            ],
            'best_for': [
                "è¿åŠ¨è§„å¾‹è¾ƒæ˜Žæ˜¾çš„åœºæ™¯",
                "éœ€è¦é¢„æµ‹åŠŸèƒ½çš„åœºæ™¯",
                "è½¨è¿¹è¿žç»­æ€§è¦æ±‚é«˜çš„åœºæ™¯",
                "æœ‰é®æŒ¡çš„è·Ÿè¸ªåœºæ™¯"
            ]
        })
        return info

# ========== trackers/overlap_tracker.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/overlap_tracker.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/overlap_tracker.py

import numpy as np
from scipy.spatial.distance import cdist
from typing import List, Tuple, Dict
from .base_tracker import BaseTracker

class OverlapTracker(BaseTracker):
    """é‡å åº¦åŒ¹é…è·Ÿè¸ªå™¨"""
    
    def __init__(self, config):
        super().__init__(config)
        self.algorithm_name = "overlap"
        self.description = "é‡å åº¦åŒ¹é… - åŸºäºŽåŒºåŸŸé‡å "
        
        # èŽ·å–ç®—æ³•ç‰¹å®šé…ç½®
        alg_config = config.get_algorithm_config('overlap')
        self.overlap_threshold = alg_config.get('overlap_threshold', 0.3)
        self.distance_threshold = alg_config.get('distance_threshold', 35.0)
        self.enable_reconnection = alg_config.get('enable_reconnection', True)
        self.max_inactive_frames = alg_config.get('max_inactive_frames', 20)
        
        self.algorithm_params = {
            'overlap_threshold': self.overlap_threshold,
            'distance_threshold': self.distance_threshold,
            'enable_reconnection': self.enable_reconnection,
            'max_inactive_frames': self.max_inactive_frames
        }
    
    def match_regions(self, current_regions: List[Dict], 
                     distance_threshold: float = None, frame_idx: int = 0) -> List[Tuple[int, int]]:
        """é‡å åº¦åŒ¹é…ç®—æ³•"""
        if not current_regions:
            return []
        
        # èŽ·å–æ´»è·ƒåŒºåŸŸ
        active_regions = [r for r in self.regions if r.active]
        if not active_regions:
            return []
        
        # ä½¿ç”¨é…ç½®çš„è·ç¦»é˜ˆå€¼
        if distance_threshold is None:
            distance_threshold = self.distance_threshold
        
        try:
            matches = []
            used_current = set()
            used_tracked = set()
            
            # ä¸ºæ¯ä¸ªè·Ÿè¸ªåŒºåŸŸå¯»æ‰¾æœ€ä½³åŒ¹é…
            for i, tracked_region in enumerate(active_regions):
                if i in used_tracked:
                    continue
                
                best_match = -1
                best_score = 0
                
                for j, current_region in enumerate(current_regions):
                    if j in used_current:
                        continue
                    
                    # è®¡ç®—ç»¼åˆåŒ¹é…åˆ†æ•°
                    score = self._calculate_match_score(tracked_region, current_region, distance_threshold)
                    
                    if score > best_score and score > self.overlap_threshold:
                        best_score = score
                        best_match = j
                
                if best_match >= 0:
                    matches.append((i, best_match))
                    used_tracked.add(i)
                    used_current.add(best_match)
                    
                    self.logger.debug(f"é‡å åŒ¹é…è½¨è¿¹{tracked_region.id}: åˆ†æ•°{best_score:.3f}")
            
            # å¦‚æžœå¯ç”¨é‡è¿žï¼Œå°è¯•é‡è¿žæœªåŒ¹é…çš„åŒºåŸŸ
            if self.enable_reconnection and len(matches) < len(active_regions):
                reconnection_matches = self._attempt_overlap_reconnection(
                    current_regions, active_regions, used_current, used_tracked, frame_idx
                )
                matches.extend(reconnection_matches)
            
            self.logger.debug(f"é‡å ç®—æ³•ç¬¬{frame_idx}å¸§: åŒ¹é…{len(matches)}å¯¹åŒºåŸŸ")
            return matches
            
        except Exception as e:
            self.logger.error(f"é‡å åŒ¹é…å¤±è´¥: {e}")
            return []
    
    def _calculate_match_score(self, tracked_region, current_region, distance_threshold: float) -> float:
        """è®¡ç®—åŒ¹é…åˆ†æ•°"""
        try:
            tracked_center = np.array(tracked_region.trajectory[-1])
            current_center = np.array(current_region['center'])
            
            # 1. è·ç¦»åˆ†æ•°
            distance = np.linalg.norm(tracked_center - current_center)
            if distance > distance_threshold:
                return 0.0
            
            distance_score = 1.0 - (distance / distance_threshold)
            
            # 2. ç©ºé—´é‡å åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼ŒåŸºäºŽè·ç¦»å’Œå¤§å°ï¼‰
            overlap_score = self._estimate_spatial_overlap(tracked_region, current_region, distance)
            
            # 3. å¼ºåº¦ç›¸ä¼¼æ€§åˆ†æ•°
            intensity_score = self._calculate_intensity_similarity(tracked_region, current_region)
            
            # 4. é¢ç§¯ç›¸ä¼¼æ€§åˆ†æ•°
            area_score = self._calculate_area_similarity(tracked_region, current_region)
            
            # 5. å½¢çŠ¶ç¨³å®šæ€§åˆ†æ•°
            stability_score = self._calculate_stability_score(tracked_region)
            
            # ç»¼åˆåˆ†æ•°ï¼ˆæƒé‡å¯è°ƒï¼‰
            total_score = (distance_score * 0.25 + 
                         overlap_score * 0.35 + 
                         intensity_score * 0.15 + 
                         area_score * 0.15 + 
                         stability_score * 0.1)
            
            return total_score
            
        except Exception as e:
            self.logger.warning(f"åŒ¹é…åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _estimate_spatial_overlap(self, tracked_region, current_region, distance: float) -> float:
        """ä¼°ç®—ç©ºé—´é‡å åº¦"""
        try:
            # ç®€åŒ–çš„é‡å ä¼°ç®—ï¼ŒåŸºäºŽè·ç¦»å’ŒåŒºåŸŸå¤§å°
            tracked_area = getattr(tracked_region, 'area', 0)
            current_area = current_region.get('area', 0)
            
            if tracked_area == 0 or current_area == 0:
                return 0.0
            
            # ä¼°ç®—é‡å åŒºåŸŸï¼šè·ç¦»è¶Šè¿‘ï¼Œé‡å åº¦è¶Šé«˜
            estimated_radius_tracked = np.sqrt(tracked_area / np.pi)
            estimated_radius_current = np.sqrt(current_area / np.pi)
            
            # å¦‚æžœä¸¤ä¸ªåœ†å¿ƒè·ç¦»å°äºŽä¸¤ä¸ªåŠå¾„ä¹‹å’Œï¼Œåˆ™æœ‰é‡å 
            radius_sum = estimated_radius_tracked + estimated_radius_current
            
            if distance >= radius_sum:
                return 0.0
            elif distance <= abs(estimated_radius_tracked - estimated_radius_current):
                # ä¸€ä¸ªå®Œå…¨åŒ…å«å¦ä¸€ä¸ª
                return 1.0
            else:
                # éƒ¨åˆ†é‡å ï¼Œä½¿ç”¨ç®€åŒ–å…¬å¼
                overlap_ratio = 1.0 - (distance / radius_sum)
                return max(0.0, min(1.0, overlap_ratio))
                
        except Exception as e:
            self.logger.warning(f"é‡å ä¼°ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_intensity_similarity(self, tracked_region, current_region) -> float:
        """è®¡ç®—å¼ºåº¦ç›¸ä¼¼æ€§"""
        try:
            tracked_intensity = getattr(tracked_region, 'intensity', 0)
            current_intensity = current_region.get('intensity', 0)
            
            if tracked_intensity == 0 and current_intensity == 0:
                return 1.0
            
            max_intensity = max(tracked_intensity, current_intensity, 0.1)
            intensity_diff = abs(tracked_intensity - current_intensity)
            
            similarity = 1.0 - min(1.0, intensity_diff / max_intensity)
            return similarity
            
        except Exception as e:
            self.logger.warning(f"å¼ºåº¦ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_area_similarity(self, tracked_region, current_region) -> float:
        """è®¡ç®—é¢ç§¯ç›¸ä¼¼æ€§"""
        try:
            tracked_area = getattr(tracked_region, 'area', 0)
            current_area = current_region.get('area', 0)
            
            if tracked_area == 0 and current_area == 0:
                return 1.0
            
            max_area = max(tracked_area, current_area, 1.0)
            area_diff = abs(tracked_area - current_area)
            
            similarity = 1.0 - min(1.0, area_diff / max_area)
            return similarity
            
        except Exception as e:
            self.logger.warning(f"é¢ç§¯ç›¸ä¼¼æ€§è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _calculate_stability_score(self, tracked_region) -> float:
        """è®¡ç®—ç¨³å®šæ€§åˆ†æ•°"""
        try:
            # åŸºäºŽè½¨è¿¹é•¿åº¦å’Œéžæ´»è·ƒå¸§æ•°çš„ç¨³å®šæ€§
            trajectory_length = len(tracked_region.trajectory)
            inactive_frames = getattr(tracked_region, 'inactive_frames', 0)
            
            # è½¨è¿¹è¶Šé•¿è¶Šç¨³å®š
            length_score = min(1.0, trajectory_length / 20.0)
            
            # éžæ´»è·ƒå¸§æ•°è¶Šå°‘è¶Šç¨³å®š
            activity_score = max(0.0, 1.0 - inactive_frames / self.max_inactive_frames)
            
            stability = (length_score * 0.6 + activity_score * 0.4)
            return stability
            
        except Exception as e:
            self.logger.warning(f"ç¨³å®šæ€§åˆ†æ•°è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _attempt_overlap_reconnection(self, current_regions: List[Dict], 
                                    active_regions: List,
                                    used_current: set, used_tracked: set, 
                                    frame_idx: int) -> List[Tuple[int, int]]:
        """å°è¯•é‡å é‡è¿ž"""
        reconnection_matches = []
        
        try:
            # èŽ·å–æœªåŒ¹é…çš„åŒºåŸŸ
            unmatched_tracked = [i for i in range(len(active_regions)) if i not in used_tracked]
            unmatched_current = [i for i in range(len(current_regions)) if i not in used_current]
            
            if not unmatched_tracked or not unmatched_current:
                return reconnection_matches
            
            # ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼è¿›è¡Œé‡è¿ž
            reconnection_threshold = self.overlap_threshold * 0.7
            
            for tracked_idx in unmatched_tracked:
                tracked_region = active_regions[tracked_idx]
                
                # åªä¸ºéžæ´»è·ƒå¸§æ•°ä¸å¤ªå¤šçš„åŒºåŸŸå°è¯•é‡è¿ž
                if tracked_region.inactive_frames >= self.max_inactive_frames:
                    continue
                
                best_match = -1
                best_score = 0
                
                for current_idx in unmatched_current:
                    current_region = current_regions[current_idx]
                    
                    # è®¡ç®—é‡è¿žåˆ†æ•°ï¼ˆæ›´å®½æ¾çš„æ¡ä»¶ï¼‰
                    score = self._calculate_match_score(tracked_region, current_region, 
                                                      self.distance_threshold * 1.5)
                    
                    if score > reconnection_threshold and score > best_score:
                        best_score = score
                        best_match = current_idx
                
                if best_match >= 0:
                    reconnection_matches.append((tracked_idx, best_match))
                    unmatched_current.remove(best_match)
                    
                    self.logger.debug(f"é‡å é‡è¿žè½¨è¿¹{tracked_region.id}: åˆ†æ•°{best_score:.3f}")
            
        except Exception as e:
            self.logger.warning(f"é‡å é‡è¿žå¤±è´¥: {e}")
        
        return reconnection_matches
    
    def get_algorithm_info(self) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        info = super().get_algorithm_info()
        info.update({
            'description': self.description,
            'characteristics': [
                "ç©ºé—´é‡å åˆ†æž",
                "å¤šç‰¹å¾ç»¼åˆè¯„åˆ†",
                "å½¢çŠ¶æ„ŸçŸ¥åŒ¹é…",
                "ç¨³å®šæ€§ä¼˜åŒ–"
            ],
            'advantages': [
                "è€ƒè™‘åŒºåŸŸå½¢çŠ¶ä¿¡æ¯",
                "å¯¹å½¢å˜æœ‰ä¸€å®šå®¹å¿åº¦",
                "åŒ¹é…ç²¾åº¦è¾ƒé«˜",
                "èƒ½å¤„ç†éƒ¨åˆ†é®æŒ¡"
            ],
            'disadvantages': [
                "è®¡ç®—å¤æ‚åº¦è¾ƒé«˜",
                "ä¾èµ–å‡†ç¡®çš„åŒºåŸŸæ£€æµ‹",
                "å¯¹å™ªå£°æ•æ„Ÿ",
                "å‚æ•°è¾ƒå¤š"
            ],
            'best_for': [
                "å½¢çŠ¶ç¨³å®šçš„ç›®æ ‡",
                "éœ€è¦ç²¾ç¡®åŒ¹é…çš„åœºæ™¯",
                "æœ‰é‡å å¯èƒ½çš„åœºæ™¯",
                "åŒºåŸŸè¾¹ç•Œæ¸…æ™°çš„åœºæ™¯"
            ]
        })
        return info

# ========== trackers/tracker_factory.py ==========
# ç›¸å¯¹è·¯å¾„: trackers/tracker_factory.py
# åœ¨é¡¹ç›®ä¸­çš„ç›¸å¯¹ä½ç½®: ./trackers/tracker_factory.py

"""
è·Ÿè¸ªå™¨å·¥åŽ‚ç±»
ç”¨äºŽåˆ›å»ºå’Œç®¡ç†ä¸åŒç±»åž‹çš„è·Ÿè¸ªç®—æ³•
"""

import logging
from typing import Dict, List, Optional
from .base_tracker import BaseTracker
from .greedy_tracker import GreedyTracker
from .hungarian_tracker import HungarianTracker
from .kalman_tracker import KalmanTracker
from .overlap_tracker import OverlapTracker
from .hybrid_tracker import HybridTracker

class TrackerFactory:
    """è·Ÿè¸ªå™¨å·¥åŽ‚ç±»"""
    
    # æ³¨å†Œçš„è·Ÿè¸ªå™¨ç±»
    _trackers = {
        'greedy': GreedyTracker,
        'hungarian': HungarianTracker, 
        'kalman': KalmanTracker,
        'overlap': OverlapTracker,
        'hybrid': HybridTracker
    }
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @classmethod
    def create_tracker(cls, algorithm_name: str, config) -> Optional[BaseTracker]:
        """åˆ›å»ºæŒ‡å®šç±»åž‹çš„è·Ÿè¸ªå™¨"""
        logger = logging.getLogger(__name__)
        
        if algorithm_name not in cls._trackers:
            logger.error(f"æœªçŸ¥çš„è·Ÿè¸ªç®—æ³•: {algorithm_name}")
            return None
        
        try:
            tracker_class = cls._trackers[algorithm_name]
            tracker = tracker_class(config)
            logger.info(f"æˆåŠŸåˆ›å»º{algorithm_name}è·Ÿè¸ªå™¨")
            return tracker
        except Exception as e:
            logger.error(f"åˆ›å»º{algorithm_name}è·Ÿè¸ªå™¨å¤±è´¥: {e}")
            import traceback
            logger.debug(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return None
    
    @classmethod
    def get_available_algorithms(cls) -> List[str]:
        """èŽ·å–å¯ç”¨çš„ç®—æ³•åˆ—è¡¨"""
        return list(cls._trackers.keys())
    
    @classmethod
    def get_algorithm_info(cls, algorithm_name: str) -> Dict:
        """èŽ·å–ç®—æ³•ä¿¡æ¯"""
        if algorithm_name not in cls._trackers:
            return {'error': f'æœªçŸ¥ç®—æ³•: {algorithm_name}'}
        
        try:
            # åˆ›å»ºä¸´æ—¶å®žä¾‹èŽ·å–ä¿¡æ¯
            from config import Config
            temp_tracker = cls._trackers[algorithm_name](Config)
            return temp_tracker.get_algorithm_info()
        except Exception as e:
            logger = logging.getLogger(__name__)
            logger.warning(f"èŽ·å–{algorithm_name}ç®—æ³•ä¿¡æ¯å¤±è´¥: {e}")
            return {
                'name': algorithm_name,
                'description': cls._trackers[algorithm_name].__doc__ or f'{algorithm_name} è·Ÿè¸ªç®—æ³•',
                'error': f'èŽ·å–ç®—æ³•ä¿¡æ¯å¤±è´¥: {e}'
            }
    
    @classmethod
    def compare_algorithms(cls, config) -> Dict:
        """æ¯”è¾ƒæ‰€æœ‰å¯ç”¨ç®—æ³•"""
        comparison = {
            'available_algorithms': cls.get_available_algorithms(),
            'algorithm_details': {}
        }
        
        for algorithm in cls.get_available_algorithms():
            info = cls.get_algorithm_info(algorithm)
            comparison['algorithm_details'][algorithm] = info
        
        return comparison
    
    @classmethod
    def register_tracker(cls, algorithm_name: str, tracker_class):
        """æ³¨å†Œæ–°çš„è·Ÿè¸ªå™¨ç±»"""
        if not issubclass(tracker_class, BaseTracker):
            raise ValueError("è·Ÿè¸ªå™¨ç±»å¿…é¡»ç»§æ‰¿è‡ªBaseTracker")
        
        cls._trackers[algorithm_name] = tracker_class
        logging.getLogger(__name__).info(f"æ³¨å†Œæ–°è·Ÿè¸ªå™¨: {algorithm_name}")
    
    @classmethod
    def create_all_trackers(cls, config, algorithms: Optional[List[str]] = None) -> Dict[str, BaseTracker]:
        """åˆ›å»ºå¤šä¸ªè·Ÿè¸ªå™¨"""
        if algorithms is None:
            algorithms = config.COMPARISON_ALGORITHMS
        
        trackers = {}
        
        for algorithm in algorithms:
            tracker = cls.create_tracker(algorithm, config)
            if tracker is not None:
                trackers[algorithm] = tracker
            else:
                logging.getLogger(__name__).warning(f"è·³è¿‡åˆ›å»ºå¤±è´¥çš„è·Ÿè¸ªå™¨: {algorithm}")
        
        return trackers
    
    @classmethod
    def validate_algorithm_config(cls, config) -> Dict[str, bool]:
        """éªŒè¯ç®—æ³•é…ç½®"""
        validation_results = {}
        
        for algorithm in config.COMPARISON_ALGORITHMS:
            try:
                if algorithm in cls._trackers:
                    # æ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨
                    alg_config = config.get_algorithm_config(algorithm)
                    validation_results[algorithm] = bool(alg_config)
                else:
                    validation_results[algorithm] = False
            except Exception as e:
                logging.getLogger(__name__).error(f"éªŒè¯{algorithm}é…ç½®å¤±è´¥: {e}")
                validation_results[algorithm] = False
        
        return validation_results

