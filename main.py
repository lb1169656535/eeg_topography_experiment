#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EEGè„‘ç”µåœ°å½¢å›¾è¿åŠ¨è½¨è¿¹åˆ†æä¸»ç¨‹åº - ç®—æ³•å¯¹æ¯”å¢å¼ºç‰ˆ
é›†æˆå¤šç§è·Ÿè¸ªç®—æ³•å¯¹æ¯”åŠŸèƒ½
ç‰ˆæœ¬: 3.0.0 - ç®—æ³•å¯¹æ¯”ç‰ˆ
æ›´æ–°æ—¶é—´: 2025-08-01
"""

import os
import sys
import logging
import json
import pickle
import numpy as np
import gc
import argparse
import platform
from datetime import datetime
from tqdm import tqdm
import warnings
import time

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning)

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('src')
sys.path.append('trackers')

# å­—ä½“é…ç½® - ä¿æŒåŸæœ‰è®¾ç½®
def setup_matplotlib_font():
    """é…ç½®matplotlibå­—ä½“"""
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    
    try:
        fm._rebuild()
    except:
        pass
    
    system = platform.system()
    use_chinese = False
    
    chinese_fonts = []
    if system == "Windows":
        chinese_fonts = ['Microsoft YaHei', 'SimHei', 'SimSun', 'KaiTi']
    elif system == "Darwin":  # macOS
        chinese_fonts = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti']
    elif system == "Linux":
        chinese_fonts = ['Noto Sans CJK SC', 'WenQuanYi Micro Hei', 'Droid Sans Fallback']
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in chinese_fonts:
        if font in available_fonts:
            try:
                plt.rcParams['font.sans-serif'] = [font] + plt.rcParams['font.sans-serif']
                plt.rcParams['axes.unicode_minus'] = False
                
                fig, ax = plt.subplots(figsize=(1, 1))
                ax.text(0.5, 0.5, 'æµ‹è¯•', ha='center', va='center')
                plt.close(fig)
                use_chinese = True
                print(f"âœ“ å­—ä½“é…ç½®æˆåŠŸ: {font}")
                break
            except:
                continue
    
    if not use_chinese:
        print("âš ï¸  ä½¿ç”¨è‹±æ–‡æ ‡ç­¾æ¨¡å¼")
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    return use_chinese

# è®¾ç½®å­—ä½“
USE_CHINESE = setup_matplotlib_font()

from config import Config
from src import EEGDataLoader, TopographyGenerator, TrajectoryAnalyzer, Visualizer
from trackers import TrackerFactory

def get_label(key, chinese_text, english_text):
    """è·å–æ ‡ç­¾æ–‡æœ¬"""
    return chinese_text if USE_CHINESE else english_text

def setup_logging():
    """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_dir = Config.LOGS_ROOT
    os.makedirs(log_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    log_file = os.path.join(log_dir, f"experiment_{timestamp}.log")
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    
    logging.basicConfig(
        level=logging.INFO,
        handlers=[file_handler, console_handler]
    )
    
    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œæ—¥å¿—æ–‡ä»¶: {log_file}")
    
    return logger

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–åº“"""
    required_packages = {
        'mne': 'MNE-Python',
        'numpy': 'NumPy',
        'scipy': 'SciPy',
        'matplotlib': 'Matplotlib',
        'sklearn': 'Scikit-learn',
        'cv2': 'OpenCV',
        'tqdm': 'tqdm'
    }
    
    missing_packages = []
    
    for package, name in required_packages.items():
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(name)
    
    if missing_packages:
        print("âŒ ç¼ºå°‘ä»¥ä¸‹å¿…è¦çš„ä¾èµ–åº“:")
        for package in missing_packages:
            print(f"  - {package}")
        print("\nè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…:")
        print("pip install -r requirements.txt")
        return False
    
    return True

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
    print("=" * 70)
    title = get_label('title', 'EEGè„‘ç”µåœ°å½¢å›¾è¿åŠ¨è½¨è¿¹åˆ†æç³»ç»Ÿ - ç®—æ³•å¯¹æ¯”ç‰ˆ', 
                     'EEG Topography Motion Trajectory Analysis System - Algorithm Comparison Edition')
    print(title)
    print("=" * 70)
    print(f"Pythonç‰ˆæœ¬: {platform.python_version()}")
    print(f"æ“ä½œç³»ç»Ÿ: {platform.system()} {platform.release()}")
    print(f"å¤„ç†å™¨: {platform.machine()}")
    print(f"å­—ä½“æ”¯æŒ: {'ä¸­æ–‡' if USE_CHINESE else 'English Only'}")
    
    # æ˜¾ç¤ºå®éªŒé…ç½®
    summary = Config.get_experiment_summary()
    print(f"\nå®éªŒé…ç½®:")
    print(f"  è¢«è¯•æ•°é‡: {summary['total_subjects']}")
    print(f"  å¯¹æ¯”ç®—æ³•æ•°é‡: {summary['algorithms_count']}")
    print(f"  ç®—æ³•åˆ—è¡¨: {', '.join(summary['algorithm_names'])}")
    print(f"  è¯„ä¼°æŒ‡æ ‡æ•°é‡: {summary['metrics_count']}")
    print(f"  æ¯ä¸ªepochæœ€å¤§å¸§æ•°: {summary['max_frames_per_epoch']}")  # æ–°å¢æ˜¾ç¤º
    print(f"  ç®—æ³•å¯¹æ¯”: {'å¯ç”¨' if summary['algorithm_comparison_enabled'] else 'ç¦ç”¨'}")
    
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"  æ€»å†…å­˜: {memory.total / (1024**3):.1f} GB")
        print(f"  å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f} GB")
    except ImportError:
        pass
    
    print("=" * 70)

def validate_config():
    """éªŒè¯é…ç½®å‚æ•°"""
    logger = logging.getLogger(__name__)
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(Config.DATA_ROOT):
        error_msg = get_label('data_error', 
                             f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {Config.DATA_ROOT}",
                             f"Data directory not found: {Config.DATA_ROOT}")
        logger.error(error_msg)
        print(f"\nâŒ {error_msg}")
        print(get_label('check_config', 
                       "è¯·æ£€æŸ¥config.pyä¸­çš„DATA_ROOTè®¾ç½®",
                       "Please check DATA_ROOT setting in config.py"))
        return False
    
    # éªŒè¯ç®—æ³•é…ç½®
    validation_results = TrackerFactory.validate_algorithm_config(Config)
    invalid_algorithms = [alg for alg, valid in validation_results.items() if not valid]
    
    if invalid_algorithms:
        logger.warning(f"ä»¥ä¸‹ç®—æ³•é…ç½®æ— æ•ˆ: {invalid_algorithms}")
        print(f"âš ï¸  ä»¥ä¸‹ç®—æ³•é…ç½®å¯èƒ½æœ‰é—®é¢˜: {', '.join(invalid_algorithms)}")
    
    # æ£€æŸ¥å¯ç”¨ç®—æ³•
    available = TrackerFactory.get_available_algorithms()
    missing = [alg for alg in Config.COMPARISON_ALGORITHMS if alg not in available]
    
    if missing:
        logger.error(f"ä»¥ä¸‹ç®—æ³•ä¸å¯ç”¨: {missing}")
        print(f"âŒ ä»¥ä¸‹ç®—æ³•ä¸å¯ç”¨: {', '.join(missing)}")
        return False
    
    # éªŒè¯å¸§æ•°é…ç½®
    if Config.MAX_FRAMES_PER_EPOCH <= 0:
        logger.error(f"æ— æ•ˆçš„æœ€å¤§å¸§æ•°é…ç½®: {Config.MAX_FRAMES_PER_EPOCH}")
        print(f"âŒ æ— æ•ˆçš„æœ€å¤§å¸§æ•°é…ç½®: {Config.MAX_FRAMES_PER_EPOCH}")
        return False
    
    logger.info(f"é…ç½®éªŒè¯å®Œæˆï¼Œæœ€å¤§å¸§æ•°é™åˆ¶: {Config.MAX_FRAMES_PER_EPOCH}")
    return True

def process_subject_with_multiple_algorithms(data_loader, topo_generator, analyzer, visualizer,
                                           subject_id, sessions, logger):
    """ä½¿ç”¨å¤šç§ç®—æ³•å¤„ç†å•ä¸ªè¢«è¯•çš„æ•°æ®"""
    subject_results = {}
    
    session_label = get_label('session_process', 
                             f"å¤„ç†è¢«è¯• {subject_id} (å…±{len(sessions)}ä¸ªsession, {len(Config.COMPARISON_ALGORITHMS)}ç§ç®—æ³•)",
                             f"Processing subject {subject_id} ({len(sessions)} sessions, {len(Config.COMPARISON_ALGORITHMS)} algorithms)")
    logger.info(session_label)
    
    # åˆ›å»ºæ‰€æœ‰è·Ÿè¸ªå™¨
    trackers = TrackerFactory.create_all_trackers(Config)
    if not trackers:
        logger.error(f"æ— æ³•åˆ›å»ºè·Ÿè¸ªå™¨")
        return None
    
    logger.info(f"æˆåŠŸåˆ›å»º {len(trackers)} ä¸ªè·Ÿè¸ªå™¨: {', '.join(trackers.keys())}")
    
    for session_id, session_data in sessions.items():
        session_key = f"{subject_id}_{session_id}"
        session_info = get_label('session_info', 
                                f"  å¤„ç†session {session_id}",
                                f"  Processing session {session_id}")
        logger.info(session_info)
        
        try:
            epochs = session_data['epochs']
            positions = session_data['positions']
            ch_names = epochs.ch_names
            
            # é€‰æ‹©å¤šä¸ªepochè¿›è¡Œåˆ†æ
            n_epochs_to_analyze = min(len(epochs), Config.MAX_EPOCHS_PER_SUBJECT)
            
            session_algorithm_results = {}
            
            for epoch_idx in range(n_epochs_to_analyze):
                try:
                    epoch_data = epochs.get_data()[epoch_idx]
                    
                    # ç”Ÿæˆåœ°å½¢å›¾åºåˆ—
                    epoch_info = get_label('epoch_topo', 
                                          f"    ç”Ÿæˆepoch {epoch_idx+1} åœ°å½¢å›¾åºåˆ—...",
                                          f"    Generating epoch {epoch_idx+1} topographies...")
                    logger.info(epoch_info)
                    
                    # ä½¿ç”¨é…ç½®å‚æ•°é™åˆ¶æ—¶é—´ç‚¹æ•°é‡
                    max_time_points = min(epoch_data.shape[1], Config.MAX_FRAMES_PER_EPOCH)
                    epoch_data_subset = epoch_data[:, :max_time_points]
                    
                    logger.info(f"    ä½¿ç”¨å¸§æ•°é™åˆ¶: {Config.MAX_FRAMES_PER_EPOCH}, å®é™…å¤„ç†: {max_time_points} å¸§")
                    
                    topographies = topo_generator.generate_time_series_topographies(
                        epoch_data_subset[np.newaxis, :, :], positions, ch_names
                    )[0]
                    
                    if topographies is None or topographies.size == 0:
                        logger.warning(f"    Epoch {epoch_idx+1}: åœ°å½¢å›¾ç”Ÿæˆå¤±è´¥")
                        continue
                    
                    # æ ‡å‡†åŒ–åœ°å½¢å›¾
                    for t in range(topographies.shape[0]):
                        topographies[t] = topo_generator.normalize_topography(topographies[t])
                    
                    # ä½¿ç”¨æ¯ç§ç®—æ³•è¿›è¡Œè½¨è¿¹è·Ÿè¸ª
                    epoch_algorithm_results = {}
                    
                    for algorithm_name, tracker in trackers.items():
                        try:
                            track_info = get_label('epoch_track',
                                                  f"    ä½¿ç”¨{algorithm_name}ç®—æ³•è·Ÿè¸ªepoch {epoch_idx+1}...",
                                                  f"    Tracking epoch {epoch_idx+1} with {algorithm_name}...")
                            logger.info(track_info)
                            
                            start_time = time.time()
                            tracking_results = tracker.track_sequence(topographies)
                            end_time = time.time()
                            
                            if not tracking_results or 'trajectories' not in tracking_results:
                                logger.warning(f"    {algorithm_name}: Epoch {epoch_idx+1} è½¨è¿¹è·Ÿè¸ªè¿”å›ç©ºç»“æœ")
                                continue
                            
                            trajectories = tracking_results['trajectories']
                            if not trajectories:
                                logger.warning(f"    {algorithm_name}: Epoch {epoch_idx+1} æœªæ£€æµ‹åˆ°æœ‰æ•ˆè½¨è¿¹")
                                continue
                            
                            # è®°å½•ç»“æœ
                            epoch_algorithm_results[algorithm_name] = {
                                'trajectories': trajectories,
                                'metrics': tracking_results.get('metrics', {}),
                                'summary': tracking_results.get('summary', {}),
                                'computation_time': end_time - start_time,
                                'processed_frames': topographies.shape[0]  # æ–°å¢ï¼šè®°å½•å®é™…å¤„ç†å¸§æ•°
                            }
                            
                            found_info = get_label('found_traj',
                                                  f"    {algorithm_name}: Epoch {epoch_idx+1} æ‰¾åˆ° {len(trajectories)} æ¡è½¨è¿¹ "
                                                  f"(å¤„ç†{topographies.shape[0]}å¸§, è€—æ—¶ {end_time - start_time:.2f}s)",
                                                  f"    {algorithm_name}: Epoch {epoch_idx+1} found {len(trajectories)} trajectories "
                                                  f"(processed {topographies.shape[0]} frames, time: {end_time - start_time:.2f}s)")
                            logger.info(found_info)
                            
                        except Exception as e:
                            logger.error(f"    {algorithm_name}: Epoch {epoch_idx+1} è½¨è¿¹è·Ÿè¸ªå¤±è´¥: {e}")
                            continue
                    
                    # å¦‚æœæœ‰ç»“æœï¼Œä¿å­˜epochçº§åˆ«çš„å¯¹æ¯”
                    if epoch_algorithm_results:
                        # ä¿å­˜æ¯ç§ç®—æ³•çš„ä»£è¡¨æ€§å¯è§†åŒ–
                        for algorithm_name, results in epoch_algorithm_results.items():
                            trajectories = results['trajectories']
                            
                            # ä¿å­˜è½¨è¿¹å›¾
                            traj_path = os.path.join(Config.RESULTS_ROOT, "trajectories", 
                                                   f"{session_key}_epoch{epoch_idx}_{algorithm_name}_trajectories.png")
                            try:
                                title = get_label('traj_title',
                                                f"è¢«è¯•{subject_id} Session{session_id} Epoch{epoch_idx} - {algorithm_name}ç®—æ³• ({results['processed_frames']}å¸§)",
                                                f"Subject {subject_id} Session {session_id} Epoch {epoch_idx} - {algorithm_name} Algorithm ({results['processed_frames']} frames)")
                                visualizer.plot_trajectories(
                                    trajectories, topographies.shape[1:],
                                    title=title,
                                    save_path=traj_path
                                )
                            except Exception as e:
                                logger.warning(f"ä¿å­˜{algorithm_name}è½¨è¿¹å›¾å¤±è´¥: {e}")
                        
                        # å°†epochç»“æœåˆå¹¶åˆ°sessionç»“æœä¸­
                        for algorithm_name, results in epoch_algorithm_results.items():
                            if algorithm_name not in session_algorithm_results:
                                session_algorithm_results[algorithm_name] = {
                                    'trajectories': {},
                                    'total_computation_time': 0,
                                    'epoch_count': 0,
                                    'metrics_sum': {},
                                    'total_frames_processed': 0  # æ–°å¢
                                }
                            
                            # åˆå¹¶è½¨è¿¹ï¼ˆæ·»åŠ epochå‰ç¼€ï¼‰
                            for traj_id, traj_data in results['trajectories'].items():
                                key = f"epoch{epoch_idx}_{traj_id}"
                                session_algorithm_results[algorithm_name]['trajectories'][key] = traj_data
                            
                            # ç´¯è®¡ç»Ÿè®¡
                            session_algorithm_results[algorithm_name]['total_computation_time'] += results['computation_time']
                            session_algorithm_results[algorithm_name]['epoch_count'] += 1
                            session_algorithm_results[algorithm_name]['total_frames_processed'] += results['processed_frames']
                            
                            # ç´¯è®¡æŒ‡æ ‡
                            for metric, value in results.get('metrics', {}).items():
                                if metric not in session_algorithm_results[algorithm_name]['metrics_sum']:
                                    session_algorithm_results[algorithm_name]['metrics_sum'][metric] = []
                                session_algorithm_results[algorithm_name]['metrics_sum'][metric].append(value)
                    
                    # å†…å­˜æ¸…ç†
                    del topographies
                    gc.collect()
                    
                except Exception as e:
                    logger.error(f"    Epoch {epoch_idx+1} å¤„ç†å¤±è´¥: {e}")
                    continue
            
            # å¤„ç†sessionçº§åˆ«çš„ç»“æœ
            if session_algorithm_results:
                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                for algorithm_name in session_algorithm_results:
                    alg_result = session_algorithm_results[algorithm_name]
                    
                    # è®¡ç®—å¹³å‡æŒ‡æ ‡
                    avg_metrics = {}
                    for metric, values in alg_result['metrics_sum'].items():
                        if values:
                            avg_metrics[metric] = np.mean(values)
                    
                    # æ›´æ–°ç»“æœ
                    alg_result['average_metrics'] = avg_metrics
                    alg_result['total_trajectories'] = len(alg_result['trajectories'])
                    alg_result['avg_frames_per_epoch'] = alg_result['total_frames_processed'] / alg_result['epoch_count'] if alg_result['epoch_count'] > 0 else 0
                    
                    session_algorithm_results[algorithm_name] = alg_result
                
                subject_results[session_id] = session_algorithm_results
                
                session_summary = get_label('session_summary',
                                          f"  Session {session_id}: ç®—æ³•å¯¹æ¯”å®Œæˆ",
                                          f"  Session {session_id}: Algorithm comparison completed")
                logger.info(session_summary)
                
                # æ˜¾ç¤ºå„ç®—æ³•çš„ç®€è¦ç»“æœ
                for algorithm_name, alg_result in session_algorithm_results.items():
                    logger.info(f"    {algorithm_name}: {alg_result['total_trajectories']} æ¡è½¨è¿¹, "
                              f"å¹³å‡è€—æ—¶ {alg_result['total_computation_time']/alg_result['epoch_count']:.2f}s, "
                              f"å¹³å‡å¤„ç† {alg_result['avg_frames_per_epoch']:.0f} å¸§/epoch")
            else:
                logger.warning(f"  Session {session_id}: æ‰€æœ‰ç®—æ³•å‡æœªæ‰¾åˆ°æœ‰æ•ˆè½¨è¿¹")
                
        except Exception as e:
            logger.error(f"  å¤„ç†session {session_id} æ—¶å‡ºé”™: {e}")
            continue
    
    return subject_results if subject_results else None

def create_algorithm_comparison_report(all_results, logger):
    """åˆ›å»ºç®—æ³•å¯¹æ¯”æŠ¥å‘Š"""
    logger.info("ç”Ÿæˆç®—æ³•å¯¹æ¯”æŠ¥å‘Š...")
    
    try:
        # æ”¶é›†æ‰€æœ‰ç®—æ³•çš„ç»Ÿè®¡æ•°æ®
        algorithm_stats = {}
        
        for subject_id, sessions in all_results.items():
            for session_id, session_data in sessions.items():
                for algorithm_name, alg_data in session_data.items():
                    if algorithm_name not in algorithm_stats:
                        algorithm_stats[algorithm_name] = {
                            'total_trajectories': [],
                            'computation_times': [],
                            'trajectory_lengths': [],
                            'trajectory_qualities': [],
                            'frames_processed': []  # æ–°å¢
                        }
                    
                    # æ”¶é›†ç»Ÿè®¡æ•°æ®
                    algorithm_stats[algorithm_name]['total_trajectories'].append(alg_data['total_trajectories'])
                    algorithm_stats[algorithm_name]['computation_times'].append(alg_data['total_computation_time'])
                    algorithm_stats[algorithm_name]['frames_processed'].append(alg_data.get('total_frames_processed', 0))
                    
                    # æ”¶é›†è½¨è¿¹ç»Ÿè®¡
                    for traj_data in alg_data['trajectories'].values():
                        algorithm_stats[algorithm_name]['trajectory_lengths'].append(traj_data['length'])
                        algorithm_stats[algorithm_name]['trajectory_qualities'].append(traj_data.get('quality_score', 0))
        
        # ç”ŸæˆæŠ¥å‘Š
        report = []
        report.append("=" * 80)
        report.append("EEGè½¨è¿¹è·Ÿè¸ªç®—æ³•å¯¹æ¯”æŠ¥å‘Š")
        report.append("=" * 80)
        report.append(f"å®éªŒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"å¯¹æ¯”ç®—æ³•æ•°é‡: {len(algorithm_stats)}")
        report.append(f"å¤„ç†è¢«è¯•æ•°é‡: {len(all_results)}")
        report.append(f"å¸§æ•°é™åˆ¶è®¾ç½®: {Config.MAX_FRAMES_PER_EPOCH} å¸§/epoch")  # æ–°å¢
        report.append("")
        
        # ç®—æ³•æ€§èƒ½æ±‡æ€»
        report.append("ç®—æ³•æ€§èƒ½æ±‡æ€»:")
        report.append("-" * 50)
        
        performance_ranking = []
        
        for algorithm_name, stats in algorithm_stats.items():
            if not stats['total_trajectories']:
                continue
            
            avg_trajectories = np.mean(stats['total_trajectories'])
            avg_time = np.mean(stats['computation_times'])
            avg_length = np.mean(stats['trajectory_lengths']) if stats['trajectory_lengths'] else 0
            avg_quality = np.mean(stats['trajectory_qualities']) if stats['trajectory_qualities'] else 0
            avg_frames = np.mean(stats['frames_processed']) if stats['frames_processed'] else 0
            
            # è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°
            performance_score = (avg_trajectories * 0.3 + 
                               avg_length * 0.25 + 
                               avg_quality * 0.25 + 
                               (10 / max(avg_time, 0.1)) * 0.2)  # æ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜
            
            performance_ranking.append((algorithm_name, performance_score, {
                'avg_trajectories': avg_trajectories,
                'avg_time': avg_time,
                'avg_length': avg_length,
                'avg_quality': avg_quality,
                'avg_frames': avg_frames
            }))
            
            report.append(f"\n{algorithm_name.upper()} ç®—æ³•:")
            report.append(f"  å¹³å‡è½¨è¿¹æ•°é‡: {avg_trajectories:.2f}")
            report.append(f"  å¹³å‡è®¡ç®—æ—¶é—´: {avg_time:.3f}s")
            report.append(f"  å¹³å‡è½¨è¿¹é•¿åº¦: {avg_length:.1f} å¸§")
            report.append(f"  å¹³å‡è½¨è¿¹è´¨é‡: {avg_quality:.3f}")
            report.append(f"  å¹³å‡å¤„ç†å¸§æ•°: {avg_frames:.0f} å¸§")  # æ–°å¢
            report.append(f"  ç»¼åˆæ€§èƒ½åˆ†æ•°: {performance_score:.3f}")
        
        # ç®—æ³•æ’å
        performance_ranking.sort(key=lambda x: x[1], reverse=True)
        
        report.append("\nç®—æ³•æ€§èƒ½æ’å:")
        report.append("-" * 30)
        
        for i, (algorithm_name, score, details) in enumerate(performance_ranking, 1):
            report.append(f"{i}. {algorithm_name.upper()}: {score:.3f}")
            if i == 1:
                report.append("   ğŸ† ç»¼åˆæ€§èƒ½æœ€ä½³")
        
        # ç®—æ³•ç‰¹è‰²åˆ†æ
        report.append("\nç®—æ³•ç‰¹è‰²åˆ†æ:")
        report.append("-" * 30)
        
        if performance_ranking:
            # æœ€å¤šè½¨è¿¹
            max_traj_alg = max(performance_ranking, key=lambda x: x[2]['avg_trajectories'])
            report.append(f"æ£€æµ‹èƒ½åŠ›æœ€å¼º: {max_traj_alg[0]} ({max_traj_alg[2]['avg_trajectories']:.1f} æ¡å¹³å‡è½¨è¿¹)")
            
            # æœ€å¿«é€Ÿåº¦
            min_time_alg = min(performance_ranking, key=lambda x: x[2]['avg_time'])
            report.append(f"è®¡ç®—é€Ÿåº¦æœ€å¿«: {min_time_alg[0]} ({min_time_alg[2]['avg_time']:.3f}s å¹³å‡æ—¶é—´)")
            
            # æœ€é«˜è´¨é‡
            max_quality_alg = max(performance_ranking, key=lambda x: x[2]['avg_quality'])
            report.append(f"è½¨è¿¹è´¨é‡æœ€é«˜: {max_quality_alg[0]} ({max_quality_alg[2]['avg_quality']:.3f} å¹³å‡è´¨é‡)")
            
            # æœ€é•¿è½¨è¿¹
            max_length_alg = max(performance_ranking, key=lambda x: x[2]['avg_length'])
            report.append(f"è·Ÿè¸ªæŒç»­æœ€é•¿: {max_length_alg[0]} ({max_length_alg[2]['avg_length']:.1f} å¸§å¹³å‡é•¿åº¦)")
        
        # ä½¿ç”¨å»ºè®®
        report.append("\nä½¿ç”¨å»ºè®®:")
        report.append("-" * 20)
        
        if performance_ranking:
            best_overall = performance_ranking[0][0]
            report.append(f"â€¢ ç»¼åˆæ¨è: {best_overall} (ç»¼åˆæ€§èƒ½æœ€ä½³)")
            
            # é’ˆå¯¹ä¸åŒéœ€æ±‚çš„æ¨è
            if len(performance_ranking) > 1:
                fastest = min(performance_ranking, key=lambda x: x[2]['avg_time'])[0]
                highest_quality = max(performance_ranking, key=lambda x: x[2]['avg_quality'])[0]
                most_trajectories = max(performance_ranking, key=lambda x: x[2]['avg_trajectories'])[0]
                
                report.append(f"â€¢ å®æ—¶å¤„ç†æ¨è: {fastest} (é€Ÿåº¦ä¼˜å…ˆ)")
                report.append(f"â€¢ é«˜ç²¾åº¦åˆ†ææ¨è: {highest_quality} (è´¨é‡ä¼˜å…ˆ)")
                report.append(f"â€¢ å¤æ‚åœºæ™¯æ¨è: {most_trajectories} (æ£€æµ‹èƒ½åŠ›ä¼˜å…ˆ)")
        
        # å‚æ•°é…ç½®ä¿¡æ¯
        report.append("\nå½“å‰å‚æ•°é…ç½®:")
        report.append("-" * 30)
        report.append(f"â€¢ æœ€å¤§å¸§æ•°é™åˆ¶: {Config.MAX_FRAMES_PER_EPOCH} å¸§/epoch")
        report.append(f"â€¢ æœ€å¤§è¢«è¯•æ•°: {Config.MAX_SUBJECTS}")
        report.append(f"â€¢ æœ€å¤§epochæ•°: {Config.MAX_EPOCHS_PER_SUBJECT}")
        report.append(f"â€¢ é˜ˆå€¼ç™¾åˆ†ä½: {Config.THRESHOLD_PERCENTILE}%")
        report.append("")
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        report_path = os.path.join(Config.RESULTS_ROOT, "algorithm_comparison", "comparison_report.txt")
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"ç®—æ³•å¯¹æ¯”æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        
        # æ˜¾ç¤ºæŠ¥å‘Šé¢„è§ˆ
        print("\n" + "="*60)
        print("ç®—æ³•å¯¹æ¯”æŠ¥å‘Šé¢„è§ˆ:")
        print("="*60)
        preview_lines = report_text.split('\n')[:30]  # æ˜¾ç¤ºå‰30è¡Œ
        print('\n'.join(preview_lines))
        if len(report) > 30:
            print("\n... (å®Œæ•´æŠ¥å‘Šè¯·æŸ¥çœ‹æ–‡ä»¶)")
        print("="*60)
        
        return algorithm_stats, performance_ranking
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆç®—æ³•å¯¹æ¯”æŠ¥å‘Šå¤±è´¥: {e}")
        return {}, []

def create_algorithm_comparison_visualizations(all_results, algorithm_stats, performance_ranking, visualizer, logger):
    """åˆ›å»ºç®—æ³•å¯¹æ¯”å¯è§†åŒ–"""
    logger.info("ç”Ÿæˆç®—æ³•å¯¹æ¯”å¯è§†åŒ–...")
    
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        comparison_dir = os.path.join(Config.RESULTS_ROOT, "algorithm_comparison")
        os.makedirs(comparison_dir, exist_ok=True)
        
        # 1. ç®—æ³•æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾
        if algorithm_stats and performance_ranking:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            fig.suptitle('ç®—æ³•æ€§èƒ½å¯¹æ¯”åˆ†æ', fontsize=16, fontweight='bold')
            
            algorithms = [item[0] for item in performance_ranking]
            
            # è½¨è¿¹æ•°é‡å¯¹æ¯”
            ax = axes[0, 0]
            traj_counts = [np.mean(algorithm_stats[alg]['total_trajectories']) for alg in algorithms]
            bars = ax.bar(algorithms, traj_counts, color='skyblue', alpha=0.7)
            ax.set_title('å¹³å‡è½¨è¿¹æ•°é‡å¯¹æ¯”')
            ax.set_ylabel('è½¨è¿¹æ•°é‡')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, count in zip(bars, traj_counts):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                       f'{count:.1f}', ha='center', va='bottom')
            
            # è®¡ç®—æ—¶é—´å¯¹æ¯”
            ax = axes[0, 1]
            comp_times = [np.mean(algorithm_stats[alg]['computation_times']) for alg in algorithms]
            bars = ax.bar(algorithms, comp_times, color='lightgreen', alpha=0.7)
            ax.set_title('å¹³å‡è®¡ç®—æ—¶é—´å¯¹æ¯”')
            ax.set_ylabel('æ—¶é—´ (ç§’)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, time in zip(bars, comp_times):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                       f'{time:.2f}s', ha='center', va='bottom')
            
            # è½¨è¿¹é•¿åº¦å¯¹æ¯”
            ax = axes[1, 0]
            traj_lengths = [np.mean(algorithm_stats[alg]['trajectory_lengths']) if algorithm_stats[alg]['trajectory_lengths'] else 0 
                           for alg in algorithms]
            bars = ax.bar(algorithms, traj_lengths, color='orange', alpha=0.7)
            ax.set_title('å¹³å‡è½¨è¿¹é•¿åº¦å¯¹æ¯”')
            ax.set_ylabel('é•¿åº¦ (å¸§)')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, length in zip(bars, traj_lengths):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                       f'{length:.1f}', ha='center', va='bottom')
            
            # ç»¼åˆæ€§èƒ½åˆ†æ•°
            ax = axes[1, 1]
            performance_scores = [item[1] for item in performance_ranking]
            bars = ax.bar(algorithms, performance_scores, color='coral', alpha=0.7)
            ax.set_title('ç»¼åˆæ€§èƒ½åˆ†æ•°')
            ax.set_ylabel('æ€§èƒ½åˆ†æ•°')
            ax.tick_params(axis='x', rotation=45)
            
            for bar, score in zip(bars, performance_scores):
                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                       f'{score:.2f}', ha='center', va='bottom')
            
            plt.tight_layout()
            comparison_path = os.path.join(comparison_dir, "algorithm_performance_comparison.png")
            plt.savefig(comparison_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {comparison_path}")
            
        # 2. ç®—æ³•ç‰¹å¾é›·è¾¾å›¾
        if len(performance_ranking) >= 2:
            fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
            
            # å‡†å¤‡æ•°æ®
            metrics = ['è½¨è¿¹æ•°é‡', 'è®¡ç®—é€Ÿåº¦', 'è½¨è¿¹é•¿åº¦', 'è½¨è¿¹è´¨é‡']
            colors = ['red', 'blue', 'green', 'orange', 'purple']
            
            for i, (algorithm_name, _, details) in enumerate(performance_ranking[:5]):  # æœ€å¤šæ˜¾ç¤º5ä¸ªç®—æ³•
                values = [
                    details['avg_trajectories'] / max([d[2]['avg_trajectories'] for d in performance_ranking]),  # æ ‡å‡†åŒ–
                    (10 / max(details['avg_time'], 0.1)) / max([10 / max(d[2]['avg_time'], 0.1) for d in performance_ranking]),  # é€Ÿåº¦è¶Šå¿«è¶Šå¥½
                    details['avg_length'] / max([d[2]['avg_length'] for d in performance_ranking]),
                    details['avg_quality'] / max([d[2]['avg_quality'] for d in performance_ranking]) if max([d[2]['avg_quality'] for d in performance_ranking]) > 0 else 0
                ]
                
                angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
                values += values[:1]  # é—­åˆå›¾å½¢
                angles += angles[:1]
                
                ax.plot(angles, values, 'o-', linewidth=2, label=algorithm_name, color=colors[i % len(colors)])
                ax.fill(angles, values, alpha=0.25, color=colors[i % len(colors)])
            
            ax.set_xticks(angles[:-1])
            ax.set_xticklabels(metrics)
            ax.set_ylim(0, 1)
            ax.set_title('ç®—æ³•æ€§èƒ½é›·è¾¾å›¾', size=16, fontweight='bold', pad=20)
            ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
            
            radar_path = os.path.join(comparison_dir, "algorithm_radar_chart.png")
            plt.savefig(radar_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"ç®—æ³•é›·è¾¾å›¾å·²ä¿å­˜: {radar_path}")
        
        logger.info("ç®—æ³•å¯¹æ¯”å¯è§†åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")

def cleanup_memory():
    """æ¸…ç†å†…å­˜"""
    gc.collect()
    
    try:
        import psutil
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / (1024 * 1024)
        
        if memory_mb > Config.MEMORY_LIMIT_MB:
            logging.getLogger(__name__).warning(
                f"å†…å­˜ä½¿ç”¨é‡è¿‡é«˜: {memory_mb:.1f} MB (é™åˆ¶: {Config.MEMORY_LIMIT_MB} MB)"
            )
            return False
    except ImportError:
        pass
    
    return True

def print_final_summary(all_results, algorithm_stats):
    """æ‰“å°æœ€ç»ˆæ€»ç»“"""
    print("\n" + "="*70)
    print("ç®—æ³•å¯¹æ¯”å®éªŒå®Œæˆæ€»ç»“")
    print("="*70)
    
    # åŸºæœ¬ç»Ÿè®¡
    n_subjects = len(all_results)
    total_sessions = sum(len(sessions) for sessions in all_results.values())
    
    print(f"âœ“ æˆåŠŸå¤„ç†è¢«è¯•æ•°é‡: {n_subjects}")
    print(f"âœ“ æ€»sessionæ•°é‡: {total_sessions}")
    print(f"âœ“ å¯¹æ¯”ç®—æ³•æ•°é‡: {len(algorithm_stats)}")
    print(f"âœ“ ç®—æ³•åˆ—è¡¨: {', '.join(algorithm_stats.keys())}")
    print(f"âœ“ å¸§æ•°é™åˆ¶è®¾ç½®: {Config.MAX_FRAMES_PER_EPOCH} å¸§/epoch")
    
    # æ˜¾ç¤ºå„ç®—æ³•çš„æ€»ä½“è¡¨ç°
    print(f"\nå„ç®—æ³•æ€»ä½“è¡¨ç°:")
    for algorithm_name, stats in algorithm_stats.items():
        if stats['total_trajectories']:
            avg_trajectories = np.mean(stats['total_trajectories'])
            avg_time = np.mean(stats['computation_times'])
            avg_frames = np.mean(stats['frames_processed']) if stats['frames_processed'] else 0
            print(f"  {algorithm_name}: å¹³å‡{avg_trajectories:.1f}æ¡è½¨è¿¹, å¹³å‡è€—æ—¶{avg_time:.2f}s, å¹³å‡å¤„ç†{avg_frames:.0f}å¸§")
    
    # è¾“å‡ºè·¯å¾„
    print(f"\nç»“æœä¿å­˜ä½ç½®:")
    print(f"  ğŸ“ è½¨è¿¹å›¾å¯¹æ¯”: {os.path.join(Config.RESULTS_ROOT, 'trajectories')}")
    print(f"  ğŸ“Š ç®—æ³•å¯¹æ¯”å›¾: {os.path.join(Config.RESULTS_ROOT, 'algorithm_comparison')}")
    print(f"  ğŸ“„ å¯¹æ¯”æŠ¥å‘Š: {os.path.join(Config.RESULTS_ROOT, 'algorithm_comparison', 'comparison_report.txt')}")
    
    print("="*70)
    print("ğŸ‰ ç®—æ³•å¯¹æ¯”å®éªŒæˆåŠŸå®Œæˆï¼")
    print("="*70)

def main():
    """ä¸»å®éªŒæµç¨‹"""
    parser = argparse.ArgumentParser(description='EEG Trajectory Analysis System with Algorithm Comparison')
    parser.add_argument('--subjects', type=int, default=None, 
                       help='Maximum number of subjects to process')
    parser.add_argument('--epochs', type=int, default=None,
                       help='Maximum epochs per subject')
    parser.add_argument('--frames', type=int, default=None,
                       help='Maximum frames per epoch')  # æ–°å¢å‚æ•°
    parser.add_argument('--algorithms', nargs='+', default=None,
                       help='Algorithms to compare', choices=Config.COMPARISON_ALGORITHMS)
    parser.add_argument('--disable-comparison', action='store_true',
                       help='Disable algorithm comparison (use greedy only)')
    
    args = parser.parse_args()
    
    # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    print_system_info()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return 1
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging()
    start_msg = get_label('start_experiment',
                         "å¼€å§‹EEGè„‘ç”µåœ°å½¢å›¾è¿åŠ¨è½¨è¿¹åˆ†æå®éªŒ - ç®—æ³•å¯¹æ¯”ç‰ˆ",
                         "Starting EEG topography motion trajectory analysis experiment - Algorithm Comparison Edition")
    logger.info(start_msg)
    
    try:
        # éªŒè¯é…ç½®
        if not validate_config():
            return 1
        
        # åº”ç”¨å‘½ä»¤è¡Œå‚æ•°
        if args.subjects:
            Config.MAX_SUBJECTS = args.subjects
        if args.epochs:
            Config.MAX_EPOCHS_PER_SUBJECT = args.epochs
        if args.frames:  # æ–°å¢ï¼šåŠ¨æ€è®¾ç½®å¸§æ•°é™åˆ¶
            Config.set_max_frames(args.frames, 'epoch')
            logger.info(f"å¸§æ•°é™åˆ¶å·²è®¾ç½®ä¸º: {args.frames}")
        if args.algorithms:
            Config.COMPARISON_ALGORITHMS = args.algorithms
        if args.disable_comparison:
            Config.ENABLE_ALGORITHM_COMPARISON = False
            Config.COMPARISON_ALGORITHMS = ['greedy']
        
        # åˆå§‹åŒ–ç»„ä»¶
        init_msg = get_label('init_components', 
                            "åˆå§‹åŒ–åˆ†æç»„ä»¶...",
                            "Initializing analysis components...")
        logger.info(init_msg)
        
        data_loader = EEGDataLoader(Config.DATA_ROOT, Config)
        topo_generator = TopographyGenerator(Config)
        analyzer = TrajectoryAnalyzer(Config)
        visualizer = Visualizer(Config)
        
        # åŠ è½½æ•°æ®
        load_msg = get_label('load_data',
                            "å¼€å§‹åŠ è½½EEGæ•°æ®...",
                            "Starting to load EEG data...")
        logger.info(load_msg)
        all_data = data_loader.load_all_subjects(Config.MAX_SUBJECTS)
        
        if not all_data:
            error_msg = get_label('no_data',
                                 "æœªèƒ½åŠ è½½ä»»ä½•EEGæ•°æ®ï¼Œè¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼",
                                 "Failed to load any EEG data, please check data path and format")
            logger.error(error_msg)
            print(f"\nâŒ {error_msg}")
            return 1
        
        success_msg = get_label('load_success',
                               f"æˆåŠŸåŠ è½½ {len(all_data)} ä¸ªè¢«è¯•çš„æ•°æ®",
                               f"Successfully loaded data from {len(all_data)} subjects")
        logger.info(success_msg)
        
        # å­˜å‚¨æ‰€æœ‰ç»“æœ
        all_results = {}
        
        # å¤„ç†æ¯ä¸ªè¢«è¯•
        total_subjects = len(all_data)
        processed_subjects = 0
        
        for subject_id, sessions in tqdm(all_data.items(), desc="Processing subjects"):
            try:
                if Config.ENABLE_ALGORITHM_COMPARISON:
                    subject_results = process_subject_with_multiple_algorithms(
                        data_loader, topo_generator, analyzer, visualizer,
                        subject_id, sessions, logger
                    )
                else:
                    # ä½¿ç”¨åŸæœ‰çš„å•ç®—æ³•å¤„ç†é€»è¾‘ï¼ˆè¿™é‡Œå¯ä»¥è°ƒç”¨åŸæ¥çš„process_subject_dataå‡½æ•°ï¼‰
                    logger.info("ä½¿ç”¨å•ç®—æ³•æ¨¡å¼ï¼ˆgreedyï¼‰")
                    # è¿™é‡Œå¯ä»¥æ·»åŠ åŸæœ‰çš„å¤„ç†é€»è¾‘
                    subject_results = None
                
                if subject_results:
                    all_results[subject_id] = subject_results
                    processed_subjects += 1
                    
                    # å®šæœŸæ¸…ç†å†…å­˜
                    if processed_subjects % 2 == 0:
                        cleanup_memory()
                        progress_msg = get_label('progress',
                                               f"å·²å¤„ç† {processed_subjects}/{total_subjects} ä¸ªè¢«è¯•",
                                               f"Processed {processed_subjects}/{total_subjects} subjects")
                        logger.info(progress_msg)
                else:
                    no_result_msg = get_label('no_result',
                                            f"è¢«è¯• {subject_id} æœªäº§ç”Ÿæœ‰æ•ˆç»“æœ",
                                            f"Subject {subject_id} produced no valid results")
                    logger.warning(no_result_msg)
                    
            except Exception as e:
                logger.error(f"å¤„ç†è¢«è¯• {subject_id} æ—¶å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                continue
        
        if processed_subjects == 0:
            no_subjects_msg = get_label('no_subjects',
                                       "æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•è¢«è¯•æ•°æ®",
                                       "No subject data was successfully processed")
            logger.error(no_subjects_msg)
            print(f"\nâŒ {no_subjects_msg}")
            return 1
        
        complete_msg = get_label('data_complete',
                                f"æ•°æ®å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_subjects} ä¸ªè¢«è¯•",
                                f"Data processing complete, successfully processed {processed_subjects} subjects")
        logger.info(complete_msg)
        
        # ç”Ÿæˆç®—æ³•å¯¹æ¯”æŠ¥å‘Šå’Œå¯è§†åŒ–
        if Config.ENABLE_ALGORITHM_COMPARISON and all_results:
            algorithm_stats, performance_ranking = create_algorithm_comparison_report(all_results, logger)
            create_algorithm_comparison_visualizations(all_results, algorithm_stats, performance_ranking, visualizer, logger)
            
            # æ‰“å°æœ€ç»ˆæ€»ç»“
            print_final_summary(all_results, algorithm_stats)
        else:
            logger.info("ç®—æ³•å¯¹æ¯”å·²ç¦ç”¨æˆ–æ— æœ‰æ•ˆç»“æœ")
        
        # ä¿å­˜å®Œæ•´ç»“æœ
        results_path = os.path.join(Config.RESULTS_ROOT, "algorithm_comparison_results.pkl")
        try:
            with open(results_path, 'wb') as f:
                pickle.dump(all_results, f, protocol=pickle.HIGHEST_PROTOCOL)
            logger.info(f"å®Œæ•´ç»“æœå·²ä¿å­˜: {results_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å®Œæ•´ç»“æœå¤±è´¥: {e}")
        
        success_final = get_label('success_final',
                                 "ç®—æ³•å¯¹æ¯”å®éªŒæˆåŠŸå®Œæˆ!",
                                 "Algorithm comparison experiment completed successfully!")
        logger.info(success_final)
        return 0
        
    except KeyboardInterrupt:
        interrupt_msg = get_label('interrupted',
                                 "ç”¨æˆ·ä¸­æ–­äº†å®éªŒ",
                                 "Experiment was interrupted by user")
        logger.info(interrupt_msg)
        print(f"\nğŸ›‘ {interrupt_msg}")
        return 130
        
    except Exception as e:
        error_final = get_label('unexpected_error',
                               f"å®éªŒè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}",
                               f"Unexpected error during experiment: {e}")
        logger.error(error_final)
        print(f"\nâŒ {error_final}")
        return 1
        
    finally:
        cleanup_memory()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)